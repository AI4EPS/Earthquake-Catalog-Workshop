{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Earthquake Catalog Workshop","text":"<p>Welcome to the Earthquake Catalog Workshop! This repository contains materials for a workshop on earthquake catalogs, including data, scripts, and documentation.</p> <p>Contributors: Eric Beauce, Gabrielle Tepp, Clara Yoon, Ellen Yu, Weiqiang Zhu (alphabetical order)</p>"},{"location":"#workshop-plan-4-hours","title":"Workshop Plan (~ 4 hours)","text":""},{"location":"#introductions-overview","title":"Introductions &amp; Overview","text":"<ul> <li>Instructor intros</li> <li>Workshop goals/schedule</li> <li>Why earthquake catalogs and how to choose the right one?</li> </ul>"},{"location":"#regional-seismic-networks-official-catalogs-and-data-access","title":"Regional Seismic Networks: Official Catalogs and Data Access","text":"<ul> <li>How regional catalogs are made and what they include</li> <li>SCSN catalog and special datasets</li> <li>ComCat</li> <li>Accessing waveform data and metadata</li> <li>Citing network data &amp; catalogs</li> </ul>"},{"location":"#break","title":"Break","text":""},{"location":"#building-custom-catalogs-with-modern-tools","title":"Building Custom Catalogs with Modern Tools","text":"<ul> <li>Machine Learning</li> <li>Template Matching</li> <li>Magnitudes</li> </ul>"},{"location":"#break_1","title":"Break","text":""},{"location":"#evaluating-catalog-quality","title":"Evaluating Catalog Quality","text":"<ul> <li>Anomaly detection</li> <li>Visualization tools</li> <li>Magnitude of completeness</li> <li>Quality control</li> <li>Tips &amp; tricks</li> </ul>"},{"location":"#conclusion","title":"Conclusion","text":"<ul> <li>Discuss limitations and reasons for using different types of catalogs</li> <li>Combining different methods (also with STA/LTA?) depending on application</li> </ul>"},{"location":"#discussion-questions-tutorial-help","title":"Discussion, Questions, &amp; Tutorial Help","text":""},{"location":"catalog_analysis/","title":"Slides","text":"This is an embedded Microsoft Office presentation, powered by Office."},{"location":"introduction/","title":"Introduction","text":"This is an embedded Microsoft Office presentation, powered by Office."},{"location":"machine_learning/","title":"Slides","text":"This is an embedded Microsoft Office presentation, powered by Office."},{"location":"seismic_network/","title":"Slides","text":"This is an embedded Microsoft Office presentation, powered by Office."},{"location":"template_matching/","title":"Slides","text":"This is an embedded Microsoft Office presentation, powered by Office."},{"location":"notebooks/catalog_analysis/","title":"Notebook","text":"In\u00a0[\u00a0]: Copied!"},{"location":"notebooks/catalog_analysis/#evaluating-and-analyzing-earthquake-catalogs","title":"Evaluating and Analyzing Earthquake Catalogs\u00b6","text":""},{"location":"notebooks/machine_learning/","title":"Notebook","text":"In\u00a0[1]: Copied! <pre>import numpy as np\n</pre> import numpy as np In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"notebooks/machine_learning/#building-earthquake-catalogs-using-quakeflow","title":"Building Earthquake Catalogs using QuakeFlow\u00b6","text":""},{"location":"notebooks/seismic_network/","title":"Notebook","text":"In\u00a0[\u00a0]: Copied!"},{"location":"notebooks/seismic_network/#regional-seismic-networks","title":"Regional Seismic Networks\u00b6","text":""},{"location":"notebooks/template_matching/","title":"Notebook","text":"In\u00a0[\u00a0]: Copied!"},{"location":"notebooks/template_matching/#building-earthquake-catalogs-using-quakeflow","title":"Building Earthquake Catalogs using QuakeFlow\u00b6","text":""},{"location":"notebooks/tm_issues_w_detection_threshold/","title":"Tm issues w detection threshold","text":"In\u00a0[1]: Copied! <pre>import os\nimport fast_matched_filter as fmf\nimport glob\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport obspy as obs\nimport pandas as pd\n</pre> import os import fast_matched_filter as fmf import glob import numpy as np import matplotlib.pyplot as plt import obspy as obs import pandas as pd  In\u00a0[2]: Copied! <pre># path variables and file names\nDIR_WAVEFORMS = \"/home/ebeauce/SSA_EQ_DETECTION_WORKSHOP/data\" # REPLACE THIS PATH WITH WHEREVER YOU DOWNLOADED THE DATA\nDIR_CATALOG = \"../picks_phasenet/\"\n\nSTATION_FILE = \"adloc_stations.csv\"\nEVENT_FILE = \"adloc_events.csv\"\nPICK_FILE = \"adloc_picks.csv\"\n</pre> # path variables and file names DIR_WAVEFORMS = \"/home/ebeauce/SSA_EQ_DETECTION_WORKSHOP/data\" # REPLACE THIS PATH WITH WHEREVER YOU DOWNLOADED THE DATA DIR_CATALOG = \"../picks_phasenet/\"  STATION_FILE = \"adloc_stations.csv\" EVENT_FILE = \"adloc_events.csv\" PICK_FILE = \"adloc_picks.csv\" In\u00a0[3]: Copied! <pre>station_meta = pd.read_csv(os.path.join(DIR_CATALOG, STATION_FILE))\nstation_meta\n</pre> station_meta = pd.read_csv(os.path.join(DIR_CATALOG, STATION_FILE)) station_meta Out[3]: network station location instrument component latitude longitude elevation_m depth_km provider station_id station_term_time_p station_term_time_s station_term_amplitude 0 CI CCC NaN BH ENZ 35.524950 -117.364530 670.0 -0.6700 SCEDC CI.CCC..BH 0.266086 0.500831 0.049399 1 CI CCC NaN HH ENZ 35.524950 -117.364530 670.0 -0.6700 SCEDC CI.CCC..HH 0.295428 0.518465 0.191475 2 CI CCC NaN HN ENZ 35.524950 -117.364530 670.0 -0.6700 SCEDC CI.CCC..HN 0.296263 0.541148 0.064485 3 CI CLC NaN BH ENZ 35.815740 -117.597510 775.0 -0.7750 SCEDC CI.CLC..BH -0.231963 -0.415271 -0.331371 4 CI CLC NaN HH ENZ 35.815740 -117.597510 775.0 -0.7750 SCEDC CI.CLC..HH -0.168743 -0.390045 -0.140313 5 CI CLC NaN HN ENZ 35.815740 -117.597510 775.0 -0.7750 SCEDC CI.CLC..HN -0.175671 -0.388116 -0.249066 6 CI DTP NaN BH ENZ 35.267420 -117.845810 908.0 -0.9080 SCEDC CI.DTP..BH -0.305881 -0.602459 -0.503411 7 CI DTP NaN HH ENZ 35.267420 -117.845810 908.0 -0.9080 SCEDC CI.DTP..HH -0.263705 -0.564867 -0.437951 8 CI DTP NaN HN ENZ 35.267420 -117.845810 908.0 -0.9080 SCEDC CI.DTP..HN -0.244383 -0.538990 -0.500516 9 CI JRC2 NaN BH ENZ 35.982490 -117.808850 1469.0 -1.4690 SCEDC CI.JRC2..BH 0.011361 -0.080285 -0.039941 10 CI JRC2 NaN HH ENZ 35.982490 -117.808850 1469.0 -1.4690 SCEDC CI.JRC2..HH 0.053539 -0.052748 0.068213 11 CI JRC2 NaN HN ENZ 35.982490 -117.808850 1469.0 -1.4690 SCEDC CI.JRC2..HN 0.059764 -0.045991 -0.007637 12 CI LRL NaN BH ENZ 35.479540 -117.682120 1340.0 -1.3400 SCEDC CI.LRL..BH -0.295604 -0.540857 0.033788 13 CI LRL NaN HH ENZ 35.479540 -117.682120 1340.0 -1.3400 SCEDC CI.LRL..HH -0.268381 -0.513955 0.146876 14 CI LRL NaN HN ENZ 35.479540 -117.682120 1340.0 -1.3400 SCEDC CI.LRL..HN -0.266329 -0.503088 0.045499 15 CI LRL 2C HN ENZ 35.479540 -117.682120 1340.0 -1.3400 SCEDC CI.LRL.2C.HN 0.000000 0.000000 0.000000 16 CI MPM NaN BH ENZ 36.057990 -117.489010 1839.0 -1.8390 SCEDC CI.MPM..BH -0.011825 -0.098089 -0.518793 17 CI MPM NaN HH ENZ 36.057990 -117.489010 1839.0 -1.8390 SCEDC CI.MPM..HH 0.009095 -0.081896 -0.459349 18 CI MPM NaN HN ENZ 36.057990 -117.489010 1839.0 -1.8390 SCEDC CI.MPM..HN 0.011870 -0.052277 -0.532764 19 CI Q0072 01 HN ENZ 35.609617 -117.666721 695.0 -0.6950 SCEDC CI.Q0072.01.HN 0.000000 0.000000 0.000000 20 CI SLA NaN BH ENZ 35.890950 -117.283320 1174.0 -1.1740 SCEDC CI.SLA..BH 0.066893 0.118500 -0.081634 21 CI SLA NaN HH ENZ 35.890950 -117.283320 1174.0 -1.1740 SCEDC CI.SLA..HH 0.089833 0.128589 -0.042928 22 CI SLA NaN HN ENZ 35.890950 -117.283320 1174.0 -1.1740 SCEDC CI.SLA..HN 0.093526 0.168238 -0.150381 23 CI SRT NaN BH ENZ 35.692350 -117.750510 667.0 -0.6670 SCEDC CI.SRT..BH 0.148171 0.653411 -0.253527 24 CI SRT NaN HH ENZ 35.692350 -117.750510 667.0 -0.6670 SCEDC CI.SRT..HH 0.183868 0.641707 -0.208859 25 CI SRT NaN HN ENZ 35.692350 -117.750510 667.0 -0.6670 SCEDC CI.SRT..HN 0.175475 0.674587 -0.295869 26 CI TOW2 NaN BH ENZ 35.808560 -117.764880 685.0 -0.6850 SCEDC CI.TOW2..BH 0.268083 0.816279 0.028038 27 CI TOW2 NaN HH ENZ 35.808560 -117.764880 685.0 -0.6850 SCEDC CI.TOW2..HH 0.285970 0.831533 0.102681 28 CI TOW2 NaN HN ENZ 35.808560 -117.764880 685.0 -0.6850 SCEDC CI.TOW2..HN 0.285113 0.843886 -0.006698 29 CI WBM NaN BH ENZ 35.608390 -117.890490 892.0 -0.8920 SCEDC CI.WBM..BH 0.136927 0.128744 -0.166279 30 CI WBM NaN HH ENZ 35.608390 -117.890490 892.0 -0.8920 SCEDC CI.WBM..HH 0.152299 0.124250 -0.138614 31 CI WBM NaN HN ENZ 35.608390 -117.890490 892.0 -0.8920 SCEDC CI.WBM..HN 0.170719 0.179461 -0.101436 32 CI WBM 2C HN ENZ 35.608390 -117.890490 892.0 -0.8920 SCEDC CI.WBM.2C.HN 0.000000 0.000000 0.000000 33 CI WCS2 NaN BH ENZ 36.025210 -117.765260 1143.0 -1.1430 SCEDC CI.WCS2..BH 0.030349 -0.126935 0.023642 34 CI WCS2 NaN HH ENZ 36.025210 -117.765260 1143.0 -1.1430 SCEDC CI.WCS2..HH 0.065321 -0.099447 0.146628 35 CI WCS2 NaN HN ENZ 36.025210 -117.765260 1143.0 -1.1430 SCEDC CI.WCS2..HN 0.061651 -0.096450 0.037563 36 CI WMF NaN BH ENZ 36.117580 -117.854860 1537.4 -1.5374 SCEDC CI.WMF..BH 0.039416 -0.005761 -0.165183 37 CI WMF NaN HH ENZ 36.117580 -117.854860 1537.4 -1.5374 SCEDC CI.WMF..HH 0.075658 0.005327 -0.079900 38 CI WMF NaN HN ENZ 36.117580 -117.854860 1537.4 -1.5374 SCEDC CI.WMF..HN 0.085427 0.025273 -0.240971 39 CI WMF 2C HN ENZ 36.117580 -117.854860 1537.4 -1.5374 SCEDC CI.WMF.2C.HN 0.000000 0.000000 0.000000 40 CI WNM NaN EH Z 35.842200 -117.906160 974.3 -0.9743 SCEDC CI.WNM..EH -0.026889 -0.070269 -0.332510 41 CI WNM NaN HN ENZ 35.842200 -117.906160 974.3 -0.9743 SCEDC CI.WNM..HN -0.011659 -0.118223 -0.038719 42 CI WNM 2C HN ENZ 35.842200 -117.906160 974.3 -0.9743 SCEDC CI.WNM.2C.HN 0.000000 0.000000 0.000000 43 CI WRC2 NaN BH ENZ 35.947900 -117.650380 943.0 -0.9430 SCEDC CI.WRC2..BH -0.023860 -0.043523 0.117833 44 CI WRC2 NaN HH ENZ 35.947900 -117.650380 943.0 -0.9430 SCEDC CI.WRC2..HH 0.010633 -0.029488 0.165234 45 CI WRC2 NaN HN ENZ 35.947900 -117.650380 943.0 -0.9430 SCEDC CI.WRC2..HN 0.014658 -0.021367 0.103905 46 CI WRV2 NaN EH Z 36.007740 -117.890400 1070.0 -1.0700 SCEDC CI.WRV2..EH -0.003461 -0.154572 -0.355199 47 CI WRV2 NaN HN ENZ 36.007740 -117.890400 1070.0 -1.0700 SCEDC CI.WRV2..HN 0.017317 -0.137916 -0.273270 48 CI WRV2 2C HN ENZ 36.007740 -117.890400 1070.0 -1.0700 SCEDC CI.WRV2.2C.HN 0.000000 0.000000 0.000000 49 CI WVP2 NaN EH Z 35.949390 -117.817690 1465.0 -1.4650 SCEDC CI.WVP2..EH 0.020325 0.008989 -0.341606 50 CI WVP2 NaN HN ENZ 35.949390 -117.817690 1465.0 -1.4650 SCEDC CI.WVP2..HN 0.024742 -0.103024 -0.089014 51 CI WVP2 2C HN ENZ 35.949390 -117.817690 1465.0 -1.4650 SCEDC CI.WVP2.2C.HN 0.000000 0.000000 0.000000 <p>The following shows a very rudimentary map of the station network. Look into the <code>cartopy</code> package for more sophisticated maps.</p> In\u00a0[4]: Copied! <pre>_station_meta = station_meta.drop_duplicates(\"station\")\n\nfig, ax = plt.subplots(num=\"station_network\", figsize=(10, 10))\nax.scatter(_station_meta[\"longitude\"], _station_meta[\"latitude\"], marker=\"v\", color=\"k\")\nfor idx, row in _station_meta.iterrows():\n    ax.text(row.longitude + 0.01, row.latitude + 0.01, row.station, va=\"bottom\", ha=\"left\")\nax.set_xlabel(\"Longitude\")\nax.set_ylabel(\"Latitude\")\nax.grid()\nax.set_title(\"Stations used to build the PhaseNet catalog\")\n</pre> _station_meta = station_meta.drop_duplicates(\"station\")  fig, ax = plt.subplots(num=\"station_network\", figsize=(10, 10)) ax.scatter(_station_meta[\"longitude\"], _station_meta[\"latitude\"], marker=\"v\", color=\"k\") for idx, row in _station_meta.iterrows():     ax.text(row.longitude + 0.01, row.latitude + 0.01, row.station, va=\"bottom\", ha=\"left\") ax.set_xlabel(\"Longitude\") ax.set_ylabel(\"Latitude\") ax.grid() ax.set_title(\"Stations used to build the PhaseNet catalog\") Out[4]: <pre>Text(0.5, 1.0, 'Stations used to build the PhaseNet catalog')</pre> In\u00a0[5]: Copied! <pre>event_meta = pd.read_csv(os.path.join(DIR_CATALOG, EVENT_FILE))\nevent_meta\n</pre> event_meta = pd.read_csv(os.path.join(DIR_CATALOG, EVENT_FILE)) event_meta Out[5]: time adloc_score adloc_residual_time num_picks magnitude adloc_residual_amplitude event_index longitude latitude depth_km 0 2019-07-04 00:46:47.342963596 0.866596 0.057355 35 0.595368 0.165480 6720 -117.882570 36.091088 4.643969 1 2019-07-04 00:55:32.648412579 0.781116 0.203567 16 1.142510 0.170509 17122 -117.799226 35.378160 11.078458 2 2019-07-04 00:56:37.232733104 0.908073 0.086183 42 0.912494 0.166681 5411 -117.880902 36.091986 4.854336 3 2019-07-04 02:00:39.149363202 0.814322 0.036164 15 0.209530 0.092534 17868 -117.866468 36.093520 4.981447 4 2019-07-04 03:05:31.018885833 0.799281 0.080708 11 0.104050 0.156833 25037 -117.846320 36.100386 5.943363 ... ... ... ... ... ... ... ... ... ... ... 20898 2019-07-09 23:58:14.298499048 0.933013 0.097816 67 1.543257 0.131523 1907 -117.711811 35.926468 6.652020 20899 2019-07-09 23:58:47.701746285 0.882434 0.090185 73 1.087089 0.140159 2051 -117.604263 35.797450 6.617413 20900 2019-07-09 23:59:05.102247662 0.798047 0.435077 26 1.147040 0.170994 12567 -117.509729 35.692722 12.815041 20901 2019-07-09 23:59:40.257837813 0.971081 0.065523 35 1.161323 0.068586 7726 -117.846289 36.061435 5.224666 20902 2019-07-09 23:59:49.650466544 0.800524 0.023674 15 0.936622 0.095959 18566 -117.896100 36.095886 6.761860 <p>20903 rows \u00d7 10 columns</p> In\u00a0[6]: Copied! <pre>picks = pd.read_csv(os.path.join(DIR_CATALOG, PICK_FILE))\npicks\n</pre> picks = pd.read_csv(os.path.join(DIR_CATALOG, PICK_FILE)) picks Out[6]: station_id phase_index phase_time phase_score phase_type dt_s phase_polarity phase_amplitude event_index sp_ratio event_amplitude adloc_mask adloc_residual_time adloc_residual_amplitude 0 CI.WMF..BH 280874 2019-07-04 00:46:48.759 0.594 P 0.01 0.110 -3.213107 6720 0.955091 3.124152e-07 1 0.003919 0.012320 1 CI.WMF..HH 280881 2019-07-04 00:46:48.818 0.973 P 0.01 0.938 -3.077638 6720 0.948896 3.368390e-07 1 0.026491 0.063669 2 CI.WMF..HN 280881 2019-07-04 00:46:48.818 0.973 P 0.01 0.898 -3.128019 6720 1.598008 2.146277e-07 1 0.017580 0.173702 3 CI.WRV2..EH 280945 2019-07-04 00:46:49.450 0.977 P 0.01 -0.855 -3.464453 6720 1.000000 3.298200e-07 1 0.077194 0.244381 4 CI.WRV2..HN 280945 2019-07-04 00:46:49.450 0.941 P 0.01 -0.906 -3.585863 6720 1.147235 3.908305e-07 1 0.056694 0.041691 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 955700 CI.WRV2..HN 8639355 2019-07-09 23:59:53.550 0.688 S 0.01 -0.022 -3.384471 18566 0.644492 1.973834e-06 1 0.019093 0.028409 955701 CI.JRC2..HH 8639499 2019-07-09 23:59:54.998 0.402 S 0.01 -0.023 -3.446238 18566 0.647843 6.197256e-06 1 0.020364 -0.154404 955702 CI.JRC2..HN 8639499 2019-07-09 23:59:54.998 0.344 S 0.01 -0.016 -3.531800 18566 0.731796 4.075353e-06 1 0.012773 -0.164659 955703 CI.WVP2..EH 8639574 2019-07-09 23:59:55.740 0.314 S 0.01 -0.103 -3.488117 18566 0.636005 2.815148e-06 1 -0.092233 0.316712 955704 CI.WVP2..HN 8639576 2019-07-09 23:59:55.760 0.695 S 0.01 0.076 -3.218676 18566 0.825223 4.657352e-06 1 0.039319 0.332722 <p>955705 rows \u00d7 14 columns</p> In\u00a0[7]: Copied! <pre>def fetch_event_waveforms(\n    event_picks,\n    dir_waveforms=DIR_WAVEFORMS,\n    time_before_phase_onset_sec=2.0,\n    duration_sec=10.0\n    ):\n    \"\"\"\n    Fetches the waveforms for a given event based on the picks.\n\n    Parameters\n    ----------\n    event_picks : pandas.DataFrame\n        DataFrame containing the picks for the event.\n    dir_waveforms : str, optional\n        Directory where the waveform data is stored, by default DIR_WAVEFORMS.\n    time_before_phase_onset_sec : float, optional\n        Time in seconds to start the waveform before the phase onset, by default 2.0.\n    duration_sec : float, optional\n        Duration in seconds of the waveform to fetch, by default 10.0.\n\n    Returns\n    -------\n    obspy.Stream\n        Stream object containing the fetched waveforms.\n    \"\"\"\n    stream = obs.Stream()\n    for _, pick in event_picks.iterrows():\n        # check whether we have a miniseed file for this waveform\n        if pick.phase_type == \"P\":\n            files = glob.glob(os.path.join(dir_waveforms, pick.station_id + \"Z*mseed\"))\n        elif pick.phase_type == \"S\":\n            files = glob.glob(os.path.join(dir_waveforms, pick.station_id + \"[N,E]*mseed\"))\n        starttime = obs.UTCDateTime(pick.phase_time) - time_before_phase_onset_sec\n        endtime = starttime + duration_sec\n        for _file in files:\n            stream += obs.read(\n                _file,\n                starttime=starttime,\n                endtime=endtime\n            )\n    return stream\n    \n</pre> def fetch_event_waveforms(     event_picks,     dir_waveforms=DIR_WAVEFORMS,     time_before_phase_onset_sec=2.0,     duration_sec=10.0     ):     \"\"\"     Fetches the waveforms for a given event based on the picks.      Parameters     ----------     event_picks : pandas.DataFrame         DataFrame containing the picks for the event.     dir_waveforms : str, optional         Directory where the waveform data is stored, by default DIR_WAVEFORMS.     time_before_phase_onset_sec : float, optional         Time in seconds to start the waveform before the phase onset, by default 2.0.     duration_sec : float, optional         Duration in seconds of the waveform to fetch, by default 10.0.      Returns     -------     obspy.Stream         Stream object containing the fetched waveforms.     \"\"\"     stream = obs.Stream()     for _, pick in event_picks.iterrows():         # check whether we have a miniseed file for this waveform         if pick.phase_type == \"P\":             files = glob.glob(os.path.join(dir_waveforms, pick.station_id + \"Z*mseed\"))         elif pick.phase_type == \"S\":             files = glob.glob(os.path.join(dir_waveforms, pick.station_id + \"[N,E]*mseed\"))         starttime = obs.UTCDateTime(pick.phase_time) - time_before_phase_onset_sec         endtime = starttime + duration_sec         for _file in files:             stream += obs.read(                 _file,                 starttime=starttime,                 endtime=endtime             )     return stream      In\u00a0[8]: Copied! <pre># explore event_meta to find a nice intermediate-size earthquake we could plot\nevent_meta.head(20)\n</pre> # explore event_meta to find a nice intermediate-size earthquake we could plot event_meta.head(20) Out[8]: time adloc_score adloc_residual_time num_picks magnitude adloc_residual_amplitude event_index longitude latitude depth_km 0 2019-07-04 00:46:47.342963596 0.866596 0.057355 35 0.595368 0.165480 6720 -117.882570 36.091088 4.643969 1 2019-07-04 00:55:32.648412579 0.781116 0.203567 16 1.142510 0.170509 17122 -117.799226 35.378160 11.078458 2 2019-07-04 00:56:37.232733104 0.908073 0.086183 42 0.912494 0.166681 5411 -117.880902 36.091986 4.854336 3 2019-07-04 02:00:39.149363202 0.814322 0.036164 15 0.209530 0.092534 17868 -117.866468 36.093520 4.981447 4 2019-07-04 03:05:31.018885833 0.799281 0.080708 11 0.104050 0.156833 25037 -117.846320 36.100386 5.943363 5 2019-07-04 03:20:28.674438914 0.755451 0.050436 44 0.869927 0.111885 6929 -117.673684 36.114308 5.894466 6 2019-07-04 04:03:01.619369274 0.897311 0.143631 32 0.386310 0.214381 7607 -117.805077 36.016063 0.396071 7 2019-07-04 05:16:47.223353119 0.835094 0.053522 22 0.575433 0.163675 13098 -117.879256 36.090409 4.268292 8 2019-07-04 06:57:32.758991812 0.922386 0.065846 23 0.256281 0.065212 15698 -117.867587 36.081665 5.939931 9 2019-07-04 11:51:07.805591259 0.692042 0.075023 48 0.818316 0.102087 4380 -117.671909 36.118573 6.085804 10 2019-07-04 15:36:04.228420696 0.652888 0.012137 9 -0.633904 0.167058 27370 -117.785299 36.005578 3.234981 11 2019-07-04 15:42:47.932558745 0.597856 0.072740 28 0.933088 0.151623 9740 -117.501116 35.707382 14.496446 12 2019-07-04 16:07:20.003321194 0.729246 0.093399 33 0.835854 0.133106 9365 -117.491503 35.711241 13.846898 13 2019-07-04 16:11:46.920083440 0.711579 0.516367 18 1.734816 0.117952 7098 -117.877675 35.196445 31.000000 14 2019-07-04 16:13:43.094792540 0.849673 0.070048 83 1.653859 0.111108 2305 -117.493716 35.710090 13.375377 15 2019-07-04 16:16:07.085486307 0.814365 0.040161 10 0.587583 0.099619 26520 -117.540198 35.689022 14.537368 16 2019-07-04 17:02:55.057058245 0.770696 0.079675 90 4.476724 0.143489 1101 -117.495094 35.711607 13.586411 17 2019-07-04 17:04:02.231614981 0.664676 0.064038 41 2.062226 0.186214 5379 -117.488446 35.711139 14.001705 18 2019-07-04 17:05:05.071421677 0.509088 0.089882 29 1.471434 0.157048 12115 -117.491307 35.710881 13.223374 19 2019-07-04 17:08:51.664841725 0.666790 0.046444 15 0.657653 0.175270 23044 -117.504506 35.706123 14.570921 In\u00a0[9]: Copied! <pre># feel free to play with the event index to plot different events\nEVENT_IDX = 1101\n\nevent_meta.set_index(\"event_index\").loc[EVENT_IDX]\n</pre> # feel free to play with the event index to plot different events EVENT_IDX = 1101  event_meta.set_index(\"event_index\").loc[EVENT_IDX] Out[9]: <pre>time                        2019-07-04 17:02:55.057058245\nadloc_score                                      0.770696\nadloc_residual_time                              0.079675\nnum_picks                                              90\nmagnitude                                        4.476724\nadloc_residual_amplitude                         0.143489\nlongitude                                     -117.495094\nlatitude                                        35.711607\ndepth_km                                        13.586411\nName: 1101, dtype: object</pre> In\u00a0[10]: Copied! <pre># fetch the corresponding picks for this event\nevent_picks = picks[picks[\"event_index\"] == EVENT_IDX]\nevent_picks\n</pre> # fetch the corresponding picks for this event event_picks = picks[picks[\"event_index\"] == EVENT_IDX] event_picks Out[10]: station_id phase_index phase_time phase_score phase_type dt_s phase_polarity phase_amplitude event_index sp_ratio event_amplitude adloc_mask adloc_residual_time adloc_residual_amplitude 565 CI.CLC..HN 6137848 2019-07-04 17:02:58.488 0.969 P 0.01 -0.879 -0.511873 1101 1.017569 2.474507e-07 1 -0.010048 -0.054426 566 CI.CLC..BH 6137847 2019-07-04 17:02:58.489 0.633 P 0.01 -0.324 -0.666956 1101 1.014822 2.022085e-07 1 0.048153 -0.127168 567 CI.CLC..HH 6137849 2019-07-04 17:02:58.498 0.965 P 0.01 -0.883 -0.586030 1101 1.086886 2.141392e-07 1 -0.007135 -0.236796 568 CI.SRT..HN 6137990 2019-07-04 17:02:59.908 0.879 P 0.01 0.699 -0.643401 1101 1.377537 4.387864e-07 1 -0.069463 0.065039 569 CI.SRT..HH 6137990 2019-07-04 17:02:59.908 0.891 P 0.01 0.734 -0.601019 1101 0.729745 5.910421e-07 1 -0.078144 0.020251 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 650 CI.WMF..HH 6139185 2019-07-04 17:03:11.858 0.672 S 0.01 0.021 -1.368759 1101 1.096820 3.073536e-07 1 0.140179 -0.329117 651 CI.WMF..HN 6139189 2019-07-04 17:03:11.898 0.633 S 0.01 0.026 -1.371407 1101 0.989939 2.891483e-07 1 0.159858 -0.171350 652 CI.DTP..BH 6139200 2019-07-04 17:03:12.019 0.734 S 0.01 0.035 -1.322484 1101 1.385395 1.624267e-07 1 0.068940 0.175198 653 CI.DTP..HH 6139203 2019-07-04 17:03:12.038 0.707 S 0.01 -0.015 -1.306977 1101 1.122809 3.205840e-07 1 0.052824 0.125587 654 CI.DTP..HN 6139211 2019-07-04 17:03:12.118 0.625 S 0.01 0.043 -1.282829 1101 0.863380 2.702479e-07 1 0.105026 0.211253 <p>90 rows \u00d7 14 columns</p> In\u00a0[11]: Copied! <pre># fetch the waveforms\nevent_waveforms = fetch_event_waveforms(event_picks, time_before_phase_onset_sec=10., duration_sec=30.)\nprint(event_waveforms.__str__(extended=True))\n</pre> # fetch the waveforms event_waveforms = fetch_event_waveforms(event_picks, time_before_phase_onset_sec=10., duration_sec=30.) print(event_waveforms.__str__(extended=True)) <pre>42 Trace(s) in Stream:\nCI.CLC..HHZ  | 2019-07-04T17:02:48.478300Z - 2019-07-04T17:03:18.478300Z | 25.0 Hz, 751 samples\nCI.SRT..HHZ  | 2019-07-04T17:02:49.918300Z - 2019-07-04T17:03:19.918300Z | 25.0 Hz, 751 samples\nCI.CCC..HHZ  | 2019-07-04T17:02:50.118300Z - 2019-07-04T17:03:20.118300Z | 25.0 Hz, 751 samples\nCI.SLA..HHZ  | 2019-07-04T17:02:50.518300Z - 2019-07-04T17:03:20.518300Z | 25.0 Hz, 751 samples\nCI.TOW2..HHZ | 2019-07-04T17:02:50.518300Z - 2019-07-04T17:03:20.518300Z | 25.0 Hz, 751 samples\nCI.LRL..HHZ  | 2019-07-04T17:02:50.638300Z - 2019-07-04T17:03:20.638300Z | 25.0 Hz, 751 samples\nCI.WRC2..HHZ | 2019-07-04T17:02:50.718300Z - 2019-07-04T17:03:20.718300Z | 25.0 Hz, 751 samples\nCI.CLC..HHN  | 2019-07-04T17:02:50.958300Z - 2019-07-04T17:03:20.958300Z | 25.0 Hz, 751 samples\nCI.CLC..HHE  | 2019-07-04T17:02:50.958300Z - 2019-07-04T17:03:20.958300Z | 25.0 Hz, 751 samples\nCI.MPM..HHZ  | 2019-07-04T17:02:52.038300Z - 2019-07-04T17:03:22.038300Z | 25.0 Hz, 751 samples\nCI.WBM..HHZ  | 2019-07-04T17:02:52.123100Z - 2019-07-04T17:03:22.123100Z | 25.0 Hz, 751 samples\nCI.WVP2..EHZ | 2019-07-04T17:02:52.160000Z - 2019-07-04T17:03:22.160000Z | 25.0 Hz, 751 samples\nCI.WNM..EHZ  | 2019-07-04T17:02:52.160000Z - 2019-07-04T17:03:22.160000Z | 25.0 Hz, 751 samples\nCI.JRC2..HHZ | 2019-07-04T17:02:52.558300Z - 2019-07-04T17:03:22.558300Z | 25.0 Hz, 751 samples\nCI.WCS2..HHZ | 2019-07-04T17:02:52.598300Z - 2019-07-04T17:03:22.598300Z | 25.0 Hz, 751 samples\nCI.WRV2..EHZ | 2019-07-04T17:02:53.600000Z - 2019-07-04T17:03:23.600000Z | 25.0 Hz, 751 samples\nCI.SRT..HHN  | 2019-07-04T17:02:53.838300Z - 2019-07-04T17:03:23.838300Z | 25.0 Hz, 751 samples\nCI.SRT..HHE  | 2019-07-04T17:02:53.838300Z - 2019-07-04T17:03:23.838300Z | 25.0 Hz, 751 samples\nCI.CCC..HHN  | 2019-07-04T17:02:54.078300Z - 2019-07-04T17:03:24.078300Z | 25.0 Hz, 751 samples\nCI.CCC..HHE  | 2019-07-04T17:02:54.078300Z - 2019-07-04T17:03:24.078300Z | 25.0 Hz, 751 samples\nCI.SLA..HHE  | 2019-07-04T17:02:54.638300Z - 2019-07-04T17:03:24.638300Z | 25.0 Hz, 751 samples\nCI.SLA..HHN  | 2019-07-04T17:02:54.638300Z - 2019-07-04T17:03:24.638300Z | 25.0 Hz, 751 samples\nCI.LRL..HHN  | 2019-07-04T17:02:54.718300Z - 2019-07-04T17:03:24.718300Z | 25.0 Hz, 751 samples\nCI.LRL..HHE  | 2019-07-04T17:02:54.718300Z - 2019-07-04T17:03:24.718300Z | 25.0 Hz, 751 samples\nCI.WMF..HHZ  | 2019-07-04T17:02:54.798300Z - 2019-07-04T17:03:24.798300Z | 25.0 Hz, 751 samples\nCI.DTP..HHZ  | 2019-07-04T17:02:54.958300Z - 2019-07-04T17:03:24.958300Z | 25.0 Hz, 751 samples\nCI.TOW2..HHN | 2019-07-04T17:02:54.998300Z - 2019-07-04T17:03:24.998300Z | 25.0 Hz, 751 samples\nCI.TOW2..HHE | 2019-07-04T17:02:54.998300Z - 2019-07-04T17:03:24.998300Z | 25.0 Hz, 751 samples\nCI.WRC2..HHN | 2019-07-04T17:02:54.998300Z - 2019-07-04T17:03:24.998300Z | 25.0 Hz, 751 samples\nCI.WRC2..HHE | 2019-07-04T17:02:54.998300Z - 2019-07-04T17:03:24.998300Z | 25.0 Hz, 751 samples\nCI.WBM..HHE  | 2019-07-04T17:02:57.243100Z - 2019-07-04T17:03:27.243100Z | 25.0 Hz, 751 samples\nCI.WBM..HHN  | 2019-07-04T17:02:57.243100Z - 2019-07-04T17:03:27.243100Z | 25.0 Hz, 751 samples\nCI.MPM..HHE  | 2019-07-04T17:02:57.318300Z - 2019-07-04T17:03:27.318300Z | 25.0 Hz, 751 samples\nCI.MPM..HHN  | 2019-07-04T17:02:57.318300Z - 2019-07-04T17:03:27.318300Z | 25.0 Hz, 751 samples\nCI.JRC2..HHN | 2019-07-04T17:02:57.958300Z - 2019-07-04T17:03:27.958300Z | 25.0 Hz, 751 samples\nCI.JRC2..HHE | 2019-07-04T17:02:57.958300Z - 2019-07-04T17:03:27.958300Z | 25.0 Hz, 751 samples\nCI.WCS2..HHE | 2019-07-04T17:02:58.118300Z - 2019-07-04T17:03:28.118300Z | 25.0 Hz, 751 samples\nCI.WCS2..HHN | 2019-07-04T17:02:58.118300Z - 2019-07-04T17:03:28.118300Z | 25.0 Hz, 751 samples\nCI.WMF..HHN  | 2019-07-04T17:03:01.838300Z - 2019-07-04T17:03:31.838300Z | 25.0 Hz, 751 samples\nCI.WMF..HHE  | 2019-07-04T17:03:01.838300Z - 2019-07-04T17:03:31.838300Z | 25.0 Hz, 751 samples\nCI.DTP..HHN  | 2019-07-04T17:03:02.038300Z - 2019-07-04T17:03:32.038300Z | 25.0 Hz, 751 samples\nCI.DTP..HHE  | 2019-07-04T17:03:02.038300Z - 2019-07-04T17:03:32.038300Z | 25.0 Hz, 751 samples\n</pre> In\u00a0[12]: Copied! <pre># plot them!\nfig = event_waveforms.select(component=\"Z\").plot(equal_scale=False)\n</pre> # plot them! fig = event_waveforms.select(component=\"Z\").plot(equal_scale=False) In\u00a0[13]: Copied! <pre>selected_event_meta = event_meta.set_index(\"event_index\").loc[EVENT_IDX]\nselected_event_meta\n</pre> selected_event_meta = event_meta.set_index(\"event_index\").loc[EVENT_IDX] selected_event_meta Out[13]: <pre>time                        2019-07-04 17:02:55.057058245\nadloc_score                                      0.770696\nadloc_residual_time                              0.079675\nnum_picks                                              90\nmagnitude                                        4.476724\nadloc_residual_amplitude                         0.143489\nlongitude                                     -117.495094\nlatitude                                        35.711607\ndepth_km                                        13.586411\nName: 1101, dtype: object</pre> In\u00a0[14]: Copied! <pre>selected_event_picks = picks[picks[\"event_index\"] == EVENT_IDX]\nselected_event_picks\n</pre> selected_event_picks = picks[picks[\"event_index\"] == EVENT_IDX] selected_event_picks Out[14]: station_id phase_index phase_time phase_score phase_type dt_s phase_polarity phase_amplitude event_index sp_ratio event_amplitude adloc_mask adloc_residual_time adloc_residual_amplitude 565 CI.CLC..HN 6137848 2019-07-04 17:02:58.488 0.969 P 0.01 -0.879 -0.511873 1101 1.017569 2.474507e-07 1 -0.010048 -0.054426 566 CI.CLC..BH 6137847 2019-07-04 17:02:58.489 0.633 P 0.01 -0.324 -0.666956 1101 1.014822 2.022085e-07 1 0.048153 -0.127168 567 CI.CLC..HH 6137849 2019-07-04 17:02:58.498 0.965 P 0.01 -0.883 -0.586030 1101 1.086886 2.141392e-07 1 -0.007135 -0.236796 568 CI.SRT..HN 6137990 2019-07-04 17:02:59.908 0.879 P 0.01 0.699 -0.643401 1101 1.377537 4.387864e-07 1 -0.069463 0.065039 569 CI.SRT..HH 6137990 2019-07-04 17:02:59.908 0.891 P 0.01 0.734 -0.601019 1101 0.729745 5.910421e-07 1 -0.078144 0.020251 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 650 CI.WMF..HH 6139185 2019-07-04 17:03:11.858 0.672 S 0.01 0.021 -1.368759 1101 1.096820 3.073536e-07 1 0.140179 -0.329117 651 CI.WMF..HN 6139189 2019-07-04 17:03:11.898 0.633 S 0.01 0.026 -1.371407 1101 0.989939 2.891483e-07 1 0.159858 -0.171350 652 CI.DTP..BH 6139200 2019-07-04 17:03:12.019 0.734 S 0.01 0.035 -1.322484 1101 1.385395 1.624267e-07 1 0.068940 0.175198 653 CI.DTP..HH 6139203 2019-07-04 17:03:12.038 0.707 S 0.01 -0.015 -1.306977 1101 1.122809 3.205840e-07 1 0.052824 0.125587 654 CI.DTP..HN 6139211 2019-07-04 17:03:12.118 0.625 S 0.01 0.043 -1.282829 1101 0.863380 2.702479e-07 1 0.105026 0.211253 <p>90 rows \u00d7 14 columns</p> In\u00a0[15]: Copied! <pre>def fetch_day_waveforms(dir_waveforms):\n    \"\"\"\n    Fetches the continuous seismograms for a given day.\n\n    Parameters\n    ----------\n    dir_waveforms : str\n        Directory where the waveform data is stored, by default DIR_WAVEFORMS.\n\n    Returns\n    -------\n    obspy.Stream\n        Stream object containing the fetched continuous seismograms.\n    \"\"\"\n    stream = obs.Stream()\n    files = glob.glob(os.path.join(dir_waveforms, \"*mseed\"))\n    for _file in files:\n        stream += obs.read(_file)\n    return stream\n</pre> def fetch_day_waveforms(dir_waveforms):     \"\"\"     Fetches the continuous seismograms for a given day.      Parameters     ----------     dir_waveforms : str         Directory where the waveform data is stored, by default DIR_WAVEFORMS.      Returns     -------     obspy.Stream         Stream object containing the fetched continuous seismograms.     \"\"\"     stream = obs.Stream()     files = glob.glob(os.path.join(dir_waveforms, \"*mseed\"))     for _file in files:         stream += obs.read(_file)     return stream In\u00a0[16]: Copied! <pre># first, read the continuous seismograms into an `obspy.Stream`\ncontinuous_seismograms = fetch_day_waveforms(DIR_WAVEFORMS)\nprint(continuous_seismograms.__str__(extended=True))\n</pre> # first, read the continuous seismograms into an `obspy.Stream` continuous_seismograms = fetch_day_waveforms(DIR_WAVEFORMS) print(continuous_seismograms.__str__(extended=True)) <pre>57 Trace(s) in Stream:\nCI.WCS2..HHE | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nPB.B916..EHZ | 2019-07-03T23:59:59.998200Z - 2019-07-04T23:59:59.958200Z | 25.0 Hz, 2160000 samples\nCI.CLC..HHN  | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.WRV2..EHZ | 2019-07-04T00:00:00.000000Z - 2019-07-04T23:59:59.960000Z | 25.0 Hz, 2160000 samples\nPB.B916..EH2 | 2019-07-03T23:59:59.998200Z - 2019-07-04T23:59:59.958200Z | 25.0 Hz, 2160000 samples\nPB.B917..EH1 | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.WMF..HHZ  | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nPB.B918..EHZ | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nPB.B918..EH2 | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.TOW2..HHZ | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.SLA..HHE  | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.WRC2..HHZ | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.JRC2..HHZ | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.CCC..HHZ  | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.DTP..HHN  | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.SRT..HHZ  | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.LRL..HHZ  | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.SLA..HHN  | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.MPM..HHZ  | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.DTP..HHE  | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.WBM..HHZ  | 2019-07-04T00:00:00.003100Z - 2019-07-04T23:59:59.963100Z | 25.0 Hz, 2160000 samples\nCI.WCS2..HHN | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.CLC..HHE  | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nPB.B921..EH1 | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.DAW..HHZ  | 2019-07-04T00:00:00.003100Z - 2019-07-04T23:59:59.963100Z | 25.0 Hz, 2160000 samples\nCI.CLC..HHZ  | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.WMF..HHN  | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.TOW2..HHN | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.DAW..HHE  | 2019-07-04T00:00:00.003100Z - 2019-07-04T23:59:59.963100Z | 25.0 Hz, 2160000 samples\nPB.B918..EH1 | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.WRC2..HHN | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.JRC2..HHN | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.CCC..HHN  | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nPB.B916..EH1 | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.MPM..HHE  | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.DTP..HHZ  | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.WBM..HHE  | 2019-07-04T00:00:00.003100Z - 2019-07-04T23:59:59.963100Z | 25.0 Hz, 2160000 samples\nCI.SRT..HHN  | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nPB.B917..EHZ | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.WVP2..EHZ | 2019-07-04T00:00:00.000000Z - 2019-07-04T23:59:59.960000Z | 25.0 Hz, 2160000 samples\nPB.B917..EH2 | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.LRL..HHN  | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.CCC..HHE  | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.SLA..HHZ  | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.WNM..EHZ  | 2019-07-04T00:00:00.000000Z - 2019-07-04T23:59:59.960000Z | 25.0 Hz, 2160000 samples\nCI.WRC2..HHE | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.JRC2..HHE | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.SRT..HHE  | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.LRL..HHE  | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.MPM..HHN  | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nPB.B921..EHZ | 2019-07-03T23:59:59.998200Z - 2019-07-04T23:59:59.958200Z | 25.0 Hz, 2160000 samples\nCI.WBM..HHN  | 2019-07-04T00:00:00.003100Z - 2019-07-04T23:59:59.963100Z | 25.0 Hz, 2160000 samples\nPB.B921..EH2 | 2019-07-03T23:59:59.998200Z - 2019-07-04T23:59:59.958200Z | 25.0 Hz, 2160000 samples\nCI.WMF..HHE  | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.WCS2..HHZ | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.TOW2..HHE | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.DAW..HHN  | 2019-07-04T00:00:00.003100Z - 2019-07-04T23:59:59.963100Z | 25.0 Hz, 2160000 samples\n</pre> In\u00a0[17]: Copied! <pre>GAP_START_SEC = 3. * 60. * 60. \nGAP_END_SEC = 14. * 60. * 60.\nstation_list = list(set([st.stats.station for st in continuous_seismograms]))\nSTATIONS_W_GAP = np.random.choice(\n    station_list, size=int(3. / 4. * len(station_list)), replace=False\n)\nfor sta in STATIONS_W_GAP:\n    for tr in continuous_seismograms.select(station=sta):\n        gap_start_samp = int(GAP_START_SEC * tr.stats.sampling_rate)\n        gap_end_samp = int(GAP_END_SEC * tr.stats.sampling_rate)\n        tr.data[gap_start_samp:gap_end_samp] = 0.\n</pre> GAP_START_SEC = 3. * 60. * 60.  GAP_END_SEC = 14. * 60. * 60. station_list = list(set([st.stats.station for st in continuous_seismograms])) STATIONS_W_GAP = np.random.choice(     station_list, size=int(3. / 4. * len(station_list)), replace=False ) for sta in STATIONS_W_GAP:     for tr in continuous_seismograms.select(station=sta):         gap_start_samp = int(GAP_START_SEC * tr.stats.sampling_rate)         gap_end_samp = int(GAP_END_SEC * tr.stats.sampling_rate)         tr.data[gap_start_samp:gap_end_samp] = 0. In\u00a0[18]: Copied! <pre># plot the continuous seismograms from a single station\nfig = continuous_seismograms.select(station=\"CLC\").plot()\n</pre> # plot the continuous seismograms from a single station fig = continuous_seismograms.select(station=\"CLC\").plot() In\u00a0[19]: Copied! <pre># then, cast data into `numpy.ndarray`\nstation_codes = list(set([st.stats.station for st in continuous_seismograms]))\ncomponent_codes = [\"N\", \"E\", \"Z\"]\ncomponent_aliases={\"E\": [\"E\", \"2\"], \"N\": [\"N\", \"1\"], \"Z\": [\"Z\"]}\n\nnum_stations = len(station_codes)\nnum_channels = len(component_codes)\nnum_samples = len(continuous_seismograms[0].data)\n\ncontinuous_seismograms_arr = np.zeros((num_stations, num_channels, num_samples), dtype=np.float32)\nfor s, sta in enumerate(station_codes):\n    for c, cp in enumerate(component_codes):\n        for cp_alias in component_aliases[cp]:\n            sel_seismogram = continuous_seismograms.select(station=sta, component=cp_alias)\n            if len(sel_seismogram) &gt; 0:\n                continuous_seismograms_arr[s, c, :] = sel_seismogram[0].data\n                break\n            \ncontinuous_seismograms_arr\n</pre> # then, cast data into `numpy.ndarray` station_codes = list(set([st.stats.station for st in continuous_seismograms])) component_codes = [\"N\", \"E\", \"Z\"] component_aliases={\"E\": [\"E\", \"2\"], \"N\": [\"N\", \"1\"], \"Z\": [\"Z\"]}  num_stations = len(station_codes) num_channels = len(component_codes) num_samples = len(continuous_seismograms[0].data)  continuous_seismograms_arr = np.zeros((num_stations, num_channels, num_samples), dtype=np.float32) for s, sta in enumerate(station_codes):     for c, cp in enumerate(component_codes):         for cp_alias in component_aliases[cp]:             sel_seismogram = continuous_seismograms.select(station=sta, component=cp_alias)             if len(sel_seismogram) &gt; 0:                 continuous_seismograms_arr[s, c, :] = sel_seismogram[0].data                 break              continuous_seismograms_arr  Out[19]: <pre>array([[[-9.8226795e-11,  5.7554624e-11,  3.4072033e-11, ...,\n         -1.8828223e-09,  8.1136459e-10, -1.7122942e-10],\n        [-1.7923078e-10,  9.7317022e-11,  3.3442499e-10, ...,\n          7.6900075e-10,  2.7582496e-09, -1.1529546e-09],\n        [-1.6065722e-10, -2.5988860e-11, -8.8432366e-11, ...,\n          6.2743988e-10,  3.0330730e-10, -1.6875973e-10]],\n\n       [[ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n          0.0000000e+00,  0.0000000e+00,  0.0000000e+00],\n        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n          0.0000000e+00,  0.0000000e+00,  0.0000000e+00],\n        [ 1.8492177e-11,  7.3153331e-11,  1.3191939e-10, ...,\n          4.4108817e-10, -3.0806005e-10, -4.0270010e-10]],\n\n       [[-1.6893502e-11,  8.4201196e-12,  2.7799072e-11, ...,\n         -1.7474208e-10, -4.5014548e-11, -2.6048433e-10],\n        [ 1.4196522e-11,  3.8935088e-12, -2.4043532e-12, ...,\n         -4.9603099e-11, -9.2666923e-11,  2.2229388e-10],\n        [-7.8133723e-12,  1.1610854e-11,  1.2903780e-11, ...,\n         -8.2966682e-11, -9.3903114e-11, -8.1583212e-12]],\n\n       ...,\n\n       [[ 5.4725401e-11,  7.0578925e-11, -6.2566233e-12, ...,\n         -3.6589140e-10, -7.8155921e-10, -6.3879835e-10],\n        [-1.3925512e-11,  2.0290504e-11,  1.0709553e-11, ...,\n          1.0249724e-09,  7.7783147e-10, -4.5876902e-10],\n        [ 1.9426023e-10, -3.3486833e-10, -1.7510682e-10, ...,\n          9.0171864e-10, -9.0758560e-11, -1.7541314e-10]],\n\n       [[-6.0105490e-11, -1.9033368e-09, -2.2194966e-09, ...,\n          9.9305908e-10, -2.5555899e-10, -1.2649487e-09],\n        [ 3.3998053e-09, -9.0392399e-10, -4.9158380e-09, ...,\n         -5.5282345e-10, -3.0974114e-09, -1.6373071e-09],\n        [-6.3921446e-10, -1.2163219e-09, -1.2187444e-09, ...,\n         -1.0883094e-09,  2.2945404e-10,  4.6940152e-10]],\n\n       [[ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n          0.0000000e+00,  0.0000000e+00,  0.0000000e+00],\n        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n          0.0000000e+00,  0.0000000e+00,  0.0000000e+00],\n        [-2.3801865e-11,  4.4190979e-12, -4.3637774e-11, ...,\n         -4.7749610e-11, -1.3966529e-10, -1.5340888e-10]]], dtype=float32)</pre> In\u00a0[20]: Copied! <pre># PHASE_ON_COMP: dictionary defining which moveout we use to extract the waveform.\n#                Here, we use windows centered around the S wave for horizontal components\n#                and windows starting 1sec before the P wave for the vertical component.\nPHASE_ON_COMP = {\"N\": \"S\", \"E\": \"S\", \"Z\": \"P\"}\n# OFFSET_PHASE_SEC: dictionary defining the time offset taken before a given phase\n#               for example OFFSET_PHASE_SEC[\"P\"] = 1.0 means that we extract the window\n#               1 second before the predicted P arrival time\nOFFSET_PHASE_SEC = {\"P\": 1.0, \"S\": 4.0}\n# TEMPLATE_DURATION_SEC\nTEMPLATE_DURATION_SEC = 8. \n# SAMPLING_RATE_HZ\nSAMPLING_RATE_HZ = 25.\n# TEMPLATE_DURATION_SAMP\nTEMPLATE_DURATION_SAMP = int(TEMPLATE_DURATION_SEC * SAMPLING_RATE_HZ)\n</pre> # PHASE_ON_COMP: dictionary defining which moveout we use to extract the waveform. #                Here, we use windows centered around the S wave for horizontal components #                and windows starting 1sec before the P wave for the vertical component. PHASE_ON_COMP = {\"N\": \"S\", \"E\": \"S\", \"Z\": \"P\"} # OFFSET_PHASE_SEC: dictionary defining the time offset taken before a given phase #               for example OFFSET_PHASE_SEC[\"P\"] = 1.0 means that we extract the window #               1 second before the predicted P arrival time OFFSET_PHASE_SEC = {\"P\": 1.0, \"S\": 4.0} # TEMPLATE_DURATION_SEC TEMPLATE_DURATION_SEC = 8.  # SAMPLING_RATE_HZ SAMPLING_RATE_HZ = 25. # TEMPLATE_DURATION_SAMP TEMPLATE_DURATION_SAMP = int(TEMPLATE_DURATION_SEC * SAMPLING_RATE_HZ)  In\u00a0[21]: Copied! <pre># add station_code columns to `selected_event_picks`\nselected_event_picks.set_index(\"station_id\", inplace=True)\nfor staid in selected_event_picks.index:\n    station_code = staid.split(\".\")[1]\n    selected_event_picks.loc[staid, \"station_code\"] = station_code\nselected_event_picks\n</pre> # add station_code columns to `selected_event_picks` selected_event_picks.set_index(\"station_id\", inplace=True) for staid in selected_event_picks.index:     station_code = staid.split(\".\")[1]     selected_event_picks.loc[staid, \"station_code\"] = station_code selected_event_picks <pre>/tmp/ipykernel_2902838/1881751651.py:5: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  selected_event_picks.loc[staid, \"station_code\"] = station_code\n</pre> Out[21]: phase_index phase_time phase_score phase_type dt_s phase_polarity phase_amplitude event_index sp_ratio event_amplitude adloc_mask adloc_residual_time adloc_residual_amplitude station_code station_id CI.CLC..HN 6137848 2019-07-04 17:02:58.488 0.969 P 0.01 -0.879 -0.511873 1101 1.017569 2.474507e-07 1 -0.010048 -0.054426 CLC CI.CLC..BH 6137847 2019-07-04 17:02:58.489 0.633 P 0.01 -0.324 -0.666956 1101 1.014822 2.022085e-07 1 0.048153 -0.127168 CLC CI.CLC..HH 6137849 2019-07-04 17:02:58.498 0.965 P 0.01 -0.883 -0.586030 1101 1.086886 2.141392e-07 1 -0.007135 -0.236796 CLC CI.SRT..HN 6137990 2019-07-04 17:02:59.908 0.879 P 0.01 0.699 -0.643401 1101 1.377537 4.387864e-07 1 -0.069463 0.065039 SRT CI.SRT..HH 6137990 2019-07-04 17:02:59.908 0.891 P 0.01 0.734 -0.601019 1101 0.729745 5.910421e-07 1 -0.078144 0.020251 SRT ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... CI.WMF..HH 6139185 2019-07-04 17:03:11.858 0.672 S 0.01 0.021 -1.368759 1101 1.096820 3.073536e-07 1 0.140179 -0.329117 WMF CI.WMF..HN 6139189 2019-07-04 17:03:11.898 0.633 S 0.01 0.026 -1.371407 1101 0.989939 2.891483e-07 1 0.159858 -0.171350 WMF CI.DTP..BH 6139200 2019-07-04 17:03:12.019 0.734 S 0.01 0.035 -1.322484 1101 1.385395 1.624267e-07 1 0.068940 0.175198 DTP CI.DTP..HH 6139203 2019-07-04 17:03:12.038 0.707 S 0.01 -0.015 -1.306977 1101 1.122809 3.205840e-07 1 0.052824 0.125587 DTP CI.DTP..HN 6139211 2019-07-04 17:03:12.118 0.625 S 0.01 0.043 -1.282829 1101 0.863380 2.702479e-07 1 0.105026 0.211253 DTP <p>90 rows \u00d7 14 columns</p> <p>In the following cell, we build the <code>numpy.ndarray</code> of moveouts $\\tilde{\\tau}_{s,c}$, expressed in units of samples.</p> In\u00a0[22]: Copied! <pre># first, we extract the set of relative delay times of the beginning of each\n# template window on a given station and component\ntau_s_c_sec = np.zeros((num_stations, num_channels), dtype=np.float64)\nfor s, sta in enumerate(station_codes):\n    for c, cp in enumerate(component_codes):\n        phase_type = PHASE_ON_COMP[cp]\n        picks_s_c = selected_event_picks[\n            (\n                (selected_event_picks[\"station_code\"] == sta)\n                &amp; (selected_event_picks[\"phase_type\"] == phase_type)\n            )\n        ]\n        if len(picks_s_c) == 0:\n            # no pick for this station/component: set to -999\n            tau_s_c_sec[s, c] = -999\n        elif len(picks_s_c) == 1:\n            # express pick relative to beginning of day (midnight)\n            _pick = pd.Timestamp(picks_s_c[\"phase_time\"])\n            _relative_pick_sec = (_pick - pd.Timestamp(_pick.strftime(\"%Y-%m-%d\"))).total_seconds()\n            tau_s_c_sec[s, c] = _relative_pick_sec - OFFSET_PHASE_SEC[phase_type]\n        else:\n            # there were several picks from different channels: average them\n            _relative_pick_sec = 0.\n            for _pick in picks_s_c[\"phase_time\"].values:\n                _pick = pd.Timestamp(_pick)\n                _relative_pick_sec += (_pick - pd.Timestamp(_pick.strftime(\"%Y-%m-%d\"))).total_seconds()\n            _relative_pick_sec /= float(len(picks_s_c[\"phase_time\"]))\n            tau_s_c_sec[s, c] = _relative_pick_sec - OFFSET_PHASE_SEC[phase_type]\n# now, we convert these relative times into samples \n# and express them relative to the earliest time\n# we also store in memory the minimum time offset `tau_min_samp` for the next step\nmoveouts_samp_arr = (tau_s_c_sec * SAMPLING_RATE_HZ).astype(np.int64)\ntau_min_samp = np.min(moveouts_samp_arr[moveouts_samp_arr &gt; 0])\nmoveouts_samp_arr = moveouts_samp_arr - tau_min_samp\nmoveouts_samp_arr\n</pre> # first, we extract the set of relative delay times of the beginning of each # template window on a given station and component tau_s_c_sec = np.zeros((num_stations, num_channels), dtype=np.float64) for s, sta in enumerate(station_codes):     for c, cp in enumerate(component_codes):         phase_type = PHASE_ON_COMP[cp]         picks_s_c = selected_event_picks[             (                 (selected_event_picks[\"station_code\"] == sta)                 &amp; (selected_event_picks[\"phase_type\"] == phase_type)             )         ]         if len(picks_s_c) == 0:             # no pick for this station/component: set to -999             tau_s_c_sec[s, c] = -999         elif len(picks_s_c) == 1:             # express pick relative to beginning of day (midnight)             _pick = pd.Timestamp(picks_s_c[\"phase_time\"])             _relative_pick_sec = (_pick - pd.Timestamp(_pick.strftime(\"%Y-%m-%d\"))).total_seconds()             tau_s_c_sec[s, c] = _relative_pick_sec - OFFSET_PHASE_SEC[phase_type]         else:             # there were several picks from different channels: average them             _relative_pick_sec = 0.             for _pick in picks_s_c[\"phase_time\"].values:                 _pick = pd.Timestamp(_pick)                 _relative_pick_sec += (_pick - pd.Timestamp(_pick.strftime(\"%Y-%m-%d\"))).total_seconds()             _relative_pick_sec /= float(len(picks_s_c[\"phase_time\"]))             tau_s_c_sec[s, c] = _relative_pick_sec - OFFSET_PHASE_SEC[phase_type] # now, we convert these relative times into samples  # and express them relative to the earliest time # we also store in memory the minimum time offset `tau_min_samp` for the next step moveouts_samp_arr = (tau_s_c_sec * SAMPLING_RATE_HZ).astype(np.int64) tau_min_samp = np.min(moveouts_samp_arr[moveouts_samp_arr &gt; 0]) moveouts_samp_arr = moveouts_samp_arr - tau_min_samp moveouts_samp_arr Out[22]: <pre>array([[      94,       94,       66],\n       [     160,      160,      105],\n       [-1559399, -1559399, -1559399],\n       [     159,      159,      102],\n       [     272,      272,      171],\n       [     179,      179,      116],\n       [     101,      101,       68],\n       [-1559399, -1559399, -1559399],\n       [      78,       78,       53],\n       [     277,      277,      174],\n       [     225,      225,      140],\n       [       0,        0,       13],\n       [-1559399, -1559399, -1559399],\n       [     102,      102,       64],\n       [-1559399, -1559399, -1559399],\n       [      72,       72,       48],\n       [-1559399, -1559399, -1559399],\n       [     175,      175,      115],\n       [     156,      156,      104],\n       [      91,       91,       62],\n       [     165,      165,      105]])</pre> <p>Next, we use the moveouts, in samples, to clip out the relevant template waveforms from the continuous seismograms.</p> In\u00a0[23]: Copied! <pre>template_waveforms_arr = np.zeros((num_stations, num_channels, TEMPLATE_DURATION_SAMP), dtype=np.float32)\nweights_arr = np.ones((num_stations, num_channels), dtype=np.float32)\n\nfor s, sta in enumerate(station_codes):\n    for c, cp in enumerate(component_codes):\n        if moveouts_samp_arr[s, c] &lt; 0:\n            # no picks were found on this station\n            weights_arr[s, c] = 0.\n            continue\n        starttime = tau_min_samp + moveouts_samp_arr[s, c]\n        endtime = starttime + TEMPLATE_DURATION_SAMP\n        template_waveforms_arr[s, c, :] = continuous_seismograms_arr[s, c, starttime:endtime]\n        if template_waveforms_arr[s, c, :].sum() == 0.:\n            # no data was available on this channel\n            weights_arr[s, c] = 0.\n        \ntemplate_waveforms_arr\n</pre> template_waveforms_arr = np.zeros((num_stations, num_channels, TEMPLATE_DURATION_SAMP), dtype=np.float32) weights_arr = np.ones((num_stations, num_channels), dtype=np.float32)  for s, sta in enumerate(station_codes):     for c, cp in enumerate(component_codes):         if moveouts_samp_arr[s, c] &lt; 0:             # no picks were found on this station             weights_arr[s, c] = 0.             continue         starttime = tau_min_samp + moveouts_samp_arr[s, c]         endtime = starttime + TEMPLATE_DURATION_SAMP         template_waveforms_arr[s, c, :] = continuous_seismograms_arr[s, c, starttime:endtime]         if template_waveforms_arr[s, c, :].sum() == 0.:             # no data was available on this channel             weights_arr[s, c] = 0.          template_waveforms_arr Out[23]: <pre>array([[[-7.95800133e-06,  8.55314720e-05,  2.11978477e-04, ...,\n         -3.52683623e-04,  9.79480028e-05,  1.74769040e-04],\n        [-1.22021929e-05,  4.51964515e-05,  1.12187183e-04, ...,\n         -4.29799955e-04, -1.41114127e-04,  4.94585664e-04],\n        [ 2.51332494e-07,  1.18968394e-07, -9.30824982e-08, ...,\n         -8.89477014e-05,  8.27156691e-05,  1.22664089e-04]],\n\n       [[ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n          0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n          0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n        [ 1.27826226e-07,  1.54385773e-07,  1.52002073e-07, ...,\n          1.35540016e-04, -3.16935242e-04, -1.91862506e-04]],\n\n       [[ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n          0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n          0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n          0.00000000e+00,  0.00000000e+00,  0.00000000e+00]],\n\n       ...,\n\n       [[-1.15102615e-04, -2.08503159e-04, -8.57295672e-05, ...,\n         -3.02342505e-05,  1.90505059e-04,  2.32697013e-04],\n        [-6.95939161e-06, -1.14591476e-05, -4.32817069e-05, ...,\n          1.95618981e-04, -6.08503397e-05, -1.64061450e-04],\n        [-3.15401110e-08,  6.19310697e-07,  1.24642884e-06, ...,\n         -6.75416959e-05,  9.95227219e-06,  9.39235106e-05]],\n\n       [[ 1.31645243e-06,  1.41594983e-05,  1.18550970e-05, ...,\n         -1.61218868e-05,  1.20281387e-04,  7.25206701e-05],\n        [ 1.01139749e-05,  3.06667994e-06,  6.11646101e-07, ...,\n          5.91952994e-04,  5.78140549e-04,  2.96404498e-04],\n        [ 5.31832107e-08,  8.85903688e-08,  1.39000178e-07, ...,\n          3.86358661e-05,  4.55262576e-04,  2.83642818e-04]],\n\n       [[ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n          0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n          0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n        [ 6.11326456e-08,  1.84645728e-07,  2.68150814e-07, ...,\n         -1.40672455e-05,  9.12318046e-06,  1.10272435e-06]]],\n      dtype=float32)</pre> In\u00a0[24]: Copied! <pre># normalize template waveforms for numerical reasons\nnorm = np.std(template_waveforms_arr, axis=-1, keepdims=True)\nnorm[norm == 0.] = 1. \ntemplate_waveforms_arr /= norm\n\n# normalize weights so that they sum up to one\nweights_arr /= np.sum(weights_arr)\n\n# normalize continuous seismograms for numerical reasons\nnorm = np.std(continuous_seismograms_arr, axis=-1, keepdims=True)\nnorm[norm == 0.] = 1. \ncontinuous_seismograms_arr /= norm\n</pre> # normalize template waveforms for numerical reasons norm = np.std(template_waveforms_arr, axis=-1, keepdims=True) norm[norm == 0.] = 1.  template_waveforms_arr /= norm  # normalize weights so that they sum up to one weights_arr /= np.sum(weights_arr)  # normalize continuous seismograms for numerical reasons norm = np.std(continuous_seismograms_arr, axis=-1, keepdims=True) norm[norm == 0.] = 1.  continuous_seismograms_arr /= norm In\u00a0[25]: Copied! <pre># FMF_STEP_SAMP: this is the step between two consecutive calculation of the correlation coefficient\nFMF_STEP_SAMP = 1\n# ARCH: it determines whether you want to use GPUs or CPUs \n#       If you do not have an Nvidia GPU, set ARCH = \"cpu\"\nARCH = \"gpu\"\n</pre> # FMF_STEP_SAMP: this is the step between two consecutive calculation of the correlation coefficient FMF_STEP_SAMP = 1 # ARCH: it determines whether you want to use GPUs or CPUs  #       If you do not have an Nvidia GPU, set ARCH = \"cpu\" ARCH = \"gpu\" In\u00a0[26]: Copied! <pre>cc = fmf.matched_filter(\n    template_waveforms_arr.astype(np.float32),\n    moveouts_samp_arr.astype(np.int32),\n    weights_arr.astype(np.float32),\n    continuous_seismograms_arr.astype(np.float32),\n    FMF_STEP_SAMP,\n    arch=ARCH,\n)\n</pre> cc = fmf.matched_filter(     template_waveforms_arr.astype(np.float32),     moveouts_samp_arr.astype(np.int32),     weights_arr.astype(np.float32),     continuous_seismograms_arr.astype(np.float32),     FMF_STEP_SAMP,     arch=ARCH, ) In\u00a0[27]: Copied! <pre># FMF is programmed to handle multiple templates at once. Here, we only used\n# a single template, hence the size of the outermost axis of \"1\"\ncc.shape\n</pre> # FMF is programmed to handle multiple templates at once. Here, we only used # a single template, hence the size of the outermost axis of \"1\" cc.shape Out[27]: <pre>(1, 2159801)</pre> In\u00a0[28]: Copied! <pre># let's print the output of our template matching run, which a time series of network-averaged correlation coefficients\n# of same duration as the continuous seismograms\n_cc = cc[0, :]\ntime_cc = np.arange(len(_cc)) / SAMPLING_RATE_HZ\n\nfig = plt.figure(\"network_averaged_cc\", figsize=(20, 6))\ngs = fig.add_gridspec(ncols=4)\n\nax1 = fig.add_subplot(gs[:3])\nax1.plot(time_cc, _cc, lw=0.75)\nax1.set_xlabel(\"Elapsed time (sec)\")\nax1.set_ylabel(\"Network-averaged cc\")\nax1.set_xlim(time_cc.min(), time_cc.max())\nax1.set_title(\"Time series of template/continuous seismograms similarity\")\n\nax2 = fig.add_subplot(gs[3], sharey=ax1)\n_ = ax2.hist(_cc, orientation=\"horizontal\", bins=250, density=True)\nax2.set_xlabel(\"Normalized count\")\nax2.set_title(\"Histogram\")\n\nfor ax in [ax1, ax2]:\n    ax.grid()\n</pre> # let's print the output of our template matching run, which a time series of network-averaged correlation coefficients # of same duration as the continuous seismograms _cc = cc[0, :] time_cc = np.arange(len(_cc)) / SAMPLING_RATE_HZ  fig = plt.figure(\"network_averaged_cc\", figsize=(20, 6)) gs = fig.add_gridspec(ncols=4)  ax1 = fig.add_subplot(gs[:3]) ax1.plot(time_cc, _cc, lw=0.75) ax1.set_xlabel(\"Elapsed time (sec)\") ax1.set_ylabel(\"Network-averaged cc\") ax1.set_xlim(time_cc.min(), time_cc.max()) ax1.set_title(\"Time series of template/continuous seismograms similarity\")  ax2 = fig.add_subplot(gs[3], sharey=ax1) _ = ax2.hist(_cc, orientation=\"horizontal\", bins=250, density=True) ax2.set_xlabel(\"Normalized count\") ax2.set_title(\"Histogram\")  for ax in [ax1, ax2]:     ax.grid()  In\u00a0[29]: Copied! <pre>def select_cc_indexes(\n    cc_t,\n    threshold,\n    search_win,\n):\n    \"\"\"Select the peaks in the CC time series.\n\n    Parameters\n    ------------\n    cc_t: (n_corr,) numpy.ndarray\n        The CC time series for one template.\n    threshold: (n_corr,) numpy.ndarray or scalar\n        The detection threshold.\n    search_win: scalar int\n        The minimum inter-event time, in units of correlation step.\n\n\n    Returns\n    --------\n    cc_idx: (n_detections,) numpy.ndarray\n        The list of all selected CC indexes. They give the timings of the\n        detected events.\n    \"\"\"\n\n    cc_detections = cc_t &gt; threshold\n    cc_idx = np.where(cc_detections)[0]\n\n    cc_idx = list(cc_idx)\n    n_rm = 0\n    for i in range(1, len(cc_idx)):\n        if (cc_idx[i - n_rm] - cc_idx[i - n_rm - 1]) &lt; search_win:\n            if cc_t[cc_idx[i - n_rm]] &gt; cc_t[cc_idx[i - n_rm - 1]]:\n                # keep (i-n_rm)-th detection\n                cc_idx.remove(cc_idx[i - n_rm - 1])\n            else:\n                # keep (i-n_rm-1)-th detection\n                cc_idx.remove(cc_idx[i - n_rm])\n            n_rm += 1\n    cc_idx = np.asarray(cc_idx)\n    return cc_idx\n    \n</pre> def select_cc_indexes(     cc_t,     threshold,     search_win, ):     \"\"\"Select the peaks in the CC time series.      Parameters     ------------     cc_t: (n_corr,) numpy.ndarray         The CC time series for one template.     threshold: (n_corr,) numpy.ndarray or scalar         The detection threshold.     search_win: scalar int         The minimum inter-event time, in units of correlation step.       Returns     --------     cc_idx: (n_detections,) numpy.ndarray         The list of all selected CC indexes. They give the timings of the         detected events.     \"\"\"      cc_detections = cc_t &gt; threshold     cc_idx = np.where(cc_detections)[0]      cc_idx = list(cc_idx)     n_rm = 0     for i in range(1, len(cc_idx)):         if (cc_idx[i - n_rm] - cc_idx[i - n_rm - 1]) &lt; search_win:             if cc_t[cc_idx[i - n_rm]] &gt; cc_t[cc_idx[i - n_rm - 1]]:                 # keep (i-n_rm)-th detection                 cc_idx.remove(cc_idx[i - n_rm - 1])             else:                 # keep (i-n_rm-1)-th detection                 cc_idx.remove(cc_idx[i - n_rm])             n_rm += 1     cc_idx = np.asarray(cc_idx)     return cc_idx      In\u00a0[30]: Copied! <pre># INTEREVENT_TIME_RESOLUTION_SEC: In some cases, a template might trigger multiple, closely spaced detections because\n#                                 of a phenomenon similar to that of \"cycle skipping\", where the waveform correlates\n#                                 well with a time-shifted version of itself. Thus, to avoid redundant detections, we\n#                                 set a minimum time separation between triggers (rule of thumb: about half the template duration)\nINTEREVENT_TIME_RESOLUTION_SEC = 5.\nINTEREVENT_TIME_RESOLUTION_SAMP = int(INTEREVENT_TIME_RESOLUTION_SEC * SAMPLING_RATE_HZ)\n_cc = cc[0, :]\ntime_cc = np.arange(len(_cc)) * FMF_STEP_SAMP / SAMPLING_RATE_HZ\nNUM_RMS = 8.\ndetection_threshold = NUM_RMS * np.std(_cc)\nevent_cc_indexes = select_cc_indexes(_cc, detection_threshold, INTEREVENT_TIME_RESOLUTION_SAMP)\n\nfig = plt.figure(\"network_averaged_cc\", figsize=(20, 6))\ngs = fig.add_gridspec(ncols=4)\n\nax1 = fig.add_subplot(gs[:3])\nax1.plot(time_cc, _cc, lw=0.75)\nax1.scatter(time_cc[event_cc_indexes], _cc[event_cc_indexes], linewidths=0.25, edgecolor=\"k\", color=\"r\", zorder=2)\nax1.set_xlabel(\"Elapsed time (sec)\")\nax1.set_ylabel(\"Network-averaged cc\")\nax1.set_xlim(time_cc.min(), time_cc.max())\nax1.set_title(\"Time series of template/continuous seismograms similarity\")\n\nax2 = fig.add_subplot(gs[3], sharey=ax1)\n_ = ax2.hist(_cc, orientation=\"horizontal\", bins=250, density=True, zorder=2)\nax2.set_xlabel(\"Normalized count\")\nax2.set_title(\"Histogram\")\n\nlabel = f\"Detection threshold: {NUM_RMS:.0f}\"r\"$\\times \\mathrm{RMS}(\\mathrm{CC}(t))$\"f\"\\n{len(event_cc_indexes):d} detected events\"\nfor ax in [ax1, ax2]:\n    ax.grid()\n    ax.axhline(\n        detection_threshold, ls=\"--\", color=\"r\",\n        label=label\n        )\nax1.legend(loc=\"upper left\")\n</pre> # INTEREVENT_TIME_RESOLUTION_SEC: In some cases, a template might trigger multiple, closely spaced detections because #                                 of a phenomenon similar to that of \"cycle skipping\", where the waveform correlates #                                 well with a time-shifted version of itself. Thus, to avoid redundant detections, we #                                 set a minimum time separation between triggers (rule of thumb: about half the template duration) INTEREVENT_TIME_RESOLUTION_SEC = 5. INTEREVENT_TIME_RESOLUTION_SAMP = int(INTEREVENT_TIME_RESOLUTION_SEC * SAMPLING_RATE_HZ) _cc = cc[0, :] time_cc = np.arange(len(_cc)) * FMF_STEP_SAMP / SAMPLING_RATE_HZ NUM_RMS = 8. detection_threshold = NUM_RMS * np.std(_cc) event_cc_indexes = select_cc_indexes(_cc, detection_threshold, INTEREVENT_TIME_RESOLUTION_SAMP)  fig = plt.figure(\"network_averaged_cc\", figsize=(20, 6)) gs = fig.add_gridspec(ncols=4)  ax1 = fig.add_subplot(gs[:3]) ax1.plot(time_cc, _cc, lw=0.75) ax1.scatter(time_cc[event_cc_indexes], _cc[event_cc_indexes], linewidths=0.25, edgecolor=\"k\", color=\"r\", zorder=2) ax1.set_xlabel(\"Elapsed time (sec)\") ax1.set_ylabel(\"Network-averaged cc\") ax1.set_xlim(time_cc.min(), time_cc.max()) ax1.set_title(\"Time series of template/continuous seismograms similarity\")  ax2 = fig.add_subplot(gs[3], sharey=ax1) _ = ax2.hist(_cc, orientation=\"horizontal\", bins=250, density=True, zorder=2) ax2.set_xlabel(\"Normalized count\") ax2.set_title(\"Histogram\")  label = f\"Detection threshold: {NUM_RMS:.0f}\"r\"$\\times \\mathrm{RMS}(\\mathrm{CC}(t))$\"f\"\\n{len(event_cc_indexes):d} detected events\" for ax in [ax1, ax2]:     ax.grid()     ax.axhline(         detection_threshold, ls=\"--\", color=\"r\",         label=label         ) ax1.legend(loc=\"upper left\") Out[30]: <pre>&lt;matplotlib.legend.Legend at 0x7fae4e49bb20&gt;</pre> <p>The time variation of the standard deviations of $CC(t)$ caused by gaps in some of the stations may lower the detection threshold and may thus trigger many false detections. In general, it's better to use a time-dependent detection threshold to adapt to possible gaps in the data. When using template matching on smaller seismic networks than in this example, a gap in a single station may strongly affect your time series $CC(t)$!</p> <p>Interested in a bullet-proof time-dependent threshold? Check out the link below: https://ebeauce.github.io/Seismic_BPMF/usage/api/similarity_search.html#BPMF.similarity_search.time_dependent_threshold</p>"},{"location":"notebooks/tm_issues_w_detection_threshold/#demonstration-of-a-common-issue-encountered-in-template-matching","title":"Demonstration of a common issue encountered in template matching\u00b6","text":"<p>Templates are selected from Weiqiang Zhu's PhaseNet catalog.</p> <p>Download the seismic data at: https://doi.org/10.5281/zenodo.15097180</p> <p></p>"},{"location":"notebooks/tm_issues_w_detection_threshold/#load-phasenet-catalog","title":"Load PhaseNet catalog\u00b6","text":"<p>Here, we read the catalog of the 2019 Ridgecrest sequence made with PhaseNet. Information is divided into three files:</p> <ul> <li>a station metadata file,</li> <li>an event metadata file (the catalog per se),</li> <li>a pick database, which contains all the P- and S-wave picks found by PhaseNet.</li> </ul>"},{"location":"notebooks/tm_issues_w_detection_threshold/#pick-one-event","title":"Pick one event\u00b6","text":"<p>Let's use the PhaseNet catalog to read the waveforms of an event.</p>"},{"location":"notebooks/tm_issues_w_detection_threshold/#run-template-matching","title":"Run template matching\u00b6","text":"<p>We will now use one of the events from the PhaseNet catalog as a template event to detect events with template matching.</p>"},{"location":"notebooks/tm_issues_w_detection_threshold/#read-data-from-same-day","title":"Read data from same day\u00b6","text":""},{"location":"notebooks/tm_issues_w_detection_threshold/#introduce-gaps-in-some-stations","title":"Introduce gaps in some stations\u00b6","text":""},{"location":"notebooks/tm_issues_w_detection_threshold/#build-template","title":"Build template\u00b6","text":""},{"location":"notebooks/tm_issues_w_detection_threshold/#-background-","title":"------------------ Background ------------------\u00b6","text":"<p>A template is a collection of waveforms at different channels, $T_{s,c}(t)$, which are clips taken from the continuous seismograms, $u_{s,c}$. These clips are taken at times defined by: $$ u_{s,c}(t)\\ |\\ t \\in \\lbrace \\tau_{s,c}; \\tau_{s,c} + D \\rbrace, $$ where $\\tau_{s,c}$ is the start time of the template window and $D$ is the template duration.</p> <p>$\\tau_{s,c}$ is given by some prior information on the event: picks or modeled arrival times. The moveouts, $\\tilde{\\tau}_{s,c}$, are the collection of delay times relative to the earliest $\\tau_{s,c}$: $$ \\tilde{\\tau}_{s,c} = \\tau_{s,c} - \\underset{s,c}{\\min} \\lbrace \\tau_{s,c} \\rbrace .$$</p>"},{"location":"notebooks/tm_issues_w_detection_threshold/#-on-the-necessity-to-clip-template-waveforms-out-of-numpyndarray-instead-of-obspystream-","title":"------------------ On the necessity to clip template waveforms out of <code>numpy.ndarray</code> instead of <code>obspy.Stream</code> ------------------\u00b6","text":"<p>Looking carefully at the output of <code>print(continuous_seismograms.__str__(extended=True))</code>, a few cells before, we see that start times are generally not exactly at midnight. This is a consequence of the discrete nature of the continuous seismograms (here, sampled at 25 samples per second). Thus, in general, the $\\tau_{s,c}$ computed from picks or modeled arrival times fall in between two samples of the seismograms.</p> <p>When running a matched-filter search, we need to make sure the moveouts, $\\tilde{\\tau}_{s,c}$, ultimately expressed in samples, match exactly the times that were used when clipping the template waveforms out of $u_{s,c}$. One way to ensure this is to first cast the $\\tau_{s,c}$ to times in samples and then operate exclusively on the <code>numpy.ndarray</code>: <code>continuous_seismograms_arr</code>:</p> <p>$$ T_{s,c}[t_n] = u_{s,c}[\\tau_{s,c} + n \\Delta t],$$ where $\\Delta t$ is the sampling time.</p>"},{"location":"notebooks/tm_issues_w_detection_threshold/#clip-out-waveforms-and-moveout-and-station-weight-arrays","title":"Clip out waveforms and moveout and station-weight arrays\u00b6","text":""},{"location":"notebooks/tm_issues_w_detection_threshold/#run-fmf","title":"Run FMF\u00b6","text":"<p>After all this data formatting, we can now run template matching (also called matched-filtering) to detect new events that are similar to our template event.</p> <p>For that, use the software Fast Matched Filter (FMF): https://github.com/beridel/fast_matched_filter</p> <p>FMF offers C and CUDA-C routines to efficiently run template matching on CPUs, or even on GPUs if available to you.</p>"},{"location":"notebooks/tm_issues_w_detection_threshold/#set-detection-threshold-and-find-events","title":"Set detection threshold and find events\u00b6","text":"<p>We will use the time series of correlation coefficients to build an earthquake catalog. For that, we need to set a detection threshold and define all times above that threshold as triggers caused by near-repeats of the template event.</p>"},{"location":"notebooks/tm_multiple_templates/","title":"Tm multiple templates","text":"In\u00a0[1]: Copied! <pre>import os\nimport fast_matched_filter as fmf\nimport glob\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport obspy as obs\nimport pandas as pd\n</pre> import os import fast_matched_filter as fmf import glob import numpy as np import matplotlib.pyplot as plt import obspy as obs import pandas as pd  In\u00a0[2]: Copied! <pre># path variables and file names\nDIR_WAVEFORMS = \"/home/ebeauce/SSA_EQ_DETECTION_WORKSHOP/data\" # REPLACE THIS PATH WITH WHEREVER YOU DOWNLOADED THE DATA\nDIR_CATALOG = \"../picks_phasenet/\"\n\nSTATION_FILE = \"adloc_stations.csv\"\nEVENT_FILE = \"adloc_events.csv\"\nPICK_FILE = \"adloc_picks.csv\"\n</pre> # path variables and file names DIR_WAVEFORMS = \"/home/ebeauce/SSA_EQ_DETECTION_WORKSHOP/data\" # REPLACE THIS PATH WITH WHEREVER YOU DOWNLOADED THE DATA DIR_CATALOG = \"../picks_phasenet/\"  STATION_FILE = \"adloc_stations.csv\" EVENT_FILE = \"adloc_events.csv\" PICK_FILE = \"adloc_picks.csv\" In\u00a0[3]: Copied! <pre>station_meta = pd.read_csv(os.path.join(DIR_CATALOG, STATION_FILE))\n</pre> station_meta = pd.read_csv(os.path.join(DIR_CATALOG, STATION_FILE)) <p>The following shows a very rudimentary map of the station network. Look into the <code>cartopy</code> package for more sophisticated maps.</p> In\u00a0[4]: Copied! <pre>_station_meta = station_meta.drop_duplicates(\"station\")\n\nfig, ax = plt.subplots(num=\"station_network\", figsize=(10, 10))\nax.scatter(_station_meta[\"longitude\"], _station_meta[\"latitude\"], marker=\"v\", color=\"k\")\nfor idx, row in _station_meta.iterrows():\n    ax.text(row.longitude + 0.01, row.latitude + 0.01, row.station, va=\"bottom\", ha=\"left\")\nax.set_xlabel(\"Longitude\")\nax.set_ylabel(\"Latitude\")\nax.grid()\nax.set_title(\"Stations used to build the PhaseNet catalog\")\n</pre> _station_meta = station_meta.drop_duplicates(\"station\")  fig, ax = plt.subplots(num=\"station_network\", figsize=(10, 10)) ax.scatter(_station_meta[\"longitude\"], _station_meta[\"latitude\"], marker=\"v\", color=\"k\") for idx, row in _station_meta.iterrows():     ax.text(row.longitude + 0.01, row.latitude + 0.01, row.station, va=\"bottom\", ha=\"left\") ax.set_xlabel(\"Longitude\") ax.set_ylabel(\"Latitude\") ax.grid() ax.set_title(\"Stations used to build the PhaseNet catalog\") Out[4]: <pre>Text(0.5, 1.0, 'Stations used to build the PhaseNet catalog')</pre> In\u00a0[5]: Copied! <pre>event_meta = pd.read_csv(os.path.join(DIR_CATALOG, EVENT_FILE))\n</pre> event_meta = pd.read_csv(os.path.join(DIR_CATALOG, EVENT_FILE)) In\u00a0[6]: Copied! <pre>picks = pd.read_csv(os.path.join(DIR_CATALOG, PICK_FILE))\n</pre> picks = pd.read_csv(os.path.join(DIR_CATALOG, PICK_FILE)) In\u00a0[7]: Copied! <pre>def fetch_day_waveforms(dir_waveforms):\n    \"\"\"\n    Fetches the continuous seismograms for a given day.\n\n    Parameters\n    ----------\n    dir_waveforms : str\n        Directory where the waveform data is stored, by default DIR_WAVEFORMS.\n\n    Returns\n    -------\n    obspy.Stream\n        Stream object containing the fetched continuous seismograms.\n    \"\"\"\n    stream = obs.Stream()\n    files = glob.glob(os.path.join(dir_waveforms, \"*mseed\"))\n    for _file in files:\n        stream += obs.read(_file)\n    return stream\n</pre> def fetch_day_waveforms(dir_waveforms):     \"\"\"     Fetches the continuous seismograms for a given day.      Parameters     ----------     dir_waveforms : str         Directory where the waveform data is stored, by default DIR_WAVEFORMS.      Returns     -------     obspy.Stream         Stream object containing the fetched continuous seismograms.     \"\"\"     stream = obs.Stream()     files = glob.glob(os.path.join(dir_waveforms, \"*mseed\"))     for _file in files:         stream += obs.read(_file)     return stream In\u00a0[8]: Copied! <pre># first, read the continuous seismograms into an `obspy.Stream`\ncontinuous_seismograms = fetch_day_waveforms(DIR_WAVEFORMS)\nprint(continuous_seismograms.__str__(extended=True))\n</pre> # first, read the continuous seismograms into an `obspy.Stream` continuous_seismograms = fetch_day_waveforms(DIR_WAVEFORMS) print(continuous_seismograms.__str__(extended=True)) <pre>57 Trace(s) in Stream:\nCI.WCS2..HHE | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nPB.B916..EHZ | 2019-07-03T23:59:59.998200Z - 2019-07-04T23:59:59.958200Z | 25.0 Hz, 2160000 samples\nCI.CLC..HHN  | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.WRV2..EHZ | 2019-07-04T00:00:00.000000Z - 2019-07-04T23:59:59.960000Z | 25.0 Hz, 2160000 samples\nPB.B916..EH2 | 2019-07-03T23:59:59.998200Z - 2019-07-04T23:59:59.958200Z | 25.0 Hz, 2160000 samples\nPB.B917..EH1 | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.WMF..HHZ  | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nPB.B918..EHZ | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nPB.B918..EH2 | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.TOW2..HHZ | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.SLA..HHE  | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.WRC2..HHZ | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.JRC2..HHZ | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.CCC..HHZ  | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.DTP..HHN  | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.SRT..HHZ  | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.LRL..HHZ  | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.SLA..HHN  | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.MPM..HHZ  | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.DTP..HHE  | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.WBM..HHZ  | 2019-07-04T00:00:00.003100Z - 2019-07-04T23:59:59.963100Z | 25.0 Hz, 2160000 samples\nCI.WCS2..HHN | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.CLC..HHE  | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nPB.B921..EH1 | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.DAW..HHZ  | 2019-07-04T00:00:00.003100Z - 2019-07-04T23:59:59.963100Z | 25.0 Hz, 2160000 samples\nCI.CLC..HHZ  | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.WMF..HHN  | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.TOW2..HHN | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.DAW..HHE  | 2019-07-04T00:00:00.003100Z - 2019-07-04T23:59:59.963100Z | 25.0 Hz, 2160000 samples\nPB.B918..EH1 | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.WRC2..HHN | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.JRC2..HHN | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.CCC..HHN  | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nPB.B916..EH1 | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.MPM..HHE  | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.DTP..HHZ  | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.WBM..HHE  | 2019-07-04T00:00:00.003100Z - 2019-07-04T23:59:59.963100Z | 25.0 Hz, 2160000 samples\nCI.SRT..HHN  | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nPB.B917..EHZ | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.WVP2..EHZ | 2019-07-04T00:00:00.000000Z - 2019-07-04T23:59:59.960000Z | 25.0 Hz, 2160000 samples\nPB.B917..EH2 | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.LRL..HHN  | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.CCC..HHE  | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.SLA..HHZ  | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.WNM..EHZ  | 2019-07-04T00:00:00.000000Z - 2019-07-04T23:59:59.960000Z | 25.0 Hz, 2160000 samples\nCI.WRC2..HHE | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.JRC2..HHE | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.SRT..HHE  | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.LRL..HHE  | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.MPM..HHN  | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nPB.B921..EHZ | 2019-07-03T23:59:59.998200Z - 2019-07-04T23:59:59.958200Z | 25.0 Hz, 2160000 samples\nCI.WBM..HHN  | 2019-07-04T00:00:00.003100Z - 2019-07-04T23:59:59.963100Z | 25.0 Hz, 2160000 samples\nPB.B921..EH2 | 2019-07-03T23:59:59.998200Z - 2019-07-04T23:59:59.958200Z | 25.0 Hz, 2160000 samples\nCI.WMF..HHE  | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.WCS2..HHZ | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.TOW2..HHE | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.DAW..HHN  | 2019-07-04T00:00:00.003100Z - 2019-07-04T23:59:59.963100Z | 25.0 Hz, 2160000 samples\n</pre> In\u00a0[9]: Copied! <pre># plot the continuous seismograms from a single station\nfig = continuous_seismograms.select(station=\"CLC\").plot()\n</pre> # plot the continuous seismograms from a single station fig = continuous_seismograms.select(station=\"CLC\").plot() In\u00a0[10]: Copied! <pre># then, cast data into `numpy.ndarray`\nstation_codes = list(set([st.stats.station for st in continuous_seismograms]))\ncomponent_codes = [\"N\", \"E\", \"Z\"]\ncomponent_aliases={\"E\": [\"E\", \"2\"], \"N\": [\"N\", \"1\"], \"Z\": [\"Z\"]}\n\nnum_stations = len(station_codes)\nnum_channels = len(component_codes)\nnum_samples = len(continuous_seismograms[0].data)\n\ncontinuous_seismograms_arr = np.zeros((num_stations, num_channels, num_samples), dtype=np.float32)\nfor s, sta in enumerate(station_codes):\n    for c, cp in enumerate(component_codes):\n        for cp_alias in component_aliases[cp]:\n            sel_seismogram = continuous_seismograms.select(station=sta, component=cp_alias)\n            if len(sel_seismogram) &gt; 0:\n                continuous_seismograms_arr[s, c, :] = sel_seismogram[0].data\n                break\n            \ncontinuous_seismograms_arr\n</pre> # then, cast data into `numpy.ndarray` station_codes = list(set([st.stats.station for st in continuous_seismograms])) component_codes = [\"N\", \"E\", \"Z\"] component_aliases={\"E\": [\"E\", \"2\"], \"N\": [\"N\", \"1\"], \"Z\": [\"Z\"]}  num_stations = len(station_codes) num_channels = len(component_codes) num_samples = len(continuous_seismograms[0].data)  continuous_seismograms_arr = np.zeros((num_stations, num_channels, num_samples), dtype=np.float32) for s, sta in enumerate(station_codes):     for c, cp in enumerate(component_codes):         for cp_alias in component_aliases[cp]:             sel_seismogram = continuous_seismograms.select(station=sta, component=cp_alias)             if len(sel_seismogram) &gt; 0:                 continuous_seismograms_arr[s, c, :] = sel_seismogram[0].data                 break              continuous_seismograms_arr  Out[10]: <pre>array([[[-2.81516824e-11, -1.62091642e-11, -3.07591973e-11, ...,\n         -1.93960692e-09, -1.66790762e-10, -2.08012232e-10],\n        [-1.96374618e-11, -2.01286435e-11, -1.94617239e-11, ...,\n          5.53151309e-11, -1.17186094e-09,  3.09513287e-12],\n        [ 4.61782036e-11,  5.02745345e-11,  5.45427419e-11, ...,\n         -7.20257021e-10,  1.28682023e-10,  4.89559338e-10]],\n\n       [[ 1.00288475e-11, -1.25283534e-11, -8.96174020e-12, ...,\n          8.97525543e-12, -6.86870838e-11, -2.98185018e-11],\n        [ 1.08289614e-12,  8.16112889e-12, -1.36335960e-11, ...,\n          4.46155092e-11,  7.22212498e-11, -3.26238862e-11],\n        [-7.30912639e-12,  1.16460912e-11,  5.44300976e-12, ...,\n          2.24157377e-11,  3.17579747e-11, -6.67901151e-11]],\n\n       [[-8.79721851e-11, -1.08885283e-10, -6.09709019e-11, ...,\n         -4.25245117e-10, -7.53284768e-10, -4.52223925e-10],\n        [ 1.25682673e-10,  7.28235354e-12, -6.59637969e-11, ...,\n          5.21759413e-10,  6.44526155e-10, -4.78027895e-10],\n        [-4.39537504e-11, -3.70419424e-11,  2.50678315e-11, ...,\n         -4.25644159e-10, -4.07093415e-10,  3.00977215e-10]],\n\n       ...,\n\n       [[ 4.53147459e-12,  2.15497811e-11,  4.97441671e-12, ...,\n          1.37320554e-11, -8.23028538e-12,  2.90706681e-10],\n        [-2.32118665e-11, -7.40775150e-12,  4.76967381e-12, ...,\n         -5.22246857e-10, -1.66053525e-11,  4.45412207e-10],\n        [-6.16842992e-12,  4.56698828e-12,  1.07985522e-11, ...,\n          1.94127353e-10,  1.03605340e-10, -3.92332278e-10]],\n\n       [[-2.02754636e-11,  7.27350949e-12,  2.90683051e-11, ...,\n         -1.27593561e-10,  4.12396610e-11,  2.02138389e-10],\n        [-8.89091231e-12,  1.46336155e-11,  4.67565708e-11, ...,\n         -1.25822366e-10,  5.78710552e-11,  4.38750390e-11],\n        [ 5.92738358e-11,  3.67502244e-11, -1.45799206e-10, ...,\n         -4.12412986e-11,  2.63505495e-10, -1.57162269e-11]],\n\n       [[-1.68935022e-11,  8.42011964e-12,  2.77990721e-11, ...,\n         -1.74742082e-10, -4.50145476e-11, -2.60484329e-10],\n        [ 1.41965224e-11,  3.89350878e-12, -2.40435319e-12, ...,\n         -4.96030994e-11, -9.26669227e-11,  2.22293878e-10],\n        [-7.81337234e-12,  1.16108538e-11,  1.29037796e-11, ...,\n         -8.29666821e-11, -9.39031145e-11, -8.15832124e-12]]],\n      dtype=float32)</pre> In\u00a0[11]: Copied! <pre>def fetch_event_waveforms(\n    event_picks,\n    folder=\"preprocessed_2_12\",\n    dir_waveforms=DIR_WAVEFORMS,\n    time_before_phase_onset_sec=2.0,\n    duration_sec=10.0\n    ):\n    \"\"\"\n    Fetches the waveforms for a given event based on the picks.\n\n    Parameters\n    ----------\n    event_picks : pandas.DataFrame\n        DataFrame containing the picks for the event.\n    folder : str, optional\n        Folder name where the preprocessed waveforms are stored, by default \"preprocessed_2_12\".\n    dir_waveforms : str, optional\n        Directory where the waveform data is stored, by default DIR_WAVEFORMS.\n    time_before_phase_onset_sec : float, optional\n        Time in seconds to start the waveform before the phase onset, by default 2.0.\n    duration_sec : float, optional\n        Duration in seconds of the waveform to fetch, by default 10.0.\n\n    Returns\n    -------\n    obspy.Stream\n        Stream object containing the fetched waveforms.\n    \"\"\"\n    date = pd.Timestamp(event_picks.iloc[0][\"phase_time\"]).strftime(\"%Y-%m-%d\")\n    # full path to waveform directory for this given day\n    dir_data = os.path.join(dir_waveforms, date.replace(\"-\", \"\"), folder)\n    stream = obs.Stream()\n    for _, pick in event_picks.iterrows():\n        # check whether we have a miniseed file for this waveform\n        if pick.phase_type == \"P\":\n            files = glob.glob(os.path.join(dir_data, pick.station_id + \"Z*mseed\"))\n        elif pick.phase_type == \"S\":\n            files = glob.glob(os.path.join(dir_data, pick.station_id + \"[N,E]*mseed\"))\n        starttime = obs.UTCDateTime(pick.phase_time) - time_before_phase_onset_sec\n        endtime = starttime + duration_sec\n        for _file in files:\n            stream += obs.read(\n                _file,\n                starttime=starttime,\n                endtime=endtime\n            )\n    return stream\n    \n</pre> def fetch_event_waveforms(     event_picks,     folder=\"preprocessed_2_12\",     dir_waveforms=DIR_WAVEFORMS,     time_before_phase_onset_sec=2.0,     duration_sec=10.0     ):     \"\"\"     Fetches the waveforms for a given event based on the picks.      Parameters     ----------     event_picks : pandas.DataFrame         DataFrame containing the picks for the event.     folder : str, optional         Folder name where the preprocessed waveforms are stored, by default \"preprocessed_2_12\".     dir_waveforms : str, optional         Directory where the waveform data is stored, by default DIR_WAVEFORMS.     time_before_phase_onset_sec : float, optional         Time in seconds to start the waveform before the phase onset, by default 2.0.     duration_sec : float, optional         Duration in seconds of the waveform to fetch, by default 10.0.      Returns     -------     obspy.Stream         Stream object containing the fetched waveforms.     \"\"\"     date = pd.Timestamp(event_picks.iloc[0][\"phase_time\"]).strftime(\"%Y-%m-%d\")     # full path to waveform directory for this given day     dir_data = os.path.join(dir_waveforms, date.replace(\"-\", \"\"), folder)     stream = obs.Stream()     for _, pick in event_picks.iterrows():         # check whether we have a miniseed file for this waveform         if pick.phase_type == \"P\":             files = glob.glob(os.path.join(dir_data, pick.station_id + \"Z*mseed\"))         elif pick.phase_type == \"S\":             files = glob.glob(os.path.join(dir_data, pick.station_id + \"[N,E]*mseed\"))         starttime = obs.UTCDateTime(pick.phase_time) - time_before_phase_onset_sec         endtime = starttime + duration_sec         for _file in files:             stream += obs.read(                 _file,                 starttime=starttime,                 endtime=endtime             )     return stream      In\u00a0[12]: Copied! <pre># select events based on magnitude and origin time\nselected_events_meta = event_meta[\n    (event_meta[\"magnitude\"] &gt; 3.0)\n    &amp; (event_meta[\"magnitude\"] &lt; 5.0)\n    &amp; (event_meta[\"time\"] &gt;= \"2019-07-04\")\n    &amp; (event_meta[\"time\"] &lt; \"2019-07-05\")\n    ]\nnum_templates = len(selected_events_meta)\nselected_events_meta\n</pre> # select events based on magnitude and origin time selected_events_meta = event_meta[     (event_meta[\"magnitude\"] &gt; 3.0)     &amp; (event_meta[\"magnitude\"] &lt; 5.0)     &amp; (event_meta[\"time\"] &gt;= \"2019-07-04\")     &amp; (event_meta[\"time\"] &lt; \"2019-07-05\")     ] num_templates = len(selected_events_meta) selected_events_meta Out[12]: time adloc_score adloc_residual_time num_picks magnitude adloc_residual_amplitude event_index longitude latitude depth_km 16 2019-07-04 17:02:55.057058245 0.770696 0.079675 90 4.476724 0.143489 1101 -117.495094 35.711607 13.586411 34 2019-07-04 17:35:01.312626662 0.789438 0.104771 70 4.829068 0.178912 1348 -117.557665 35.647086 11.567451 35 2019-07-04 17:35:52.045665172 0.659257 0.147855 58 4.382361 0.178848 4325 -117.496379 35.694142 8.195642 36 2019-07-04 17:36:58.761904132 0.874038 0.054701 13 3.358073 0.088606 20605 -117.553046 35.741222 9.369735 37 2019-07-04 17:37:14.796448320 0.757529 0.097453 46 4.123699 0.179097 9324 -117.514428 35.666419 8.910110 ... ... ... ... ... ... ... ... ... ... ... 876 2019-07-04 23:12:40.040009915 0.755675 0.060584 89 3.688291 0.176078 17 -117.462513 35.673782 12.185880 877 2019-07-04 23:13:04.722581442 0.864683 0.051126 86 3.107446 0.142655 3543 -117.574288 35.751324 9.233525 928 2019-07-04 23:34:02.133079438 0.785515 0.112390 58 4.179479 0.273368 4452 -117.600550 35.602102 11.011717 947 2019-07-04 23:42:33.589915803 0.848159 0.085900 89 3.727230 0.196362 190 -117.563485 35.753536 9.027394 973 2019-07-04 23:54:36.296744929 0.649527 0.118062 85 3.448722 0.273801 573 -117.510329 35.658466 9.482599 <p>91 rows \u00d7 10 columns</p> <p>In general, when handling a database of template events, it is convenient to keep track of a unique template id for each template, which may be anything. However, template indexes in <code>numpy.ndarray</code> will go from 0 to <code>num_templates</code> - 1.</p> In\u00a0[13]: Copied! <pre>template_ids = pd.Series(selected_events_meta[\"event_index\"].values, name=\"template_id\")\ntemplate_ids\n</pre> template_ids = pd.Series(selected_events_meta[\"event_index\"].values, name=\"template_id\") template_ids Out[13]: <pre>0      1101\n1      1348\n2      4325\n3     20605\n4      9324\n      ...  \n86       17\n87     3543\n88     4452\n89      190\n90      573\nName: template_id, Length: 91, dtype: int64</pre> In\u00a0[14]: Copied! <pre># for example, the id of the template indexed by 3 is:\ntemplate_ids.iloc[3]\n</pre> # for example, the id of the template indexed by 3 is: template_ids.iloc[3] Out[14]: <pre>20605</pre> In\u00a0[15]: Copied! <pre># PHASE_ON_COMP: dictionary defining which moveout we use to extract the waveform.\n#                Here, we use windows centered around the S wave for horizontal components\n#                and windows starting 1sec before the P wave for the vertical component.\nPHASE_ON_COMP = {\"N\": \"S\", \"E\": \"S\", \"Z\": \"P\"}\n# OFFSET_PHASE_SEC: dictionary defining the time offset taken before a given phase\n#               for example OFFSET_PHASE_SEC[\"P\"] = 1.0 means that we extract the window\n#               1 second before the predicted P arrival time\nOFFSET_PHASE_SEC = {\"P\": 1.0, \"S\": 4.0}\n# TEMPLATE_DURATION_SEC\nTEMPLATE_DURATION_SEC = 8. \n# SAMPLING_RATE_HZ\nSAMPLING_RATE_HZ = 25.\n# TEMPLATE_DURATION_SAMP\nTEMPLATE_DURATION_SAMP = int(TEMPLATE_DURATION_SEC * SAMPLING_RATE_HZ)\n</pre> # PHASE_ON_COMP: dictionary defining which moveout we use to extract the waveform. #                Here, we use windows centered around the S wave for horizontal components #                and windows starting 1sec before the P wave for the vertical component. PHASE_ON_COMP = {\"N\": \"S\", \"E\": \"S\", \"Z\": \"P\"} # OFFSET_PHASE_SEC: dictionary defining the time offset taken before a given phase #               for example OFFSET_PHASE_SEC[\"P\"] = 1.0 means that we extract the window #               1 second before the predicted P arrival time OFFSET_PHASE_SEC = {\"P\": 1.0, \"S\": 4.0} # TEMPLATE_DURATION_SEC TEMPLATE_DURATION_SEC = 8.  # SAMPLING_RATE_HZ SAMPLING_RATE_HZ = 25. # TEMPLATE_DURATION_SAMP TEMPLATE_DURATION_SAMP = int(TEMPLATE_DURATION_SEC * SAMPLING_RATE_HZ)  <p>In the following cell, we build the <code>numpy.ndarray</code> of moveouts $\\tilde{\\tau}_{s,c}$, expressed in units of samples.</p> In\u00a0[16]: Copied! <pre># first, we extract the set of relative delay times of the beginning of each\n# template window on a given station and component\nmoveouts_samp_arr = np.zeros((num_templates, num_stations, num_channels), dtype=np.int64)\ntau_min_samp_arr = np.zeros(num_templates, dtype=np.int64)\nfor t, tid in enumerate(template_ids):\n    # add station_code columns to `selected_event_picks`\n    selected_event_picks = picks[picks[\"event_index\"] == tid].copy()\n    selected_event_picks.set_index(\"station_id\", inplace=True)\n    for staid in selected_event_picks.index:\n        station_code = staid.split(\".\")[1]\n        selected_event_picks.loc[staid, \"station_code\"] = station_code\n    tau_s_c_sec = np.zeros((num_stations, num_channels), dtype=np.float64)\n    for s, sta in enumerate(station_codes):\n        for c, cp in enumerate(component_codes):\n            phase_type = PHASE_ON_COMP[cp]\n            picks_s_c = selected_event_picks[\n                (\n                    (selected_event_picks[\"station_code\"] == sta)\n                    &amp; (selected_event_picks[\"phase_type\"] == phase_type)\n                )\n            ]\n            if len(picks_s_c) == 0:\n                # no pick for this station/component: set to -999\n                tau_s_c_sec[s, c] = -999\n            elif len(picks_s_c) == 1:\n                # express pick relative to beginning of day (midnight)\n                _pick = pd.Timestamp(picks_s_c[\"phase_time\"].iloc[0])\n                _relative_pick_sec = (_pick - pd.Timestamp(_pick.strftime(\"%Y-%m-%d\"))).total_seconds()\n                tau_s_c_sec[s, c] = _relative_pick_sec - OFFSET_PHASE_SEC[phase_type]\n            else:\n                # there were several picks from different channels: average them\n                _relative_pick_sec = 0.\n                for _pick in picks_s_c[\"phase_time\"].values:\n                    _pick = pd.Timestamp(_pick)\n                    _relative_pick_sec += (_pick - pd.Timestamp(_pick.strftime(\"%Y-%m-%d\"))).total_seconds()\n                _relative_pick_sec /= float(len(picks_s_c[\"phase_time\"]))\n                tau_s_c_sec[s, c] = _relative_pick_sec - OFFSET_PHASE_SEC[phase_type]\n    # now, we convert these relative times into samples \n    # and express them relative to the earliest time\n    # we also store in memory the minimum time offset `tau_min_samp` for the next step\n    moveouts_samp_arr[t, ...] = (tau_s_c_sec * SAMPLING_RATE_HZ).astype(np.int64)\n    tau_min_samp_arr[t] = np.min(moveouts_samp_arr[t, moveouts_samp_arr[t, ...] &gt; 0])\n    moveouts_samp_arr[t, ...] = moveouts_samp_arr[t, ...] - tau_min_samp_arr[t]\nmoveouts_samp_arr[1, ...]\n</pre> # first, we extract the set of relative delay times of the beginning of each # template window on a given station and component moveouts_samp_arr = np.zeros((num_templates, num_stations, num_channels), dtype=np.int64) tau_min_samp_arr = np.zeros(num_templates, dtype=np.int64) for t, tid in enumerate(template_ids):     # add station_code columns to `selected_event_picks`     selected_event_picks = picks[picks[\"event_index\"] == tid].copy()     selected_event_picks.set_index(\"station_id\", inplace=True)     for staid in selected_event_picks.index:         station_code = staid.split(\".\")[1]         selected_event_picks.loc[staid, \"station_code\"] = station_code     tau_s_c_sec = np.zeros((num_stations, num_channels), dtype=np.float64)     for s, sta in enumerate(station_codes):         for c, cp in enumerate(component_codes):             phase_type = PHASE_ON_COMP[cp]             picks_s_c = selected_event_picks[                 (                     (selected_event_picks[\"station_code\"] == sta)                     &amp; (selected_event_picks[\"phase_type\"] == phase_type)                 )             ]             if len(picks_s_c) == 0:                 # no pick for this station/component: set to -999                 tau_s_c_sec[s, c] = -999             elif len(picks_s_c) == 1:                 # express pick relative to beginning of day (midnight)                 _pick = pd.Timestamp(picks_s_c[\"phase_time\"].iloc[0])                 _relative_pick_sec = (_pick - pd.Timestamp(_pick.strftime(\"%Y-%m-%d\"))).total_seconds()                 tau_s_c_sec[s, c] = _relative_pick_sec - OFFSET_PHASE_SEC[phase_type]             else:                 # there were several picks from different channels: average them                 _relative_pick_sec = 0.                 for _pick in picks_s_c[\"phase_time\"].values:                     _pick = pd.Timestamp(_pick)                     _relative_pick_sec += (_pick - pd.Timestamp(_pick.strftime(\"%Y-%m-%d\"))).total_seconds()                 _relative_pick_sec /= float(len(picks_s_c[\"phase_time\"]))                 tau_s_c_sec[s, c] = _relative_pick_sec - OFFSET_PHASE_SEC[phase_type]     # now, we convert these relative times into samples      # and express them relative to the earliest time     # we also store in memory the minimum time offset `tau_min_samp` for the next step     moveouts_samp_arr[t, ...] = (tau_s_c_sec * SAMPLING_RATE_HZ).astype(np.int64)     tau_min_samp_arr[t] = np.min(moveouts_samp_arr[t, moveouts_samp_arr[t, ...] &gt; 0])     moveouts_samp_arr[t, ...] = moveouts_samp_arr[t, ...] - tau_min_samp_arr[t] moveouts_samp_arr[1, ...] Out[16]: <pre>array([[     167,      167,      106],\n       [-1607573, -1607573, -1607573],\n       [     280,      280,      166],\n       [     129,      129,       84],\n       [       0,        0,        6],\n       [      19,       19,        9],\n       [      81,       81,       42],\n       [     109,      109,       68],\n       [-1607573, -1607573,      131],\n       [      91,       91,       58],\n       [      41,       41,       27],\n       [-1607573, -1607573, -1607573],\n       [      16,       16,       13],\n       [-1607573, -1607573, -1607573],\n       [     158,      158,       95],\n       [-1607573, -1607573,      125],\n       [     183,      183,      113],\n       [     142,      142,       76],\n       [     191,      191,      118],\n       [-1607573, -1607573, -1607573],\n       [-1607573, -1607573, -1607573]])</pre> <p>Next, we use the moveouts, in samples, to clip out the relevant template waveforms from the continuous seismograms.</p> In\u00a0[17]: Copied! <pre>template_waveforms_arr = np.zeros((num_templates, num_stations, num_channels, TEMPLATE_DURATION_SAMP), dtype=np.float32)\nweights_arr = np.ones((num_templates, num_stations, num_channels), dtype=np.float32)\n\nfor t in range(num_templates):\n    for s, sta in enumerate(station_codes):\n        for c, cp in enumerate(component_codes):\n            if moveouts_samp_arr[t, s, c] &lt; 0:\n                # no picks were found on this station\n                weights_arr[t, s, c] = 0.\n                continue\n            starttime = tau_min_samp_arr[t] + moveouts_samp_arr[t, s, c]\n            endtime = starttime + TEMPLATE_DURATION_SAMP\n            template_waveforms_arr[t, s, c, :] = continuous_seismograms_arr[s, c, starttime:endtime]\n            if template_waveforms_arr[t, s, c, :].sum() == 0.:\n                # no data was available on this channel\n                weights_arr[t, s, c] = 0.\n            \ntemplate_waveforms_arr[0, ...]\n</pre> template_waveforms_arr = np.zeros((num_templates, num_stations, num_channels, TEMPLATE_DURATION_SAMP), dtype=np.float32) weights_arr = np.ones((num_templates, num_stations, num_channels), dtype=np.float32)  for t in range(num_templates):     for s, sta in enumerate(station_codes):         for c, cp in enumerate(component_codes):             if moveouts_samp_arr[t, s, c] &lt; 0:                 # no picks were found on this station                 weights_arr[t, s, c] = 0.                 continue             starttime = tau_min_samp_arr[t] + moveouts_samp_arr[t, s, c]             endtime = starttime + TEMPLATE_DURATION_SAMP             template_waveforms_arr[t, s, c, :] = continuous_seismograms_arr[s, c, starttime:endtime]             if template_waveforms_arr[t, s, c, :].sum() == 0.:                 # no data was available on this channel                 weights_arr[t, s, c] = 0.              template_waveforms_arr[0, ...] Out[17]: <pre>array([[[ 8.97313148e-05,  1.98242778e-05,  2.78249881e-05, ...,\n         -7.64243596e-05,  8.15730600e-05, -1.36619390e-04],\n        [-1.53143628e-04,  1.09062545e-04,  4.91616520e-05, ...,\n          1.25622420e-04,  1.38057658e-04, -1.39361102e-04],\n        [ 1.20851581e-07,  1.38758310e-07,  2.11948830e-07, ...,\n          2.26182208e-04, -1.26758649e-04, -1.29398366e-04]],\n\n       [[ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n          0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n          0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n          0.00000000e+00,  0.00000000e+00,  0.00000000e+00]],\n\n       [[-1.17751943e-05, -1.94716686e-05, -1.26916411e-05, ...,\n          1.26965097e-05, -5.42943926e-05, -3.75940072e-05],\n        [ 1.44362775e-05, -6.69798646e-06, -1.81024643e-05, ...,\n         -1.30919288e-04, -4.79771152e-05,  5.89142401e-05],\n        [-1.70647674e-08, -4.51735644e-08, -5.31642250e-08, ...,\n         -2.76465835e-05,  1.09849934e-05,  7.53651184e-05]],\n\n       ...,\n\n       [[ 9.79182732e-05,  1.20548120e-05, -6.78718134e-05, ...,\n         -4.39741780e-05, -8.56743718e-05, -4.07844163e-05],\n        [-1.06544954e-04, -2.09494847e-05,  3.22320338e-05, ...,\n         -2.05596989e-05, -2.35213724e-06,  1.43820762e-05],\n        [ 7.50264846e-08, -1.39887931e-07, -4.09550466e-07, ...,\n          2.37395070e-05, -6.56054617e-05,  1.63327525e-06]],\n\n       [[ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n          0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n          0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n          0.00000000e+00,  0.00000000e+00,  0.00000000e+00]],\n\n       [[ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n          0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n          0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n          0.00000000e+00,  0.00000000e+00,  0.00000000e+00]]],\n      dtype=float32)</pre> In\u00a0[18]: Copied! <pre>template_waveforms_arr.shape\n</pre> template_waveforms_arr.shape Out[18]: <pre>(91, 21, 3, 200)</pre> In\u00a0[19]: Copied! <pre># normalize template waveforms for numerical reasons\nnorm = np.std(template_waveforms_arr, axis=-1, keepdims=True)\nnorm[norm == 0.] = 1. \ntemplate_waveforms_arr /= norm\n\n# normalize weights so that they sum up to one\nweights_arr /= np.sum(weights_arr, axis=(1, 2), keepdims=True)\n\n# normalize continuous seismograms for numerical reasons\nnorm = np.std(continuous_seismograms_arr, axis=-1, keepdims=True)\nnorm[norm == 0.] = 1. \ncontinuous_seismograms_arr /= norm\n</pre> # normalize template waveforms for numerical reasons norm = np.std(template_waveforms_arr, axis=-1, keepdims=True) norm[norm == 0.] = 1.  template_waveforms_arr /= norm  # normalize weights so that they sum up to one weights_arr /= np.sum(weights_arr, axis=(1, 2), keepdims=True)  # normalize continuous seismograms for numerical reasons norm = np.std(continuous_seismograms_arr, axis=-1, keepdims=True) norm[norm == 0.] = 1.  continuous_seismograms_arr /= norm In\u00a0[20]: Copied! <pre># FMF_STEP_SAMP: this is the step between two consecutive calculation of the correlation coefficient\nFMF_STEP_SAMP = 1\n# ARCH: it determines whether you want to use GPUs or CPUs \n#       If you do not have an Nvidia GPU, set ARCH = \"cpu\"\nARCH = \"gpu\"\n</pre> # FMF_STEP_SAMP: this is the step between two consecutive calculation of the correlation coefficient FMF_STEP_SAMP = 1 # ARCH: it determines whether you want to use GPUs or CPUs  #       If you do not have an Nvidia GPU, set ARCH = \"cpu\" ARCH = \"gpu\" In\u00a0[21]: Copied! <pre>cc = fmf.matched_filter(\n    template_waveforms_arr.astype(np.float32),\n    moveouts_samp_arr.astype(np.int32),\n    weights_arr.astype(np.float32),\n    continuous_seismograms_arr.astype(np.float32),\n    FMF_STEP_SAMP,\n    arch=ARCH,\n)\n</pre> cc = fmf.matched_filter(     template_waveforms_arr.astype(np.float32),     moveouts_samp_arr.astype(np.int32),     weights_arr.astype(np.float32),     continuous_seismograms_arr.astype(np.float32),     FMF_STEP_SAMP,     arch=ARCH, ) In\u00a0[22]: Copied! <pre># unlike in the previous notebook, we now have multiple time series of correlation coefficients,\n# one for each template\ncc.shape\n</pre> # unlike in the previous notebook, we now have multiple time series of correlation coefficients, # one for each template cc.shape Out[22]: <pre>(91, 2159801)</pre> In\u00a0[23]: Copied! <pre># let's print the output of our template matching run, which a time series of network-averaged correlation coefficients\n# of same duration as the continuous seismograms\ntemplate_idx = 1\n_cc = cc[template_idx, :]\ntime_cc = np.arange(len(_cc)) / SAMPLING_RATE_HZ\n\nfig = plt.figure(\"network_averaged_cc\", figsize=(20, 6))\ngs = fig.add_gridspec(ncols=4)\n\nax1 = fig.add_subplot(gs[:3])\nax1.plot(time_cc, _cc, lw=0.75)\nax1.set_xlabel(\"Elapsed time (sec)\")\nax1.set_ylabel(\"Network-averaged cc\")\nax1.set_xlim(time_cc.min(), time_cc.max())\nax1.set_title(f\"Time series of template {template_idx:d}/continuous seismograms similarity\")\n\nax2 = fig.add_subplot(gs[3], sharey=ax1)\n_ = ax2.hist(_cc, orientation=\"horizontal\", bins=250, density=True)\nax2.set_xlabel(\"Normalized count\")\nax2.set_title(\"Histogram\")\n\nfor ax in [ax1, ax2]:\n    ax.grid()\n</pre> # let's print the output of our template matching run, which a time series of network-averaged correlation coefficients # of same duration as the continuous seismograms template_idx = 1 _cc = cc[template_idx, :] time_cc = np.arange(len(_cc)) / SAMPLING_RATE_HZ  fig = plt.figure(\"network_averaged_cc\", figsize=(20, 6)) gs = fig.add_gridspec(ncols=4)  ax1 = fig.add_subplot(gs[:3]) ax1.plot(time_cc, _cc, lw=0.75) ax1.set_xlabel(\"Elapsed time (sec)\") ax1.set_ylabel(\"Network-averaged cc\") ax1.set_xlim(time_cc.min(), time_cc.max()) ax1.set_title(f\"Time series of template {template_idx:d}/continuous seismograms similarity\")  ax2 = fig.add_subplot(gs[3], sharey=ax1) _ = ax2.hist(_cc, orientation=\"horizontal\", bins=250, density=True) ax2.set_xlabel(\"Normalized count\") ax2.set_title(\"Histogram\")  for ax in [ax1, ax2]:     ax.grid()  In\u00a0[24]: Copied! <pre>def select_cc_indexes(\n    cc_t,\n    threshold,\n    search_win,\n):\n    \"\"\"Select the peaks in the CC time series.\n\n    Parameters\n    ------------\n    cc_t: (n_corr,) numpy.ndarray\n        The CC time series for one template.\n    threshold: (n_corr,) numpy.ndarray or scalar\n        The detection threshold.\n    search_win: scalar int\n        The minimum inter-event time, in units of correlation step.\n\n\n    Returns\n    --------\n    cc_idx: (n_detections,) numpy.ndarray\n        The list of all selected CC indexes. They give the timings of the\n        detected events.\n    \"\"\"\n\n    cc_detections = cc_t &gt; threshold\n    cc_idx = np.where(cc_detections)[0]\n\n    cc_idx = list(cc_idx)\n    n_rm = 0\n    for i in range(1, len(cc_idx)):\n        if (cc_idx[i - n_rm] - cc_idx[i - n_rm - 1]) &lt; search_win:\n            if cc_t[cc_idx[i - n_rm]] &gt; cc_t[cc_idx[i - n_rm - 1]]:\n                # keep (i-n_rm)-th detection\n                cc_idx.remove(cc_idx[i - n_rm - 1])\n            else:\n                # keep (i-n_rm-1)-th detection\n                cc_idx.remove(cc_idx[i - n_rm])\n            n_rm += 1\n    cc_idx = np.asarray(cc_idx)\n    return cc_idx\n    \n</pre> def select_cc_indexes(     cc_t,     threshold,     search_win, ):     \"\"\"Select the peaks in the CC time series.      Parameters     ------------     cc_t: (n_corr,) numpy.ndarray         The CC time series for one template.     threshold: (n_corr,) numpy.ndarray or scalar         The detection threshold.     search_win: scalar int         The minimum inter-event time, in units of correlation step.       Returns     --------     cc_idx: (n_detections,) numpy.ndarray         The list of all selected CC indexes. They give the timings of the         detected events.     \"\"\"      cc_detections = cc_t &gt; threshold     cc_idx = np.where(cc_detections)[0]      cc_idx = list(cc_idx)     n_rm = 0     for i in range(1, len(cc_idx)):         if (cc_idx[i - n_rm] - cc_idx[i - n_rm - 1]) &lt; search_win:             if cc_t[cc_idx[i - n_rm]] &gt; cc_t[cc_idx[i - n_rm - 1]]:                 # keep (i-n_rm)-th detection                 cc_idx.remove(cc_idx[i - n_rm - 1])             else:                 # keep (i-n_rm-1)-th detection                 cc_idx.remove(cc_idx[i - n_rm])             n_rm += 1     cc_idx = np.asarray(cc_idx)     return cc_idx      In\u00a0[25]: Copied! <pre># INTEREVENT_TIME_RESOLUTION_SEC: In some cases, a template might trigger multiple, closely spaced detections because\n#                                 of a phenomenon similar to that of \"cycle skipping\", where the waveform correlates\n#                                 well with a time-shifted version of itself. Thus, to avoid redundant detections, we\n#                                 set a minimum time separation between triggers (rule of thumb: about half the template duration)\nINTEREVENT_TIME_RESOLUTION_SEC = 5.\nINTEREVENT_TIME_RESOLUTION_SAMP = int(INTEREVENT_TIME_RESOLUTION_SEC * SAMPLING_RATE_HZ)\ntemplate_idx = 2\n_cc = cc[template_idx, :]\ntime_cc = np.arange(len(_cc)) * FMF_STEP_SAMP / SAMPLING_RATE_HZ\nNUM_RMS = 8.\ndetection_threshold = NUM_RMS * np.std(_cc)\nevent_cc_indexes = select_cc_indexes(_cc, detection_threshold, INTEREVENT_TIME_RESOLUTION_SAMP)\n\nfig = plt.figure(\"network_averaged_cc\", figsize=(20, 6))\ngs = fig.add_gridspec(ncols=4)\n\nax1 = fig.add_subplot(gs[:3])\nax1.plot(time_cc, _cc, lw=0.75)\nax1.scatter(time_cc[event_cc_indexes], _cc[event_cc_indexes], linewidths=0.25, edgecolor=\"k\", color=\"r\", zorder=2)\nax1.set_xlabel(\"Elapsed time (sec)\")\nax1.set_ylabel(\"Network-averaged cc\")\nax1.set_xlim(time_cc.min(), time_cc.max())\nax1.set_title(\"Time series of template/continuous seismograms similarity\")\n\nax2 = fig.add_subplot(gs[3], sharey=ax1)\n_ = ax2.hist(_cc, orientation=\"horizontal\", bins=250, density=True, zorder=2)\nax2.set_xlabel(\"Normalized count\")\nax2.set_title(\"Histogram\")\n\nlabel = f\"Detection threshold: {NUM_RMS:.0f}\"r\"$\\times \\mathrm{RMS}(\\mathrm{CC}(t))$\"f\"\\n{len(event_cc_indexes):d} detected events\"\nfor ax in [ax1, ax2]:\n    ax.grid()\n    ax.axhline(\n        detection_threshold, ls=\"--\", color=\"r\",\n        label=label\n        )\nax1.legend(loc=\"upper left\")\n</pre> # INTEREVENT_TIME_RESOLUTION_SEC: In some cases, a template might trigger multiple, closely spaced detections because #                                 of a phenomenon similar to that of \"cycle skipping\", where the waveform correlates #                                 well with a time-shifted version of itself. Thus, to avoid redundant detections, we #                                 set a minimum time separation between triggers (rule of thumb: about half the template duration) INTEREVENT_TIME_RESOLUTION_SEC = 5. INTEREVENT_TIME_RESOLUTION_SAMP = int(INTEREVENT_TIME_RESOLUTION_SEC * SAMPLING_RATE_HZ) template_idx = 2 _cc = cc[template_idx, :] time_cc = np.arange(len(_cc)) * FMF_STEP_SAMP / SAMPLING_RATE_HZ NUM_RMS = 8. detection_threshold = NUM_RMS * np.std(_cc) event_cc_indexes = select_cc_indexes(_cc, detection_threshold, INTEREVENT_TIME_RESOLUTION_SAMP)  fig = plt.figure(\"network_averaged_cc\", figsize=(20, 6)) gs = fig.add_gridspec(ncols=4)  ax1 = fig.add_subplot(gs[:3]) ax1.plot(time_cc, _cc, lw=0.75) ax1.scatter(time_cc[event_cc_indexes], _cc[event_cc_indexes], linewidths=0.25, edgecolor=\"k\", color=\"r\", zorder=2) ax1.set_xlabel(\"Elapsed time (sec)\") ax1.set_ylabel(\"Network-averaged cc\") ax1.set_xlim(time_cc.min(), time_cc.max()) ax1.set_title(\"Time series of template/continuous seismograms similarity\")  ax2 = fig.add_subplot(gs[3], sharey=ax1) _ = ax2.hist(_cc, orientation=\"horizontal\", bins=250, density=True, zorder=2) ax2.set_xlabel(\"Normalized count\") ax2.set_title(\"Histogram\")  label = f\"Detection threshold: {NUM_RMS:.0f}\"r\"$\\times \\mathrm{RMS}(\\mathrm{CC}(t))$\"f\"\\n{len(event_cc_indexes):d} detected events\" for ax in [ax1, ax2]:     ax.grid()     ax.axhline(         detection_threshold, ls=\"--\", color=\"r\",         label=label         ) ax1.legend(loc=\"upper left\") Out[25]: <pre>&lt;matplotlib.legend.Legend at 0x7fa64c6ccc70&gt;</pre> In\u00a0[26]: Copied! <pre>NUM_RMS = 8.\ndate = pd.Timestamp(\n    (continuous_seismograms[0].stats.starttime.timestamp + continuous_seismograms[0].stats.endtime.timestamp) / 2.,\n    unit=\"s\"\n).strftime(\"%Y-%m-%d\")\n\ncatalog = {\n    \"detection_time\": [],\n    \"cc\": [],\n    \"normalized_cc\": [],\n    \"tid\": []\n}\n\nfor t, tid in enumerate(template_ids):\n    detection_threshold = NUM_RMS * np.std(cc[t, :])\n    event_cc_indexes = select_cc_indexes(cc[t, :], detection_threshold, INTEREVENT_TIME_RESOLUTION_SAMP)\n    for i in range(len(event_cc_indexes)):\n        # --------------------------------------\n        catalog[\"detection_time\"].append(pd.Timestamp(date) + pd.Timedelta(event_cc_indexes[i] * FMF_STEP_SAMP / SAMPLING_RATE_HZ, \"s\"))\n        catalog[\"cc\"].append(cc[t, event_cc_indexes[i]])\n        catalog[\"normalized_cc\"].append(cc[t, event_cc_indexes[i]] / detection_threshold)\n        catalog[\"tid\"].append(tid)\ncatalog = pd.DataFrame(catalog)\ncatalog.sort_values(\"detection_time\", inplace=True)\ncatalog\n</pre> NUM_RMS = 8. date = pd.Timestamp(     (continuous_seismograms[0].stats.starttime.timestamp + continuous_seismograms[0].stats.endtime.timestamp) / 2.,     unit=\"s\" ).strftime(\"%Y-%m-%d\")  catalog = {     \"detection_time\": [],     \"cc\": [],     \"normalized_cc\": [],     \"tid\": [] }  for t, tid in enumerate(template_ids):     detection_threshold = NUM_RMS * np.std(cc[t, :])     event_cc_indexes = select_cc_indexes(cc[t, :], detection_threshold, INTEREVENT_TIME_RESOLUTION_SAMP)     for i in range(len(event_cc_indexes)):         # --------------------------------------         catalog[\"detection_time\"].append(pd.Timestamp(date) + pd.Timedelta(event_cc_indexes[i] * FMF_STEP_SAMP / SAMPLING_RATE_HZ, \"s\"))         catalog[\"cc\"].append(cc[t, event_cc_indexes[i]])         catalog[\"normalized_cc\"].append(cc[t, event_cc_indexes[i]] / detection_threshold)         catalog[\"tid\"].append(tid) catalog = pd.DataFrame(catalog) catalog.sort_values(\"detection_time\", inplace=True) catalog Out[26]: detection_time cc normalized_cc tid 0 2019-07-04 15:42:49.840 0.167660 1.426171 1101 1 2019-07-04 16:07:21.880 0.162098 1.378858 1101 2 2019-07-04 16:13:44.960 0.252016 2.143727 1101 3 2019-07-04 17:02:56.960 1.000000 8.506305 1101 4 2019-07-04 17:09:21.680 0.339568 2.888471 1101 ... ... ... ... ... 1116 2019-07-04 23:55:30.280 0.211336 1.695065 677 986 2019-07-04 23:55:56.000 0.119222 1.033934 72 1077 2019-07-04 23:56:39.680 0.209909 1.773958 8 168 2019-07-04 23:57:45.440 0.173444 1.433532 1043 316 2019-07-04 23:59:12.120 0.141436 1.191650 1075 <p>1279 rows \u00d7 4 columns</p> <p>When building a catalog, it is always necessary to visualize some of the detected event waveforms to get a sense of the ratio of true-to-false detection rate.</p> <p>In the following, we plot the waveforms of each detected event and we also compare the stack of all the waveforms to the original template waveform. Since all events share similar waveforms, the stack is similar to the template waveform. Moreover, since noise across all these waveforms sums up incoherently, stacking acts as a denoiser which may help you produce a cleaner version of the template waveform, for example on remote stations.</p> In\u00a0[27]: Copied! <pre>STATION_NAME = \"CLC\"\nCOMPONENT_NAME = \"Z\"\n\nsta_idx = station_codes.index(STATION_NAME)\ncp_idx = component_codes.index(COMPONENT_NAME)\ntp_idx = 1\n\nsubcatalog = catalog[catalog[\"tid\"] == template_ids.iloc[tp_idx]]\n\ndetected_event_waveforms = []\nfor i in range(len(subcatalog)):\n    detection_time_samp = (\n        subcatalog[\"detection_time\"].iloc[i] - pd.Timestamp(date)\n    ).total_seconds() * SAMPLING_RATE_HZ\n    idx_start = int(detection_time_samp) + moveouts_samp_arr[tp_idx, sta_idx, cp_idx]\n    idx_end = idx_start + TEMPLATE_DURATION_SAMP\n    detected_event_waveforms.append(continuous_seismograms_arr[sta_idx, cp_idx, idx_start:idx_end])\n\ndetected_event_waveforms = np.asarray(detected_event_waveforms)\ndetected_event_waveforms.shape\n</pre> STATION_NAME = \"CLC\" COMPONENT_NAME = \"Z\"  sta_idx = station_codes.index(STATION_NAME) cp_idx = component_codes.index(COMPONENT_NAME) tp_idx = 1  subcatalog = catalog[catalog[\"tid\"] == template_ids.iloc[tp_idx]]  detected_event_waveforms = [] for i in range(len(subcatalog)):     detection_time_samp = (         subcatalog[\"detection_time\"].iloc[i] - pd.Timestamp(date)     ).total_seconds() * SAMPLING_RATE_HZ     idx_start = int(detection_time_samp) + moveouts_samp_arr[tp_idx, sta_idx, cp_idx]     idx_end = idx_start + TEMPLATE_DURATION_SAMP     detected_event_waveforms.append(continuous_seismograms_arr[sta_idx, cp_idx, idx_start:idx_end])  detected_event_waveforms = np.asarray(detected_event_waveforms) detected_event_waveforms.shape Out[27]: <pre>(4, 200)</pre> In\u00a0[28]: Copied! <pre>fig = plt.figure(\"detected_event_waveforms\", figsize=(10, 15))\ngs = fig.add_gridspec(nrows=4)\n\nax1 = fig.add_subplot(gs[:3])\n\n_time_wav = np.arange(detected_event_waveforms.shape[1]) / SAMPLING_RATE_HZ\n\nstack = np.zeros(detected_event_waveforms.shape[1])\nfor i in range(detected_event_waveforms.shape[0]):\n    norm = np.abs(detected_event_waveforms[i, :]).max()\n    if subcatalog[\"cc\"].iloc[i] &gt; 0.999:\n        color = \"r\"\n        template_wav = detected_event_waveforms[i, :] / norm\n    else:\n        color = \"k\"\n    time_of_day = subcatalog[\"detection_time\"].iloc[i].strftime(\"%H:%M:%S\")\n    ax1.plot(_time_wav, detected_event_waveforms[i, :] / norm + i * 1.5, color=color)\n    ax1.text(0.98 * _time_wav.max(), i * 1.5 + 0.1, time_of_day, ha=\"right\", va=\"bottom\")\n    stack += detected_event_waveforms[i, :] / norm\nstack /= np.abs(stack).max()\nax1.set_xlabel(\"Time (s)\")\nax1.set_xlim(_time_wav.min(), _time_wav.max())\nax1.set_ylabel(\"Normalized offset amplitude\")\nax1.set_title(f\"Events detected on {date} and recorded by {STATION_NAME}.{COMPONENT_NAME}\")\n\nax2 = fig.add_subplot(gs[3], sharex=ax1)\nax2.plot(_time_wav, stack, color=\"blue\", label=\"Stacked waveforms\")\nax2.plot(_time_wav, template_wav, color=\"red\", ls=\"--\", label=\"Template waveform\")\nax2.legend(loc=\"upper left\")\nax2.set_xlabel(\"Time (s)\")\nax2.set_ylabel(\"Normalized amplitude\")\n</pre> fig = plt.figure(\"detected_event_waveforms\", figsize=(10, 15)) gs = fig.add_gridspec(nrows=4)  ax1 = fig.add_subplot(gs[:3])  _time_wav = np.arange(detected_event_waveforms.shape[1]) / SAMPLING_RATE_HZ  stack = np.zeros(detected_event_waveforms.shape[1]) for i in range(detected_event_waveforms.shape[0]):     norm = np.abs(detected_event_waveforms[i, :]).max()     if subcatalog[\"cc\"].iloc[i] &gt; 0.999:         color = \"r\"         template_wav = detected_event_waveforms[i, :] / norm     else:         color = \"k\"     time_of_day = subcatalog[\"detection_time\"].iloc[i].strftime(\"%H:%M:%S\")     ax1.plot(_time_wav, detected_event_waveforms[i, :] / norm + i * 1.5, color=color)     ax1.text(0.98 * _time_wav.max(), i * 1.5 + 0.1, time_of_day, ha=\"right\", va=\"bottom\")     stack += detected_event_waveforms[i, :] / norm stack /= np.abs(stack).max() ax1.set_xlabel(\"Time (s)\") ax1.set_xlim(_time_wav.min(), _time_wav.max()) ax1.set_ylabel(\"Normalized offset amplitude\") ax1.set_title(f\"Events detected on {date} and recorded by {STATION_NAME}.{COMPONENT_NAME}\")  ax2 = fig.add_subplot(gs[3], sharex=ax1) ax2.plot(_time_wav, stack, color=\"blue\", label=\"Stacked waveforms\") ax2.plot(_time_wav, template_wav, color=\"red\", ls=\"--\", label=\"Template waveform\") ax2.legend(loc=\"upper left\") ax2.set_xlabel(\"Time (s)\") ax2.set_ylabel(\"Normalized amplitude\") Out[28]: <pre>Text(0, 0.5, 'Normalized amplitude')</pre> In\u00a0[29]: Copied! <pre>catalog[\"interevent_time_s\"] = catalog[\"detection_time\"].diff().dt.total_seconds()\nfor tid in template_ids:\n    subcatalog = catalog[catalog[\"tid\"] == tid]\n    catalog.loc[subcatalog.index, \"return_time_s\"] = subcatalog[\"detection_time\"].diff().dt.total_seconds()\ncatalog\n</pre> catalog[\"interevent_time_s\"] = catalog[\"detection_time\"].diff().dt.total_seconds() for tid in template_ids:     subcatalog = catalog[catalog[\"tid\"] == tid]     catalog.loc[subcatalog.index, \"return_time_s\"] = subcatalog[\"detection_time\"].diff().dt.total_seconds() catalog Out[29]: detection_time cc normalized_cc tid interevent_time_s return_time_s 0 2019-07-04 15:42:49.840 0.167660 1.426171 1101 NaN NaN 1 2019-07-04 16:07:21.880 0.162098 1.378858 1101 1472.04 1472.04 2 2019-07-04 16:13:44.960 0.252016 2.143727 1101 383.08 383.08 3 2019-07-04 17:02:56.960 1.000000 8.506305 1101 2952.00 2952.00 4 2019-07-04 17:09:21.680 0.339568 2.888471 1101 384.72 384.72 ... ... ... ... ... ... ... 1116 2019-07-04 23:55:30.280 0.211336 1.695065 677 51.56 777.12 986 2019-07-04 23:55:56.000 0.119222 1.033934 72 25.72 12553.08 1077 2019-07-04 23:56:39.680 0.209909 1.773958 8 43.68 7773.08 168 2019-07-04 23:57:45.440 0.173444 1.433532 1043 65.76 6180.08 316 2019-07-04 23:59:12.120 0.141436 1.191650 1075 86.68 794.52 <p>1279 rows \u00d7 6 columns</p> In\u00a0[30]: Copied! <pre>fig, axes = plt.subplots(num=\"interevent_time_vs_detection_time\", nrows=2, figsize=(18, 14))\n\naxes[0].scatter(\n    catalog[\"detection_time\"], catalog[\"interevent_time_s\"], c=catalog[\"cc\"],\n    linewidths=0.25, edgecolor=\"k\", cmap=\"magma\", s=50, zorder=2\n    )\naxes[0].set_xlabel(\"Detection time\")\naxes[0].set_ylabel(\"Interevent time (s)\")\naxes[0].set_title(\"Interevent time vs detection time\")\n\naxes[1].scatter(\n    catalog[\"detection_time\"], catalog[\"return_time_s\"], c=catalog[\"cc\"],\n    linewidths=0.25, edgecolor=\"k\", cmap=\"magma\", s=50, zorder=2\n    )\naxes[1].set_xlabel(\"Detection time\")\naxes[1].set_ylabel(\"Return time (s)\")\naxes[1].set_title(\"Return time vs detection time\")\n\nfor ax in axes:\n    ax.grid()\n    cbar = plt.colorbar(ax.collections[0], ax=ax)\n    cbar.set_label(\"CC\")\n    ax.set_yscale(\"log\")\n</pre> fig, axes = plt.subplots(num=\"interevent_time_vs_detection_time\", nrows=2, figsize=(18, 14))  axes[0].scatter(     catalog[\"detection_time\"], catalog[\"interevent_time_s\"], c=catalog[\"cc\"],     linewidths=0.25, edgecolor=\"k\", cmap=\"magma\", s=50, zorder=2     ) axes[0].set_xlabel(\"Detection time\") axes[0].set_ylabel(\"Interevent time (s)\") axes[0].set_title(\"Interevent time vs detection time\")  axes[1].scatter(     catalog[\"detection_time\"], catalog[\"return_time_s\"], c=catalog[\"cc\"],     linewidths=0.25, edgecolor=\"k\", cmap=\"magma\", s=50, zorder=2     ) axes[1].set_xlabel(\"Detection time\") axes[1].set_ylabel(\"Return time (s)\") axes[1].set_title(\"Return time vs detection time\")  for ax in axes:     ax.grid()     cbar = plt.colorbar(ax.collections[0], ax=ax)     cbar.set_label(\"CC\")     ax.set_yscale(\"log\") In\u00a0[31]: Copied! <pre>def compute_distances(\n    source_longitudes,\n    source_latitudes,\n    source_depths,\n    receiver_longitudes,\n    receiver_latitudes,\n    receiver_depths,\n):\n    \"\"\"\n    Fast distance computation between all source points and all receivers.\n\n    This function uses `cartopy.geodesic.Geodesic` to compute pair-wise distances\n    between source points and receivers. It computes both hypocentral distances\n    and, if specified, epicentral distances.\n\n    Parameters\n    ----------\n    source_longitudes : numpy.ndarray or list\n        Longitudes, in decimal degrees, of the source points.\n    source_latitudes : numpy.ndarray or list\n        Latitudes, in decimal degrees, of the source points.\n    source_depths : numpy.ndarray or list\n        Depths, in kilometers, of the source points.\n    receiver_longitudes : numpy.ndarray or list\n        Longitudes, in decimal degrees, of the receivers.\n    receiver_latitudes : numpy.ndarray or list\n        Latitudes, in decimal degrees, of the receivers.\n    receiver_depths : numpy.ndarray or list\n        Depths, in kilometers, of the receivers. Negative depths indicate\n        receivers located at the surface.\n\n    Returns\n    -------\n    hypocentral_distances : numpy.ndarray\n        Array of hypocentral distances between source points and receivers.\n        The shape of the array is (n_sources, n_receivers).\n    \"\"\"\n    from cartopy.geodesic import Geodesic\n\n    # convert types if necessary\n    if isinstance(source_longitudes, list):\n        source_longitudes = np.asarray(source_longitudes)\n    if isinstance(source_latitudes, list):\n        source_latitudes = np.asarray(source_latitudes)\n    if isinstance(source_depths, list):\n        source_depths = np.asarray(source_depths)\n\n    # initialize distance array\n    hypocentral_distances = np.zeros(\n        (len(source_latitudes), len(receiver_latitudes)), dtype=np.float32\n    )\n    epicentral_distances = np.zeros(\n        (len(source_latitudes), len(receiver_latitudes)), dtype=np.float32\n    )\n    # initialize the Geodesic instance\n    G = Geodesic()\n    for s in range(len(receiver_latitudes)):\n        epi_distances = G.inverse(\n            np.array([[receiver_longitudes[s], receiver_latitudes[s]]]),\n            np.hstack(\n                (source_longitudes[:, np.newaxis], source_latitudes[:, np.newaxis])\n            ),\n        )\n        epicentral_distances[:, s] = np.asarray(epi_distances)[:, 0].squeeze() / 1000.0\n        hypocentral_distances[:, s] = np.sqrt(\n            epicentral_distances[:, s] ** 2 + (source_depths - receiver_depths[s]) ** 2\n        )\n    return hypocentral_distances\n</pre> def compute_distances(     source_longitudes,     source_latitudes,     source_depths,     receiver_longitudes,     receiver_latitudes,     receiver_depths, ):     \"\"\"     Fast distance computation between all source points and all receivers.      This function uses `cartopy.geodesic.Geodesic` to compute pair-wise distances     between source points and receivers. It computes both hypocentral distances     and, if specified, epicentral distances.      Parameters     ----------     source_longitudes : numpy.ndarray or list         Longitudes, in decimal degrees, of the source points.     source_latitudes : numpy.ndarray or list         Latitudes, in decimal degrees, of the source points.     source_depths : numpy.ndarray or list         Depths, in kilometers, of the source points.     receiver_longitudes : numpy.ndarray or list         Longitudes, in decimal degrees, of the receivers.     receiver_latitudes : numpy.ndarray or list         Latitudes, in decimal degrees, of the receivers.     receiver_depths : numpy.ndarray or list         Depths, in kilometers, of the receivers. Negative depths indicate         receivers located at the surface.      Returns     -------     hypocentral_distances : numpy.ndarray         Array of hypocentral distances between source points and receivers.         The shape of the array is (n_sources, n_receivers).     \"\"\"     from cartopy.geodesic import Geodesic      # convert types if necessary     if isinstance(source_longitudes, list):         source_longitudes = np.asarray(source_longitudes)     if isinstance(source_latitudes, list):         source_latitudes = np.asarray(source_latitudes)     if isinstance(source_depths, list):         source_depths = np.asarray(source_depths)      # initialize distance array     hypocentral_distances = np.zeros(         (len(source_latitudes), len(receiver_latitudes)), dtype=np.float32     )     epicentral_distances = np.zeros(         (len(source_latitudes), len(receiver_latitudes)), dtype=np.float32     )     # initialize the Geodesic instance     G = Geodesic()     for s in range(len(receiver_latitudes)):         epi_distances = G.inverse(             np.array([[receiver_longitudes[s], receiver_latitudes[s]]]),             np.hstack(                 (source_longitudes[:, np.newaxis], source_latitudes[:, np.newaxis])             ),         )         epicentral_distances[:, s] = np.asarray(epi_distances)[:, 0].squeeze() / 1000.0         hypocentral_distances[:, s] = np.sqrt(             epicentral_distances[:, s] ** 2 + (source_depths - receiver_depths[s]) ** 2         )     return hypocentral_distances In\u00a0[32]: Copied! <pre>intertemplate_distances = compute_distances(\n    selected_events_meta[\"longitude\"].values,\n    selected_events_meta[\"latitude\"].values,\n    selected_events_meta[\"depth_km\"].values,\n    selected_events_meta[\"longitude\"].values,\n    selected_events_meta[\"latitude\"].values, \n    selected_events_meta[\"depth_km\"].values\n)\n\nintertemplate_distances = pd.DataFrame(\n        index=template_ids.values, columns=template_ids.values, data=intertemplate_distances\n    )\n\nintertemplate_distances\n</pre> intertemplate_distances = compute_distances(     selected_events_meta[\"longitude\"].values,     selected_events_meta[\"latitude\"].values,     selected_events_meta[\"depth_km\"].values,     selected_events_meta[\"longitude\"].values,     selected_events_meta[\"latitude\"].values,      selected_events_meta[\"depth_km\"].values )  intertemplate_distances = pd.DataFrame(         index=template_ids.values, columns=template_ids.values, data=intertemplate_distances     )  intertemplate_distances Out[32]: 1101 1348 4325 20605 9324 6837 606 2547 4786 2213 ... 8 665 271 677 9 17 3543 4452 190 573 1101 0.000000 9.349293 5.729656 7.487901 7.075881 5.755127 6.929080 5.604232 11.501103 2.912213 ... 12.841033 4.477149 4.850606 8.501280 4.599574 5.317030 9.470930 15.666500 8.983744 7.314927 1348 9.349293 0.000000 8.331551 10.681546 5.195356 10.503522 11.425143 11.467293 4.430634 7.780132 ... 4.486320 6.397411 11.108863 11.902843 9.754032 9.131742 11.894234 6.349260 12.092418 4.931087 4325 5.729656 8.331551 0.000000 7.413243 3.555481 7.894967 8.270176 3.439519 8.268929 4.622073 ... 12.529959 2.891850 3.436299 8.918022 4.543046 5.515726 9.540435 14.185110 8.999001 4.349711 20605 7.487901 10.681546 7.413243 0.000000 9.017220 12.880819 1.935980 7.698983 10.531159 9.012992 ... 12.998297 8.196445 8.056205 1.544554 3.521500 11.447443 2.228568 16.107647 1.695657 9.963426 9324 7.075881 5.195356 3.555481 9.017220 0.000000 8.099398 9.971913 6.911278 5.506651 5.024450 ... 9.584186 2.683345 6.662618 10.534163 6.866254 5.787068 10.871763 10.779346 10.637229 1.115412 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 17 5.317030 9.131742 5.515726 11.447443 5.787068 2.633469 11.540643 6.332531 11.124969 2.819244 ... 13.378164 3.957289 5.397058 12.790235 8.149417 0.000000 13.602967 14.864223 13.105728 5.379503 3543 9.470930 11.894234 9.540435 2.228568 10.871763 14.967089 2.924127 9.864626 11.527185 11.092483 ... 13.555637 10.256493 10.260924 1.261910 5.740344 13.602967 0.000000 16.820774 1.028291 11.820169 4452 15.666500 6.349260 14.185110 16.107647 10.779346 16.092836 17.032705 17.520269 7.058576 13.977184 ... 4.299100 12.554879 17.238419 17.161259 15.822165 14.864223 16.820774 0.000000 17.248327 10.403678 190 8.983744 12.092418 8.999001 1.695657 10.637229 14.485799 2.433021 9.052017 11.731926 10.668187 ... 14.058927 9.890747 9.486581 0.693620 5.069254 13.105728 1.028291 17.248327 0.000000 11.602349 573 7.314927 4.931087 4.349711 9.963426 1.115412 7.596488 10.811090 7.554186 5.805594 4.947604 ... 9.404045 2.919080 7.184767 11.464071 7.713250 5.379503 11.820169 10.403678 11.602349 0.000000 <p>91 rows \u00d7 91 columns</p> In\u00a0[33]: Copied! <pre>def compute_intertemplate_cc(\n    template_waveforms_arr,\n    max_lag=5,\n):\n    \"\"\"\n    Compute the pairwise template cross-correlations (CCs).\n\n    Parameters\n    ----------\n    max_lag : int, default to 5\n        The maximum lag, in samples, allowed when searching for the maximum\n        CC on each channel. This parameter accounts for small discrepancies\n        in windowing that could occur for two templates highly similar but\n        associated with slightly different locations.\n        reading a potentially large file.\n\n    Returns\n    -------\n    intertemplate_cc : numpy.ndarray\n        The computed inter-template correlation coefficients.\n    \"\"\"\n    # format arrays for FMF\n    data_arr = template_waveforms_arr.copy()\n    template_arr = template_waveforms_arr[..., max_lag:-max_lag]\n    moveouts_arr = np.zeros(template_waveforms_arr.shape[:-1], dtype=np.int32)\n    num_templates = template_waveforms_arr.shape[0]\n    intertp_cc = np.zeros(\n        (num_templates, num_templates), dtype=np.float32\n    )\n    # use FMF on one template at a time against all others\n    for t in range(num_templates):\n        weights = np.ones(template_arr.shape[:-1], dtype=np.float32)\n        for s in range(template_arr.shape[1]):\n            for c in range(template_arr.shape[2]):\n                if np.sum(template_arr[t, s, c, :]) == 0.:\n                    weights[t, s, c] = 0.\n        weights /= np.sum(weights, axis=(1, 2), keepdims=True)\n        keep = np.sum(weights != 0.0, axis=(1, 2)) &gt; 0\n        cc = fmf.matched_filter(\n            template_arr[keep, ...],\n            moveouts_arr[keep, ...],\n            weights[keep, ...],\n            data_arr[t, ...],\n            1,\n            arch=\"cpu\",\n            network_sum=False,\n            check_zeros=False,\n        )\n        intertp_cc[t, keep] = np.sum(\n            weights[keep, ...] * np.max(cc, axis=1), axis=(-1, -2)\n        )\n    # make the CC matrix symmetric by averaging the lower\n    # and upper triangles\n    intertemplate_cc = (intertp_cc + intertp_cc.T) / 2.0\n    return intertemplate_cc\n</pre> def compute_intertemplate_cc(     template_waveforms_arr,     max_lag=5, ):     \"\"\"     Compute the pairwise template cross-correlations (CCs).      Parameters     ----------     max_lag : int, default to 5         The maximum lag, in samples, allowed when searching for the maximum         CC on each channel. This parameter accounts for small discrepancies         in windowing that could occur for two templates highly similar but         associated with slightly different locations.         reading a potentially large file.      Returns     -------     intertemplate_cc : numpy.ndarray         The computed inter-template correlation coefficients.     \"\"\"     # format arrays for FMF     data_arr = template_waveforms_arr.copy()     template_arr = template_waveforms_arr[..., max_lag:-max_lag]     moveouts_arr = np.zeros(template_waveforms_arr.shape[:-1], dtype=np.int32)     num_templates = template_waveforms_arr.shape[0]     intertp_cc = np.zeros(         (num_templates, num_templates), dtype=np.float32     )     # use FMF on one template at a time against all others     for t in range(num_templates):         weights = np.ones(template_arr.shape[:-1], dtype=np.float32)         for s in range(template_arr.shape[1]):             for c in range(template_arr.shape[2]):                 if np.sum(template_arr[t, s, c, :]) == 0.:                     weights[t, s, c] = 0.         weights /= np.sum(weights, axis=(1, 2), keepdims=True)         keep = np.sum(weights != 0.0, axis=(1, 2)) &gt; 0         cc = fmf.matched_filter(             template_arr[keep, ...],             moveouts_arr[keep, ...],             weights[keep, ...],             data_arr[t, ...],             1,             arch=\"cpu\",             network_sum=False,             check_zeros=False,         )         intertp_cc[t, keep] = np.sum(             weights[keep, ...] * np.max(cc, axis=1), axis=(-1, -2)         )     # make the CC matrix symmetric by averaging the lower     # and upper triangles     intertemplate_cc = (intertp_cc + intertp_cc.T) / 2.0     return intertemplate_cc In\u00a0[34]: Copied! <pre>intertemplate_cc = compute_intertemplate_cc(\n    template_waveforms_arr.astype(np.float32),\n)\n\nintertemplate_cc = pd.DataFrame(\n        index=template_ids.values, columns=template_ids.values, data=intertemplate_cc\n    )\n\nintertemplate_cc\n</pre> intertemplate_cc = compute_intertemplate_cc(     template_waveforms_arr.astype(np.float32), )  intertemplate_cc = pd.DataFrame(         index=template_ids.values, columns=template_ids.values, data=intertemplate_cc     )  intertemplate_cc Out[34]: 1101 1348 4325 20605 9324 6837 606 2547 4786 2213 ... 8 665 271 677 9 17 3543 4452 190 573 1101 1.000000 0.168783 0.152849 0.038095 0.169056 0.140194 0.202947 0.159919 0.150238 0.236168 ... 0.205217 0.228647 0.222859 0.186294 0.174423 0.207843 0.174352 0.139704 0.188377 0.178896 1348 0.168783 1.000000 0.129109 0.031816 0.144986 0.118015 0.139677 0.115961 0.111395 0.135386 ... 0.163028 0.156824 0.157585 0.182688 0.126702 0.137380 0.129408 0.160123 0.163061 0.146483 4325 0.152849 0.129109 1.000000 0.017472 0.168945 0.090611 0.156312 0.117083 0.090731 0.133255 ... 0.128058 0.172581 0.174417 0.172038 0.109456 0.133973 0.120997 0.130567 0.172986 0.150668 20605 0.038095 0.031816 0.017472 1.000000 0.033383 0.023836 0.039508 0.032409 0.025857 0.035573 ... 0.039945 0.031393 0.037404 0.046661 0.036617 0.030602 0.034026 0.027355 0.038628 0.031347 9324 0.169056 0.144986 0.168945 0.033383 1.000000 0.099626 0.152790 0.111780 0.106815 0.143344 ... 0.125143 0.166077 0.177516 0.162897 0.107802 0.129211 0.128553 0.127405 0.159463 0.148714 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 17 0.207843 0.137380 0.133973 0.030602 0.129211 0.152333 0.175084 0.148283 0.130708 0.228636 ... 0.190502 0.228889 0.178534 0.173232 0.183706 1.000000 0.184948 0.138462 0.169083 0.177538 3543 0.174352 0.129408 0.120997 0.034026 0.128553 0.111915 0.202738 0.137794 0.130493 0.161382 ... 0.185677 0.199266 0.178054 0.209915 0.206832 0.184948 1.000000 0.122939 0.231893 0.156152 4452 0.139704 0.160123 0.130567 0.027355 0.127405 0.100273 0.142169 0.099644 0.109379 0.122473 ... 0.167342 0.145697 0.151059 0.138129 0.129540 0.138462 0.122939 1.000000 0.141270 0.135185 190 0.188377 0.163061 0.172986 0.038628 0.159463 0.118327 0.184763 0.122494 0.138524 0.180362 ... 0.152410 0.205656 0.168475 0.300840 0.174207 0.169083 0.231893 0.141270 1.000000 0.186951 573 0.178896 0.146483 0.150668 0.031347 0.148714 0.105022 0.156341 0.122110 0.112509 0.169780 ... 0.175112 0.204454 0.180234 0.206652 0.159696 0.177538 0.156152 0.135185 0.186951 1.000000 <p>91 rows \u00d7 91 columns</p> In\u00a0[35]: Copied! <pre>def flag_multiples(\n    catalog,\n    intertemplate_distances,\n    intertemplate_cc,\n    dt_criterion=4.0,\n    distance_criterion=15.,\n    similarity_criterion=-1.0,\n):\n    \"\"\"\n    Search for events detected by multiple templates and flag them.\n\n    Parameters\n    ----------\n    catalog : pd.DataFrame\n    dt_criterion : float, optional\n        The time interval, in seconds, under which two events are examined for redundancy.\n    distance_criterion : float, optional\n        The inter-event distance, in kilometers, under which two events are examined for redundancy.\n    similarity_criterion : float, optional\n        The template similarity threshold, in terms of average cc, over which two events\n        are examined for redundancy. The default value of -1 means that similarity is not\n        taken into account.\n    intertemplate_cc : pd.DataFrame\n    progress : bool, optional\n        If True, print progress bar with `tqdm`.\n    **kwargs\n        Additional keyword arguments.\n\n    Returns\n    -------\n    unique_event : numpy.ndarray\n        A (`len(detection_times)`,) `numpy.ndarray` of booleans with `unique_event[i]=True`\n        if event number `i` is a unique detection of a single event.\n    \"\"\"\n    detection_time_sec = catalog[\"detection_time\"].values.astype(\"datetime64[ms]\").astype(\"float64\") / 1000.\n    interevent_time = np.hstack(\n        ([1.10 * dt_criterion], detection_time_sec[1:] - detection_time_sec[:-1])\n    )\n    # -----------------------------------\n    n_events = len(catalog)\n    unique_event = np.ones(n_events, dtype=bool)\n    for n1 in range(n_events):\n        if not unique_event[n1]:\n            # was already flagged as a multiple\n            continue\n        tid1 = catalog[\"tid\"].iloc[n1]\n        # apply the time criterion\n        n2 = n1 + 1\n        if n2 &lt; n_events:\n            dt_n1n2 = interevent_time[n2]\n        else:\n            continue\n        temporal_neighbors = [n1]\n        while dt_n1n2 &lt; dt_criterion:\n            temporal_neighbors.append(n2)\n            n2 += 1\n            if n2 &gt;= n_events:\n                break\n            dt_n1n2 += interevent_time[n2]\n        temporal_neighbors = np.array(temporal_neighbors).astype(\"int64\")\n        if len(temporal_neighbors) == 1:\n            # did not find any temporal neighbors\n            continue\n        # remove events that were already flagged as non unique\n        temporal_neighbors = temporal_neighbors[unique_event[temporal_neighbors]]\n        candidates = temporal_neighbors\n        if len(candidates) == 1:\n            continue\n        tids_candidates = catalog[\"tid\"].values[temporal_neighbors]\n        \n        similarities = intertemplate_cc.loc[tid1, tids_candidates].values\n        \n        distances = intertemplate_distances.loc[tid1, tids_candidates].values\n        \n        multiples = candidates[\n            np.where(\n                (similarities &gt;= similarity_criterion)\n                &amp; (distances &lt;= distance_criterion)\n                )[0]\n        ]\n        unique_event[multiples] = False\n        # find best CC and keep it\n        ccs = catalog[\"cc\"].values[multiples]\n        best_cc = multiples[ccs.argmax()]\n        unique_event[best_cc] = True\n    # -------------------------------------------\n    return unique_event\n</pre> def flag_multiples(     catalog,     intertemplate_distances,     intertemplate_cc,     dt_criterion=4.0,     distance_criterion=15.,     similarity_criterion=-1.0, ):     \"\"\"     Search for events detected by multiple templates and flag them.      Parameters     ----------     catalog : pd.DataFrame     dt_criterion : float, optional         The time interval, in seconds, under which two events are examined for redundancy.     distance_criterion : float, optional         The inter-event distance, in kilometers, under which two events are examined for redundancy.     similarity_criterion : float, optional         The template similarity threshold, in terms of average cc, over which two events         are examined for redundancy. The default value of -1 means that similarity is not         taken into account.     intertemplate_cc : pd.DataFrame     progress : bool, optional         If True, print progress bar with `tqdm`.     **kwargs         Additional keyword arguments.      Returns     -------     unique_event : numpy.ndarray         A (`len(detection_times)`,) `numpy.ndarray` of booleans with `unique_event[i]=True`         if event number `i` is a unique detection of a single event.     \"\"\"     detection_time_sec = catalog[\"detection_time\"].values.astype(\"datetime64[ms]\").astype(\"float64\") / 1000.     interevent_time = np.hstack(         ([1.10 * dt_criterion], detection_time_sec[1:] - detection_time_sec[:-1])     )     # -----------------------------------     n_events = len(catalog)     unique_event = np.ones(n_events, dtype=bool)     for n1 in range(n_events):         if not unique_event[n1]:             # was already flagged as a multiple             continue         tid1 = catalog[\"tid\"].iloc[n1]         # apply the time criterion         n2 = n1 + 1         if n2 &lt; n_events:             dt_n1n2 = interevent_time[n2]         else:             continue         temporal_neighbors = [n1]         while dt_n1n2 &lt; dt_criterion:             temporal_neighbors.append(n2)             n2 += 1             if n2 &gt;= n_events:                 break             dt_n1n2 += interevent_time[n2]         temporal_neighbors = np.array(temporal_neighbors).astype(\"int64\")         if len(temporal_neighbors) == 1:             # did not find any temporal neighbors             continue         # remove events that were already flagged as non unique         temporal_neighbors = temporal_neighbors[unique_event[temporal_neighbors]]         candidates = temporal_neighbors         if len(candidates) == 1:             continue         tids_candidates = catalog[\"tid\"].values[temporal_neighbors]                  similarities = intertemplate_cc.loc[tid1, tids_candidates].values                  distances = intertemplate_distances.loc[tid1, tids_candidates].values                  multiples = candidates[             np.where(                 (similarities &gt;= similarity_criterion)                 &amp; (distances &lt;= distance_criterion)                 )[0]         ]         unique_event[multiples] = False         # find best CC and keep it         ccs = catalog[\"cc\"].values[multiples]         best_cc = multiples[ccs.argmax()]         unique_event[best_cc] = True     # -------------------------------------------     return unique_event In\u00a0[36]: Copied! <pre>SIMILARITY_CRITERION = 0.10\nDT_CRITERION = INTEREVENT_TIME_RESOLUTION_SEC # re-use same time resolution as before\nDISTANCE_CRITERION = 10.0\n\n# if the inter-template cc value is below this value, and there are two or more detections that are close in time to each other, these will NOT be flagged as multiples. \n# So they *might* be discrete events... We need to do some tests to see how sensitive the catalog looks to this parameter ?  \nunique_event = flag_multiples(\n    catalog,\n    intertemplate_distances,\n    intertemplate_cc,\n    dt_criterion=DT_CRITERION,\n    similarity_criterion=SIMILARITY_CRITERION,\n    distance_criterion=DISTANCE_CRITERION,\n)\n\ncatalog[\"unique_event\"] = unique_event\n\ncatalog\n</pre> SIMILARITY_CRITERION = 0.10 DT_CRITERION = INTEREVENT_TIME_RESOLUTION_SEC # re-use same time resolution as before DISTANCE_CRITERION = 10.0  # if the inter-template cc value is below this value, and there are two or more detections that are close in time to each other, these will NOT be flagged as multiples.  # So they *might* be discrete events... We need to do some tests to see how sensitive the catalog looks to this parameter ?   unique_event = flag_multiples(     catalog,     intertemplate_distances,     intertemplate_cc,     dt_criterion=DT_CRITERION,     similarity_criterion=SIMILARITY_CRITERION,     distance_criterion=DISTANCE_CRITERION, )  catalog[\"unique_event\"] = unique_event  catalog Out[36]: detection_time cc normalized_cc tid interevent_time_s return_time_s unique_event 0 2019-07-04 15:42:49.840 0.167660 1.426171 1101 NaN NaN True 1 2019-07-04 16:07:21.880 0.162098 1.378858 1101 1472.04 1472.04 True 2 2019-07-04 16:13:44.960 0.252016 2.143727 1101 383.08 383.08 True 3 2019-07-04 17:02:56.960 1.000000 8.506305 1101 2952.00 2952.00 True 4 2019-07-04 17:09:21.680 0.339568 2.888471 1101 384.72 384.72 True ... ... ... ... ... ... ... ... 1116 2019-07-04 23:55:30.280 0.211336 1.695065 677 51.56 777.12 True 986 2019-07-04 23:55:56.000 0.119222 1.033934 72 25.72 12553.08 True 1077 2019-07-04 23:56:39.680 0.209909 1.773958 8 43.68 7773.08 True 168 2019-07-04 23:57:45.440 0.173444 1.433532 1043 65.76 6180.08 True 316 2019-07-04 23:59:12.120 0.141436 1.191650 1075 86.68 794.52 True <p>1279 rows \u00d7 7 columns</p> In\u00a0[37]: Copied! <pre>catalog[catalog[\"unique_event\"]]\n</pre> catalog[catalog[\"unique_event\"]] Out[37]: detection_time cc normalized_cc tid interevent_time_s return_time_s unique_event 0 2019-07-04 15:42:49.840 0.167660 1.426171 1101 NaN NaN True 1 2019-07-04 16:07:21.880 0.162098 1.378858 1101 1472.04 1472.04 True 2 2019-07-04 16:13:44.960 0.252016 2.143727 1101 383.08 383.08 True 3 2019-07-04 17:02:56.960 1.000000 8.506305 1101 2952.00 2952.00 True 4 2019-07-04 17:09:21.680 0.339568 2.888471 1101 384.72 384.72 True ... ... ... ... ... ... ... ... 1116 2019-07-04 23:55:30.280 0.211336 1.695065 677 51.56 777.12 True 986 2019-07-04 23:55:56.000 0.119222 1.033934 72 25.72 12553.08 True 1077 2019-07-04 23:56:39.680 0.209909 1.773958 8 43.68 7773.08 True 168 2019-07-04 23:57:45.440 0.173444 1.433532 1043 65.76 6180.08 True 316 2019-07-04 23:59:12.120 0.141436 1.191650 1075 86.68 794.52 True <p>697 rows \u00d7 7 columns</p> In\u00a0[38]: Copied! <pre>cat_delumped = catalog[catalog[\"unique_event\"]]\ninterevent_times_delumped_cat = cat_delumped[\"detection_time\"].diff().dt.total_seconds()\n\nfig, ax = plt.subplots(num=\"interevent_time_vs_detection_time\", figsize=(18, 7))\n\nax.scatter(cat_delumped[\"detection_time\"], interevent_times_delumped_cat, c=cat_delumped[\"cc\"], linewidths=0.25, edgecolor=\"k\", cmap=\"magma\", s=50)\nax.set_xlabel(\"Detection time\")\nax.set_ylabel(\"Interevent time (s)\")\nax.set_title(\"Interevent time after 'delumping' vs detection time\")\n\nax.grid()\ncbar = plt.colorbar(ax.collections[0], ax=ax)\ncbar.set_label(\"Normalized CC\")\nax.set_yscale(\"log\")\n</pre> cat_delumped = catalog[catalog[\"unique_event\"]] interevent_times_delumped_cat = cat_delumped[\"detection_time\"].diff().dt.total_seconds()  fig, ax = plt.subplots(num=\"interevent_time_vs_detection_time\", figsize=(18, 7))  ax.scatter(cat_delumped[\"detection_time\"], interevent_times_delumped_cat, c=cat_delumped[\"cc\"], linewidths=0.25, edgecolor=\"k\", cmap=\"magma\", s=50) ax.set_xlabel(\"Detection time\") ax.set_ylabel(\"Interevent time (s)\") ax.set_title(\"Interevent time after 'delumping' vs detection time\")  ax.grid() cbar = plt.colorbar(ax.collections[0], ax=ax) cbar.set_label(\"Normalized CC\") ax.set_yscale(\"log\") In\u00a0[39]: Copied! <pre>cat_delumped = catalog[catalog[\"unique_event\"]]\ninterevent_times_delumped_cat = cat_delumped[\"detection_time\"].diff().dt.total_seconds()\n\nfig, axes = plt.subplots(num=\"delumped_interevent_time_vs_detection_time\", nrows=2, figsize=(18, 14))\n\naxes[0].scatter(\n    cat_delumped[\"detection_time\"], interevent_times_delumped_cat, c=cat_delumped[\"cc\"],\n    linewidths=0.25, edgecolor=\"k\", cmap=\"magma\", s=50, zorder=2\n    )\naxes[0].set_xlabel(\"Detection time\")\naxes[0].set_ylabel(\"Interevent time (s)\")\naxes[0].set_title(\"'Delumped' interevent time vs detection time\")\n\naxes[1].scatter(\n    catalog[\"detection_time\"], catalog[\"return_time_s\"], c=catalog[\"cc\"],\n    linewidths=0.25, edgecolor=\"k\", cmap=\"magma\", s=50, zorder=2\n    )\naxes[1].set_xlabel(\"Detection time\")\naxes[1].set_ylabel(\"Return time (s)\")\naxes[1].set_title(\"Return time vs detection time\")\n\nfor ax in axes:\n    ax.grid()\n    cbar = plt.colorbar(ax.collections[0], ax=ax)\n    cbar.set_label(\"CC\")\n    ax.set_yscale(\"log\")\n</pre> cat_delumped = catalog[catalog[\"unique_event\"]] interevent_times_delumped_cat = cat_delumped[\"detection_time\"].diff().dt.total_seconds()  fig, axes = plt.subplots(num=\"delumped_interevent_time_vs_detection_time\", nrows=2, figsize=(18, 14))  axes[0].scatter(     cat_delumped[\"detection_time\"], interevent_times_delumped_cat, c=cat_delumped[\"cc\"],     linewidths=0.25, edgecolor=\"k\", cmap=\"magma\", s=50, zorder=2     ) axes[0].set_xlabel(\"Detection time\") axes[0].set_ylabel(\"Interevent time (s)\") axes[0].set_title(\"'Delumped' interevent time vs detection time\")  axes[1].scatter(     catalog[\"detection_time\"], catalog[\"return_time_s\"], c=catalog[\"cc\"],     linewidths=0.25, edgecolor=\"k\", cmap=\"magma\", s=50, zorder=2     ) axes[1].set_xlabel(\"Detection time\") axes[1].set_ylabel(\"Return time (s)\") axes[1].set_title(\"Return time vs detection time\")  for ax in axes:     ax.grid()     cbar = plt.colorbar(ax.collections[0], ax=ax)     cbar.set_label(\"CC\")     ax.set_yscale(\"log\") <p>Note how the plots of inter-event times and return times differ even after \"delumping\" the catalog. In the process of delumping, we make arbitrary choices (see the criteria above) and we end up discarding some real events and keeping some repeats of the same event. Thus, some valuable information may be lost in the process. The pre-delumping catalog should always be kept because some studies may be well-suited for a template-by-template analysis, or may not be negatively affected by the presence of redundant detections, thus making optimal use of the template matching catalog.</p> In\u00a0[40]: Copied! <pre># first, extract clips from the continuous seismograms\ndetected_event_waveforms_all_channels = np.zeros(\n    (len(catalog), num_stations, num_channels, TEMPLATE_DURATION_SAMP), dtype=np.float32\n)\nfor i in range(len(catalog)):\n    tid = catalog[\"tid\"].iloc[i]\n    t =template_ids.tolist().index(tid)\n    for s in range(num_stations):\n        for c in range(num_channels):\n            if moveouts_samp_arr[t, s, c] &lt; 0:\n                continue\n            detection_time_samp = (\n                catalog[\"detection_time\"].iloc[i] - pd.Timestamp(date)\n            ).total_seconds() * SAMPLING_RATE_HZ\n            idx_start = int(detection_time_samp) * FMF_STEP_SAMP + moveouts_samp_arr[t, s, c]\n            idx_end = idx_start + TEMPLATE_DURATION_SAMP\n            detected_event_waveforms_all_channels[i, s, c, :] = (\n                continuous_seismograms_arr[s, c, idx_start:idx_end]\n                )\n</pre> # first, extract clips from the continuous seismograms detected_event_waveforms_all_channels = np.zeros(     (len(catalog), num_stations, num_channels, TEMPLATE_DURATION_SAMP), dtype=np.float32 ) for i in range(len(catalog)):     tid = catalog[\"tid\"].iloc[i]     t =template_ids.tolist().index(tid)     for s in range(num_stations):         for c in range(num_channels):             if moveouts_samp_arr[t, s, c] &lt; 0:                 continue             detection_time_samp = (                 catalog[\"detection_time\"].iloc[i] - pd.Timestamp(date)             ).total_seconds() * SAMPLING_RATE_HZ             idx_start = int(detection_time_samp) * FMF_STEP_SAMP + moveouts_samp_arr[t, s, c]             idx_end = idx_start + TEMPLATE_DURATION_SAMP             detected_event_waveforms_all_channels[i, s, c, :] = (                 continuous_seismograms_arr[s, c, idx_start:idx_end]                 )  In\u00a0[41]: Copied! <pre>m_rel = np.zeros(len(catalog), dtype=np.float32)\n\n# then, measure the peak amplitudes and apply the above formula to get the relative magnitudes\npeak_amplitudes = np.max(np.abs(detected_event_waveforms_all_channels), axis=-1)\n\nfor tid in catalog[\"tid\"].unique():\n    t = selected_events_meta[\"event_index\"].tolist().index(tid)\n    m_ref = selected_events_meta[\"magnitude\"].iloc[t]\n    ref_event_index = np.where(\n        (catalog[\"tid\"] == tid) &amp; (catalog[\"cc\"]&gt; 0.99)\n    )[0][0]\n    peak_amplitudes_ref = peak_amplitudes[ref_event_index, ...]\n    template_subcat = catalog[catalog[\"tid\"] == tid]\n    \n    amplitude_ratios = peak_amplitudes[catalog[\"tid\"] == tid, ...] / peak_amplitudes_ref\n    invalid = (np.isnan(amplitude_ratios) | np.isinf(amplitude_ratios))\n    amplitude_ratios = np.ma.masked_where(invalid, amplitude_ratios)\n    \n    catalog.loc[template_subcat.index, \"m_rel\"] = (\n        m_ref + np.ma.mean(np.log10(amplitude_ratios), axis=(1, 2))\n    )\ncatalog\n</pre> m_rel = np.zeros(len(catalog), dtype=np.float32)  # then, measure the peak amplitudes and apply the above formula to get the relative magnitudes peak_amplitudes = np.max(np.abs(detected_event_waveforms_all_channels), axis=-1)  for tid in catalog[\"tid\"].unique():     t = selected_events_meta[\"event_index\"].tolist().index(tid)     m_ref = selected_events_meta[\"magnitude\"].iloc[t]     ref_event_index = np.where(         (catalog[\"tid\"] == tid) &amp; (catalog[\"cc\"]&gt; 0.99)     )[0][0]     peak_amplitudes_ref = peak_amplitudes[ref_event_index, ...]     template_subcat = catalog[catalog[\"tid\"] == tid]          amplitude_ratios = peak_amplitudes[catalog[\"tid\"] == tid, ...] / peak_amplitudes_ref     invalid = (np.isnan(amplitude_ratios) | np.isinf(amplitude_ratios))     amplitude_ratios = np.ma.masked_where(invalid, amplitude_ratios)          catalog.loc[template_subcat.index, \"m_rel\"] = (         m_ref + np.ma.mean(np.log10(amplitude_ratios), axis=(1, 2))     ) catalog <pre>/tmp/ipykernel_2902775/1462564979.py:15: RuntimeWarning: invalid value encountered in true_divide\n  amplitude_ratios = peak_amplitudes[catalog[\"tid\"] == tid, ...] / peak_amplitudes_ref\n/tmp/ipykernel_2902775/1462564979.py:15: RuntimeWarning: invalid value encountered in true_divide\n  amplitude_ratios = peak_amplitudes[catalog[\"tid\"] == tid, ...] / peak_amplitudes_ref\n</pre> Out[41]: detection_time cc normalized_cc tid interevent_time_s return_time_s unique_event m_rel 0 2019-07-04 15:42:49.840 0.167660 1.426171 1101 NaN NaN True 0.694889 1 2019-07-04 16:07:21.880 0.162098 1.378858 1101 1472.04 1472.04 True 0.671568 2 2019-07-04 16:13:44.960 0.252016 2.143727 1101 383.08 383.08 True 1.696695 3 2019-07-04 17:02:56.960 1.000000 8.506305 1101 2952.00 2952.00 True 4.476724 4 2019-07-04 17:09:21.680 0.339568 2.888471 1101 384.72 384.72 True 2.565838 ... ... ... ... ... ... ... ... ... 1116 2019-07-04 23:55:30.280 0.211336 1.695065 677 51.56 777.12 True 0.126559 986 2019-07-04 23:55:56.000 0.119222 1.033934 72 25.72 12553.08 True 0.481692 1077 2019-07-04 23:56:39.680 0.209909 1.773958 8 43.68 7773.08 True -0.711008 168 2019-07-04 23:57:45.440 0.173444 1.433532 1043 65.76 6180.08 True -0.615430 316 2019-07-04 23:59:12.120 0.141436 1.191650 1075 86.68 794.52 True -0.788380 <p>1279 rows \u00d7 8 columns</p> <p>Next, we plot the distribution of earthquake magnitudes. The cumulative distribution usually follows the so-called Gutenberg-Richter law: $$ \\log N(m \\geq M) = a - b M$$</p> In\u00a0[42]: Copied! <pre>fig, ax = plt.subplots(num=\"magnitude_distribution\", figsize=(8, 8))\naxb = ax.twinx()\nax.set_title(\"Distribution of earthquake magnitudes\")\n\ncount, m_bins, _ = ax.hist(catalog[\"m_rel\"], bins=15, color=\"k\", alpha=0.5, label=\"All channels\")\n\n# calculate the b-value\nm_midbins = 0.5 * (m_bins[:-1] + m_bins[1:])\n# estimate the magnitude of completeness with the maximum curvature method (Mc is taken as the mode of the magnitude distribution)\nm_c = m_midbins[count.argmax()]\nm_above_Mc = catalog[\"m_rel\"][catalog[\"m_rel\"] &gt; m_c].values\nm_c_w_buffer = m_c + 0.1\n# estimate the b-value with the maximum likelihood method\nbvalue = 1.0 / (np.log(10) * np.mean(m_above_Mc - m_c_w_buffer))\n\naxb.plot(np.sort(catalog[\"m_rel\"]), np.arange(len(catalog))[::-1], color=\"k\", lw=2, label=f\"MLE b-value: {bvalue:.2f}\")\n\nax.set_xlabel(\"Magnitude\")\nax.set_ylabel(\"Event Count\")\naxb.set_ylabel(\"Cumulative Event Count\")\naxb.legend(loc=\"lower left\")\n\nfor ax in [ax, axb]:\n    ax.set_yscale(\"log\")\n</pre> fig, ax = plt.subplots(num=\"magnitude_distribution\", figsize=(8, 8)) axb = ax.twinx() ax.set_title(\"Distribution of earthquake magnitudes\")  count, m_bins, _ = ax.hist(catalog[\"m_rel\"], bins=15, color=\"k\", alpha=0.5, label=\"All channels\")  # calculate the b-value m_midbins = 0.5 * (m_bins[:-1] + m_bins[1:]) # estimate the magnitude of completeness with the maximum curvature method (Mc is taken as the mode of the magnitude distribution) m_c = m_midbins[count.argmax()] m_above_Mc = catalog[\"m_rel\"][catalog[\"m_rel\"] &gt; m_c].values m_c_w_buffer = m_c + 0.1 # estimate the b-value with the maximum likelihood method bvalue = 1.0 / (np.log(10) * np.mean(m_above_Mc - m_c_w_buffer))  axb.plot(np.sort(catalog[\"m_rel\"]), np.arange(len(catalog))[::-1], color=\"k\", lw=2, label=f\"MLE b-value: {bvalue:.2f}\")  ax.set_xlabel(\"Magnitude\") ax.set_ylabel(\"Event Count\") axb.set_ylabel(\"Cumulative Event Count\") axb.legend(loc=\"lower left\")  for ax in [ax, axb]:     ax.set_yscale(\"log\") <p>Note: the physical meaning of the b-value is better understood when the Gutenberg-Richter law is recast in terms of a power-law of the seismic moment, $M_0$. Assuming we have the following scaling relationship between magnitude and seismic moment: $M \\sim c \\log M_0$, then: $$ \\log N \\sim b c \\log M_0 \\Leftrightarrow N \\sim M_0^{bc}.$$ The value of the exponent $bc$ is fixed by the physical properties of the system. Thus, it is clear that the b-value depends on the choice of the magnitude scale.</p> <p>Moreover, one must keep in mind that such a scaling relationship, $M \\sim c \\log M_0$, may not hold over the entire magnitude range. Moment magnitudes, $M_w$, were introduced to make sure that magnitudes and seismic moments were related through a unique scaling relationship for any magnitudes, but the relative magnitudes used here hardly ensure such a consistent scaling. Relative magnitudes, therefore, should not be used for thorough analyses of the b-value (and things like time variations of the b-value).</p>"},{"location":"notebooks/tm_multiple_templates/#template-matching-with-a-multiple-templates","title":"Template matching with a multiple templates\u00b6","text":"<p>Templates are selected from Weiqiang Zhu's PhaseNet catalog.</p> <p>This notebook is the continuation of the first notebook, <code>tm_one_template.ipynb</code>.</p> <p>Download the seismic data at: https://doi.org/10.5281/zenodo.15097180</p> <p></p>"},{"location":"notebooks/tm_multiple_templates/#load-phasenet-catalog","title":"Load PhaseNet catalog\u00b6","text":"<p>Here, we read the catalog of the 2019 Ridgecrest sequence made with PhaseNet. Information is divided into three files:</p> <ul> <li>a station metadata file,</li> <li>an event metadata file (the catalog per se),</li> <li>a pick database, which contains all the P- and S-wave picks found by PhaseNet.</li> </ul>"},{"location":"notebooks/tm_multiple_templates/#read-continuous-seismograms-from-2019-07-04","title":"Read continuous seismograms from 2019-07-04\u00b6","text":""},{"location":"notebooks/tm_multiple_templates/#build-template-database","title":"Build template database\u00b6","text":"<p>Using the PhaseNet catalog, we will select all events with magnitudes between 3 and 5 as template events (totally arbitrary choice!).</p>"},{"location":"notebooks/tm_multiple_templates/#build-the-template-matching-catalog","title":"Build the template matching catalog\u00b6","text":""},{"location":"notebooks/tm_multiple_templates/#run-fmf","title":"Run FMF\u00b6","text":"<p>After all this data formatting, we can now run template matching (also called matched-filtering) to detect new events that are similar to our template events.</p> <p>For that, use the software Fast Matched Filter (FMF): https://github.com/beridel/fast_matched_filter</p> <p>FMF offers C and CUDA-C routines to efficiently run template matching on CPUs, or even on GPUs if available to you.</p>"},{"location":"notebooks/tm_multiple_templates/#set-detection-threshold-and-find-events","title":"Set detection threshold and find events\u00b6","text":"<p>We will use the time series of correlation coefficients to build an earthquake catalog. For that, we need to set a detection threshold and define all times above that threshold as triggers caused by near-repeats of the template event.</p>"},{"location":"notebooks/tm_multiple_templates/#assemble-all-detections-to-build-the-template-matching-catalog","title":"Assemble all detections to build the template matching catalog\u00b6","text":"<p>Use the trigger times to build the earthquake catalog and extract event waveforms on a given station/component.</p>"},{"location":"notebooks/tm_multiple_templates/#plot-some-waveforms","title":"Plot some waveforms\u00b6","text":""},{"location":"notebooks/tm_multiple_templates/#plot-inter-event-time-and-template-return-time-vs-detection-time","title":"Plot inter-event time and template return time vs detection time\u00b6","text":"<p>Having multiple templates, we can define two types of inter-event times:</p> <ul> <li>inter-event time: this is the time between two consecutive events in the catalog, disregarding their locations,</li> <li>return time: this is the time between two consecutive events detected by a same template, that is, two consecutive co-located events.</li> </ul> <p>We naively assembled all the detected events from all the templates, but closely located templates can detect the same events, leading to multiple detections in the catalog. These multiples are visible with horizontal lines in the inter-event time vs detection time plot (see below).</p>"},{"location":"notebooks/tm_multiple_templates/#de-lumping-the-catalog","title":"\"De-lumping\" the catalog\u00b6","text":"<p>Templates with similar waveforms and similar moveouts detect similar events. Thus, naively assembling the detected events from all templates results in \"lumped\" detections that represent multiple detections of the same event. (note: we avoid using \"clustered\" detections because the term \"cluster\" has lots of meaning in statistical seismology).</p> <p>In the following, we:</p> <ul> <li>compute inter-template distances,</li> <li>compute inter-template waveform similarity (correlation coefficient),</li> <li>flag the \"lumped\" detections based on three criteria on inter-event time, inter-event distance and inter-event waveform similarity.</li> </ul>"},{"location":"notebooks/tm_multiple_templates/#inter-template-distances","title":"Inter-template distances\u00b6","text":""},{"location":"notebooks/tm_multiple_templates/#inter-template-waveform-similarity","title":"Inter-template waveform similarity\u00b6","text":""},{"location":"notebooks/tm_multiple_templates/#flag-multiples","title":"Flag multiples\u00b6","text":""},{"location":"notebooks/tm_multiple_templates/#relative-magnitudes","title":"Relative magnitudes\u00b6","text":""},{"location":"notebooks/tm_one_template/","title":"Tm one template","text":"In\u00a0[1]: Copied! <pre>import os\nimport fast_matched_filter as fmf\nimport glob\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport obspy as obs\nimport pandas as pd\n</pre> import os import fast_matched_filter as fmf import glob import numpy as np import matplotlib.pyplot as plt import obspy as obs import pandas as pd  In\u00a0[2]: Copied! <pre># path variables and file names\nDIR_WAVEFORMS = \"/home/ebeauce/SSA_EQ_DETECTION_WORKSHOP/data\" # REPLACE THIS PATH WITH WHEREVER YOU DOWNLOADED THE DATA\nDIR_CATALOG = \"../picks_phasenet/\"\n\nSTATION_FILE = \"adloc_stations.csv\"\nEVENT_FILE = \"adloc_events.csv\"\nPICK_FILE = \"adloc_picks.csv\"\n</pre> # path variables and file names DIR_WAVEFORMS = \"/home/ebeauce/SSA_EQ_DETECTION_WORKSHOP/data\" # REPLACE THIS PATH WITH WHEREVER YOU DOWNLOADED THE DATA DIR_CATALOG = \"../picks_phasenet/\"  STATION_FILE = \"adloc_stations.csv\" EVENT_FILE = \"adloc_events.csv\" PICK_FILE = \"adloc_picks.csv\" In\u00a0[3]: Copied! <pre>station_meta = pd.read_csv(os.path.join(DIR_CATALOG, STATION_FILE))\nstation_meta\n</pre> station_meta = pd.read_csv(os.path.join(DIR_CATALOG, STATION_FILE)) station_meta Out[3]: network station location instrument component latitude longitude elevation_m depth_km provider station_id station_term_time_p station_term_time_s station_term_amplitude 0 CI CCC NaN BH ENZ 35.524950 -117.364530 670.0 -0.6700 SCEDC CI.CCC..BH 0.266086 0.500831 0.049399 1 CI CCC NaN HH ENZ 35.524950 -117.364530 670.0 -0.6700 SCEDC CI.CCC..HH 0.295428 0.518465 0.191475 2 CI CCC NaN HN ENZ 35.524950 -117.364530 670.0 -0.6700 SCEDC CI.CCC..HN 0.296263 0.541148 0.064485 3 CI CLC NaN BH ENZ 35.815740 -117.597510 775.0 -0.7750 SCEDC CI.CLC..BH -0.231963 -0.415271 -0.331371 4 CI CLC NaN HH ENZ 35.815740 -117.597510 775.0 -0.7750 SCEDC CI.CLC..HH -0.168743 -0.390045 -0.140313 5 CI CLC NaN HN ENZ 35.815740 -117.597510 775.0 -0.7750 SCEDC CI.CLC..HN -0.175671 -0.388116 -0.249066 6 CI DTP NaN BH ENZ 35.267420 -117.845810 908.0 -0.9080 SCEDC CI.DTP..BH -0.305881 -0.602459 -0.503411 7 CI DTP NaN HH ENZ 35.267420 -117.845810 908.0 -0.9080 SCEDC CI.DTP..HH -0.263705 -0.564867 -0.437951 8 CI DTP NaN HN ENZ 35.267420 -117.845810 908.0 -0.9080 SCEDC CI.DTP..HN -0.244383 -0.538990 -0.500516 9 CI JRC2 NaN BH ENZ 35.982490 -117.808850 1469.0 -1.4690 SCEDC CI.JRC2..BH 0.011361 -0.080285 -0.039941 10 CI JRC2 NaN HH ENZ 35.982490 -117.808850 1469.0 -1.4690 SCEDC CI.JRC2..HH 0.053539 -0.052748 0.068213 11 CI JRC2 NaN HN ENZ 35.982490 -117.808850 1469.0 -1.4690 SCEDC CI.JRC2..HN 0.059764 -0.045991 -0.007637 12 CI LRL NaN BH ENZ 35.479540 -117.682120 1340.0 -1.3400 SCEDC CI.LRL..BH -0.295604 -0.540857 0.033788 13 CI LRL NaN HH ENZ 35.479540 -117.682120 1340.0 -1.3400 SCEDC CI.LRL..HH -0.268381 -0.513955 0.146876 14 CI LRL NaN HN ENZ 35.479540 -117.682120 1340.0 -1.3400 SCEDC CI.LRL..HN -0.266329 -0.503088 0.045499 15 CI LRL 2C HN ENZ 35.479540 -117.682120 1340.0 -1.3400 SCEDC CI.LRL.2C.HN 0.000000 0.000000 0.000000 16 CI MPM NaN BH ENZ 36.057990 -117.489010 1839.0 -1.8390 SCEDC CI.MPM..BH -0.011825 -0.098089 -0.518793 17 CI MPM NaN HH ENZ 36.057990 -117.489010 1839.0 -1.8390 SCEDC CI.MPM..HH 0.009095 -0.081896 -0.459349 18 CI MPM NaN HN ENZ 36.057990 -117.489010 1839.0 -1.8390 SCEDC CI.MPM..HN 0.011870 -0.052277 -0.532764 19 CI Q0072 01 HN ENZ 35.609617 -117.666721 695.0 -0.6950 SCEDC CI.Q0072.01.HN 0.000000 0.000000 0.000000 20 CI SLA NaN BH ENZ 35.890950 -117.283320 1174.0 -1.1740 SCEDC CI.SLA..BH 0.066893 0.118500 -0.081634 21 CI SLA NaN HH ENZ 35.890950 -117.283320 1174.0 -1.1740 SCEDC CI.SLA..HH 0.089833 0.128589 -0.042928 22 CI SLA NaN HN ENZ 35.890950 -117.283320 1174.0 -1.1740 SCEDC CI.SLA..HN 0.093526 0.168238 -0.150381 23 CI SRT NaN BH ENZ 35.692350 -117.750510 667.0 -0.6670 SCEDC CI.SRT..BH 0.148171 0.653411 -0.253527 24 CI SRT NaN HH ENZ 35.692350 -117.750510 667.0 -0.6670 SCEDC CI.SRT..HH 0.183868 0.641707 -0.208859 25 CI SRT NaN HN ENZ 35.692350 -117.750510 667.0 -0.6670 SCEDC CI.SRT..HN 0.175475 0.674587 -0.295869 26 CI TOW2 NaN BH ENZ 35.808560 -117.764880 685.0 -0.6850 SCEDC CI.TOW2..BH 0.268083 0.816279 0.028038 27 CI TOW2 NaN HH ENZ 35.808560 -117.764880 685.0 -0.6850 SCEDC CI.TOW2..HH 0.285970 0.831533 0.102681 28 CI TOW2 NaN HN ENZ 35.808560 -117.764880 685.0 -0.6850 SCEDC CI.TOW2..HN 0.285113 0.843886 -0.006698 29 CI WBM NaN BH ENZ 35.608390 -117.890490 892.0 -0.8920 SCEDC CI.WBM..BH 0.136927 0.128744 -0.166279 30 CI WBM NaN HH ENZ 35.608390 -117.890490 892.0 -0.8920 SCEDC CI.WBM..HH 0.152299 0.124250 -0.138614 31 CI WBM NaN HN ENZ 35.608390 -117.890490 892.0 -0.8920 SCEDC CI.WBM..HN 0.170719 0.179461 -0.101436 32 CI WBM 2C HN ENZ 35.608390 -117.890490 892.0 -0.8920 SCEDC CI.WBM.2C.HN 0.000000 0.000000 0.000000 33 CI WCS2 NaN BH ENZ 36.025210 -117.765260 1143.0 -1.1430 SCEDC CI.WCS2..BH 0.030349 -0.126935 0.023642 34 CI WCS2 NaN HH ENZ 36.025210 -117.765260 1143.0 -1.1430 SCEDC CI.WCS2..HH 0.065321 -0.099447 0.146628 35 CI WCS2 NaN HN ENZ 36.025210 -117.765260 1143.0 -1.1430 SCEDC CI.WCS2..HN 0.061651 -0.096450 0.037563 36 CI WMF NaN BH ENZ 36.117580 -117.854860 1537.4 -1.5374 SCEDC CI.WMF..BH 0.039416 -0.005761 -0.165183 37 CI WMF NaN HH ENZ 36.117580 -117.854860 1537.4 -1.5374 SCEDC CI.WMF..HH 0.075658 0.005327 -0.079900 38 CI WMF NaN HN ENZ 36.117580 -117.854860 1537.4 -1.5374 SCEDC CI.WMF..HN 0.085427 0.025273 -0.240971 39 CI WMF 2C HN ENZ 36.117580 -117.854860 1537.4 -1.5374 SCEDC CI.WMF.2C.HN 0.000000 0.000000 0.000000 40 CI WNM NaN EH Z 35.842200 -117.906160 974.3 -0.9743 SCEDC CI.WNM..EH -0.026889 -0.070269 -0.332510 41 CI WNM NaN HN ENZ 35.842200 -117.906160 974.3 -0.9743 SCEDC CI.WNM..HN -0.011659 -0.118223 -0.038719 42 CI WNM 2C HN ENZ 35.842200 -117.906160 974.3 -0.9743 SCEDC CI.WNM.2C.HN 0.000000 0.000000 0.000000 43 CI WRC2 NaN BH ENZ 35.947900 -117.650380 943.0 -0.9430 SCEDC CI.WRC2..BH -0.023860 -0.043523 0.117833 44 CI WRC2 NaN HH ENZ 35.947900 -117.650380 943.0 -0.9430 SCEDC CI.WRC2..HH 0.010633 -0.029488 0.165234 45 CI WRC2 NaN HN ENZ 35.947900 -117.650380 943.0 -0.9430 SCEDC CI.WRC2..HN 0.014658 -0.021367 0.103905 46 CI WRV2 NaN EH Z 36.007740 -117.890400 1070.0 -1.0700 SCEDC CI.WRV2..EH -0.003461 -0.154572 -0.355199 47 CI WRV2 NaN HN ENZ 36.007740 -117.890400 1070.0 -1.0700 SCEDC CI.WRV2..HN 0.017317 -0.137916 -0.273270 48 CI WRV2 2C HN ENZ 36.007740 -117.890400 1070.0 -1.0700 SCEDC CI.WRV2.2C.HN 0.000000 0.000000 0.000000 49 CI WVP2 NaN EH Z 35.949390 -117.817690 1465.0 -1.4650 SCEDC CI.WVP2..EH 0.020325 0.008989 -0.341606 50 CI WVP2 NaN HN ENZ 35.949390 -117.817690 1465.0 -1.4650 SCEDC CI.WVP2..HN 0.024742 -0.103024 -0.089014 51 CI WVP2 2C HN ENZ 35.949390 -117.817690 1465.0 -1.4650 SCEDC CI.WVP2.2C.HN 0.000000 0.000000 0.000000 <p>The following shows a very rudimentary map of the station network. Look into the <code>cartopy</code> package for more sophisticated maps.</p> In\u00a0[4]: Copied! <pre>_station_meta = station_meta.drop_duplicates(\"station\")\n\nfig, ax = plt.subplots(num=\"station_network\", figsize=(10, 10))\nax.scatter(_station_meta[\"longitude\"], _station_meta[\"latitude\"], marker=\"v\", color=\"k\")\nfor idx, row in _station_meta.iterrows():\n    ax.text(row.longitude + 0.01, row.latitude + 0.01, row.station, va=\"bottom\", ha=\"left\")\nax.set_xlabel(\"Longitude\")\nax.set_ylabel(\"Latitude\")\nax.grid()\nax.set_title(\"Stations used to build the PhaseNet catalog\")\n</pre> _station_meta = station_meta.drop_duplicates(\"station\")  fig, ax = plt.subplots(num=\"station_network\", figsize=(10, 10)) ax.scatter(_station_meta[\"longitude\"], _station_meta[\"latitude\"], marker=\"v\", color=\"k\") for idx, row in _station_meta.iterrows():     ax.text(row.longitude + 0.01, row.latitude + 0.01, row.station, va=\"bottom\", ha=\"left\") ax.set_xlabel(\"Longitude\") ax.set_ylabel(\"Latitude\") ax.grid() ax.set_title(\"Stations used to build the PhaseNet catalog\") Out[4]: <pre>Text(0.5, 1.0, 'Stations used to build the PhaseNet catalog')</pre> In\u00a0[5]: Copied! <pre>event_meta = pd.read_csv(os.path.join(DIR_CATALOG, EVENT_FILE))\nevent_meta\n</pre> event_meta = pd.read_csv(os.path.join(DIR_CATALOG, EVENT_FILE)) event_meta Out[5]: time adloc_score adloc_residual_time num_picks magnitude adloc_residual_amplitude event_index longitude latitude depth_km 0 2019-07-04 00:46:47.342963596 0.866596 0.057355 35 0.595368 0.165480 6720 -117.882570 36.091088 4.643969 1 2019-07-04 00:55:32.648412579 0.781116 0.203567 16 1.142510 0.170509 17122 -117.799226 35.378160 11.078458 2 2019-07-04 00:56:37.232733104 0.908073 0.086183 42 0.912494 0.166681 5411 -117.880902 36.091986 4.854336 3 2019-07-04 02:00:39.149363202 0.814322 0.036164 15 0.209530 0.092534 17868 -117.866468 36.093520 4.981447 4 2019-07-04 03:05:31.018885833 0.799281 0.080708 11 0.104050 0.156833 25037 -117.846320 36.100386 5.943363 ... ... ... ... ... ... ... ... ... ... ... 20898 2019-07-09 23:58:14.298499048 0.933013 0.097816 67 1.543257 0.131523 1907 -117.711811 35.926468 6.652020 20899 2019-07-09 23:58:47.701746285 0.882434 0.090185 73 1.087089 0.140159 2051 -117.604263 35.797450 6.617413 20900 2019-07-09 23:59:05.102247662 0.798047 0.435077 26 1.147040 0.170994 12567 -117.509729 35.692722 12.815041 20901 2019-07-09 23:59:40.257837813 0.971081 0.065523 35 1.161323 0.068586 7726 -117.846289 36.061435 5.224666 20902 2019-07-09 23:59:49.650466544 0.800524 0.023674 15 0.936622 0.095959 18566 -117.896100 36.095886 6.761860 <p>20903 rows \u00d7 10 columns</p> In\u00a0[6]: Copied! <pre>picks = pd.read_csv(os.path.join(DIR_CATALOG, PICK_FILE))\npicks\n</pre> picks = pd.read_csv(os.path.join(DIR_CATALOG, PICK_FILE)) picks Out[6]: station_id phase_index phase_time phase_score phase_type dt_s phase_polarity phase_amplitude event_index sp_ratio event_amplitude adloc_mask adloc_residual_time adloc_residual_amplitude 0 CI.WMF..BH 280874 2019-07-04 00:46:48.759 0.594 P 0.01 0.110 -3.213107 6720 0.955091 3.124152e-07 1 0.003919 0.012320 1 CI.WMF..HH 280881 2019-07-04 00:46:48.818 0.973 P 0.01 0.938 -3.077638 6720 0.948896 3.368390e-07 1 0.026491 0.063669 2 CI.WMF..HN 280881 2019-07-04 00:46:48.818 0.973 P 0.01 0.898 -3.128019 6720 1.598008 2.146277e-07 1 0.017580 0.173702 3 CI.WRV2..EH 280945 2019-07-04 00:46:49.450 0.977 P 0.01 -0.855 -3.464453 6720 1.000000 3.298200e-07 1 0.077194 0.244381 4 CI.WRV2..HN 280945 2019-07-04 00:46:49.450 0.941 P 0.01 -0.906 -3.585863 6720 1.147235 3.908305e-07 1 0.056694 0.041691 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 955700 CI.WRV2..HN 8639355 2019-07-09 23:59:53.550 0.688 S 0.01 -0.022 -3.384471 18566 0.644492 1.973834e-06 1 0.019093 0.028409 955701 CI.JRC2..HH 8639499 2019-07-09 23:59:54.998 0.402 S 0.01 -0.023 -3.446238 18566 0.647843 6.197256e-06 1 0.020364 -0.154404 955702 CI.JRC2..HN 8639499 2019-07-09 23:59:54.998 0.344 S 0.01 -0.016 -3.531800 18566 0.731796 4.075353e-06 1 0.012773 -0.164659 955703 CI.WVP2..EH 8639574 2019-07-09 23:59:55.740 0.314 S 0.01 -0.103 -3.488117 18566 0.636005 2.815148e-06 1 -0.092233 0.316712 955704 CI.WVP2..HN 8639576 2019-07-09 23:59:55.760 0.695 S 0.01 0.076 -3.218676 18566 0.825223 4.657352e-06 1 0.039319 0.332722 <p>955705 rows \u00d7 14 columns</p> In\u00a0[7]: Copied! <pre>def fetch_event_waveforms(\n    event_picks,\n    dir_waveforms=DIR_WAVEFORMS,\n    time_before_phase_onset_sec=2.0,\n    duration_sec=10.0\n    ):\n    \"\"\"\n    Fetches the waveforms for a given event based on the picks.\n\n    Parameters\n    ----------\n    event_picks : pandas.DataFrame\n        DataFrame containing the picks for the event.\n    dir_waveforms : str, optional\n        Directory where the waveform data is stored, by default DIR_WAVEFORMS.\n    time_before_phase_onset_sec : float, optional\n        Time in seconds to start the waveform before the phase onset, by default 2.0.\n    duration_sec : float, optional\n        Duration in seconds of the waveform to fetch, by default 10.0.\n\n    Returns\n    -------\n    obspy.Stream\n        Stream object containing the fetched waveforms.\n    \"\"\"\n    stream = obs.Stream()\n    for _, pick in event_picks.iterrows():\n        # check whether we have a miniseed file for this waveform\n        if pick.phase_type == \"P\":\n            files = glob.glob(os.path.join(dir_waveforms, pick.station_id + \"Z*mseed\"))\n        elif pick.phase_type == \"S\":\n            files = glob.glob(os.path.join(dir_waveforms, pick.station_id + \"[N,E]*mseed\"))\n        starttime = obs.UTCDateTime(pick.phase_time) - time_before_phase_onset_sec\n        endtime = starttime + duration_sec\n        for _file in files:\n            stream += obs.read(\n                _file,\n                starttime=starttime,\n                endtime=endtime\n            )\n    return stream\n    \n</pre> def fetch_event_waveforms(     event_picks,     dir_waveforms=DIR_WAVEFORMS,     time_before_phase_onset_sec=2.0,     duration_sec=10.0     ):     \"\"\"     Fetches the waveforms for a given event based on the picks.      Parameters     ----------     event_picks : pandas.DataFrame         DataFrame containing the picks for the event.     dir_waveforms : str, optional         Directory where the waveform data is stored, by default DIR_WAVEFORMS.     time_before_phase_onset_sec : float, optional         Time in seconds to start the waveform before the phase onset, by default 2.0.     duration_sec : float, optional         Duration in seconds of the waveform to fetch, by default 10.0.      Returns     -------     obspy.Stream         Stream object containing the fetched waveforms.     \"\"\"     stream = obs.Stream()     for _, pick in event_picks.iterrows():         # check whether we have a miniseed file for this waveform         if pick.phase_type == \"P\":             files = glob.glob(os.path.join(dir_waveforms, pick.station_id + \"Z*mseed\"))         elif pick.phase_type == \"S\":             files = glob.glob(os.path.join(dir_waveforms, pick.station_id + \"[N,E]*mseed\"))         starttime = obs.UTCDateTime(pick.phase_time) - time_before_phase_onset_sec         endtime = starttime + duration_sec         for _file in files:             stream += obs.read(                 _file,                 starttime=starttime,                 endtime=endtime             )     return stream      In\u00a0[8]: Copied! <pre># explore event_meta to find a nice intermediate-size earthquake we could plot\nevent_meta.head(20)\n</pre> # explore event_meta to find a nice intermediate-size earthquake we could plot event_meta.head(20) Out[8]: time adloc_score adloc_residual_time num_picks magnitude adloc_residual_amplitude event_index longitude latitude depth_km 0 2019-07-04 00:46:47.342963596 0.866596 0.057355 35 0.595368 0.165480 6720 -117.882570 36.091088 4.643969 1 2019-07-04 00:55:32.648412579 0.781116 0.203567 16 1.142510 0.170509 17122 -117.799226 35.378160 11.078458 2 2019-07-04 00:56:37.232733104 0.908073 0.086183 42 0.912494 0.166681 5411 -117.880902 36.091986 4.854336 3 2019-07-04 02:00:39.149363202 0.814322 0.036164 15 0.209530 0.092534 17868 -117.866468 36.093520 4.981447 4 2019-07-04 03:05:31.018885833 0.799281 0.080708 11 0.104050 0.156833 25037 -117.846320 36.100386 5.943363 5 2019-07-04 03:20:28.674438914 0.755451 0.050436 44 0.869927 0.111885 6929 -117.673684 36.114308 5.894466 6 2019-07-04 04:03:01.619369274 0.897311 0.143631 32 0.386310 0.214381 7607 -117.805077 36.016063 0.396071 7 2019-07-04 05:16:47.223353119 0.835094 0.053522 22 0.575433 0.163675 13098 -117.879256 36.090409 4.268292 8 2019-07-04 06:57:32.758991812 0.922386 0.065846 23 0.256281 0.065212 15698 -117.867587 36.081665 5.939931 9 2019-07-04 11:51:07.805591259 0.692042 0.075023 48 0.818316 0.102087 4380 -117.671909 36.118573 6.085804 10 2019-07-04 15:36:04.228420696 0.652888 0.012137 9 -0.633904 0.167058 27370 -117.785299 36.005578 3.234981 11 2019-07-04 15:42:47.932558745 0.597856 0.072740 28 0.933088 0.151623 9740 -117.501116 35.707382 14.496446 12 2019-07-04 16:07:20.003321194 0.729246 0.093399 33 0.835854 0.133106 9365 -117.491503 35.711241 13.846898 13 2019-07-04 16:11:46.920083440 0.711579 0.516367 18 1.734816 0.117952 7098 -117.877675 35.196445 31.000000 14 2019-07-04 16:13:43.094792540 0.849673 0.070048 83 1.653859 0.111108 2305 -117.493716 35.710090 13.375377 15 2019-07-04 16:16:07.085486307 0.814365 0.040161 10 0.587583 0.099619 26520 -117.540198 35.689022 14.537368 16 2019-07-04 17:02:55.057058245 0.770696 0.079675 90 4.476724 0.143489 1101 -117.495094 35.711607 13.586411 17 2019-07-04 17:04:02.231614981 0.664676 0.064038 41 2.062226 0.186214 5379 -117.488446 35.711139 14.001705 18 2019-07-04 17:05:05.071421677 0.509088 0.089882 29 1.471434 0.157048 12115 -117.491307 35.710881 13.223374 19 2019-07-04 17:08:51.664841725 0.666790 0.046444 15 0.657653 0.175270 23044 -117.504506 35.706123 14.570921 In\u00a0[9]: Copied! <pre># feel free to play with the event index to plot different events\nEVENT_IDX = 1101\n\nevent_meta.set_index(\"event_index\").loc[EVENT_IDX]\n</pre> # feel free to play with the event index to plot different events EVENT_IDX = 1101  event_meta.set_index(\"event_index\").loc[EVENT_IDX] Out[9]: <pre>time                        2019-07-04 17:02:55.057058245\nadloc_score                                      0.770696\nadloc_residual_time                              0.079675\nnum_picks                                              90\nmagnitude                                        4.476724\nadloc_residual_amplitude                         0.143489\nlongitude                                     -117.495094\nlatitude                                        35.711607\ndepth_km                                        13.586411\nName: 1101, dtype: object</pre> In\u00a0[10]: Copied! <pre># fetch the corresponding picks for this event\nevent_picks = picks[picks[\"event_index\"] == EVENT_IDX]\nevent_picks\n</pre> # fetch the corresponding picks for this event event_picks = picks[picks[\"event_index\"] == EVENT_IDX] event_picks Out[10]: station_id phase_index phase_time phase_score phase_type dt_s phase_polarity phase_amplitude event_index sp_ratio event_amplitude adloc_mask adloc_residual_time adloc_residual_amplitude 565 CI.CLC..HN 6137848 2019-07-04 17:02:58.488 0.969 P 0.01 -0.879 -0.511873 1101 1.017569 2.474507e-07 1 -0.010048 -0.054426 566 CI.CLC..BH 6137847 2019-07-04 17:02:58.489 0.633 P 0.01 -0.324 -0.666956 1101 1.014822 2.022085e-07 1 0.048153 -0.127168 567 CI.CLC..HH 6137849 2019-07-04 17:02:58.498 0.965 P 0.01 -0.883 -0.586030 1101 1.086886 2.141392e-07 1 -0.007135 -0.236796 568 CI.SRT..HN 6137990 2019-07-04 17:02:59.908 0.879 P 0.01 0.699 -0.643401 1101 1.377537 4.387864e-07 1 -0.069463 0.065039 569 CI.SRT..HH 6137990 2019-07-04 17:02:59.908 0.891 P 0.01 0.734 -0.601019 1101 0.729745 5.910421e-07 1 -0.078144 0.020251 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 650 CI.WMF..HH 6139185 2019-07-04 17:03:11.858 0.672 S 0.01 0.021 -1.368759 1101 1.096820 3.073536e-07 1 0.140179 -0.329117 651 CI.WMF..HN 6139189 2019-07-04 17:03:11.898 0.633 S 0.01 0.026 -1.371407 1101 0.989939 2.891483e-07 1 0.159858 -0.171350 652 CI.DTP..BH 6139200 2019-07-04 17:03:12.019 0.734 S 0.01 0.035 -1.322484 1101 1.385395 1.624267e-07 1 0.068940 0.175198 653 CI.DTP..HH 6139203 2019-07-04 17:03:12.038 0.707 S 0.01 -0.015 -1.306977 1101 1.122809 3.205840e-07 1 0.052824 0.125587 654 CI.DTP..HN 6139211 2019-07-04 17:03:12.118 0.625 S 0.01 0.043 -1.282829 1101 0.863380 2.702479e-07 1 0.105026 0.211253 <p>90 rows \u00d7 14 columns</p> In\u00a0[11]: Copied! <pre># fetch the waveforms\nevent_waveforms = fetch_event_waveforms(event_picks, time_before_phase_onset_sec=10., duration_sec=30.)\nprint(event_waveforms.__str__(extended=True))\n</pre> # fetch the waveforms event_waveforms = fetch_event_waveforms(event_picks, time_before_phase_onset_sec=10., duration_sec=30.) print(event_waveforms.__str__(extended=True)) <pre>42 Trace(s) in Stream:\nCI.CLC..HHZ  | 2019-07-04T17:02:48.478300Z - 2019-07-04T17:03:18.478300Z | 25.0 Hz, 751 samples\nCI.SRT..HHZ  | 2019-07-04T17:02:49.918300Z - 2019-07-04T17:03:19.918300Z | 25.0 Hz, 751 samples\nCI.CCC..HHZ  | 2019-07-04T17:02:50.118300Z - 2019-07-04T17:03:20.118300Z | 25.0 Hz, 751 samples\nCI.SLA..HHZ  | 2019-07-04T17:02:50.518300Z - 2019-07-04T17:03:20.518300Z | 25.0 Hz, 751 samples\nCI.TOW2..HHZ | 2019-07-04T17:02:50.518300Z - 2019-07-04T17:03:20.518300Z | 25.0 Hz, 751 samples\nCI.LRL..HHZ  | 2019-07-04T17:02:50.638300Z - 2019-07-04T17:03:20.638300Z | 25.0 Hz, 751 samples\nCI.WRC2..HHZ | 2019-07-04T17:02:50.718300Z - 2019-07-04T17:03:20.718300Z | 25.0 Hz, 751 samples\nCI.CLC..HHN  | 2019-07-04T17:02:50.958300Z - 2019-07-04T17:03:20.958300Z | 25.0 Hz, 751 samples\nCI.CLC..HHE  | 2019-07-04T17:02:50.958300Z - 2019-07-04T17:03:20.958300Z | 25.0 Hz, 751 samples\nCI.MPM..HHZ  | 2019-07-04T17:02:52.038300Z - 2019-07-04T17:03:22.038300Z | 25.0 Hz, 751 samples\nCI.WBM..HHZ  | 2019-07-04T17:02:52.123100Z - 2019-07-04T17:03:22.123100Z | 25.0 Hz, 751 samples\nCI.WVP2..EHZ | 2019-07-04T17:02:52.160000Z - 2019-07-04T17:03:22.160000Z | 25.0 Hz, 751 samples\nCI.WNM..EHZ  | 2019-07-04T17:02:52.160000Z - 2019-07-04T17:03:22.160000Z | 25.0 Hz, 751 samples\nCI.JRC2..HHZ | 2019-07-04T17:02:52.558300Z - 2019-07-04T17:03:22.558300Z | 25.0 Hz, 751 samples\nCI.WCS2..HHZ | 2019-07-04T17:02:52.598300Z - 2019-07-04T17:03:22.598300Z | 25.0 Hz, 751 samples\nCI.WRV2..EHZ | 2019-07-04T17:02:53.600000Z - 2019-07-04T17:03:23.600000Z | 25.0 Hz, 751 samples\nCI.SRT..HHN  | 2019-07-04T17:02:53.838300Z - 2019-07-04T17:03:23.838300Z | 25.0 Hz, 751 samples\nCI.SRT..HHE  | 2019-07-04T17:02:53.838300Z - 2019-07-04T17:03:23.838300Z | 25.0 Hz, 751 samples\nCI.CCC..HHN  | 2019-07-04T17:02:54.078300Z - 2019-07-04T17:03:24.078300Z | 25.0 Hz, 751 samples\nCI.CCC..HHE  | 2019-07-04T17:02:54.078300Z - 2019-07-04T17:03:24.078300Z | 25.0 Hz, 751 samples\nCI.SLA..HHE  | 2019-07-04T17:02:54.638300Z - 2019-07-04T17:03:24.638300Z | 25.0 Hz, 751 samples\nCI.SLA..HHN  | 2019-07-04T17:02:54.638300Z - 2019-07-04T17:03:24.638300Z | 25.0 Hz, 751 samples\nCI.LRL..HHN  | 2019-07-04T17:02:54.718300Z - 2019-07-04T17:03:24.718300Z | 25.0 Hz, 751 samples\nCI.LRL..HHE  | 2019-07-04T17:02:54.718300Z - 2019-07-04T17:03:24.718300Z | 25.0 Hz, 751 samples\nCI.WMF..HHZ  | 2019-07-04T17:02:54.798300Z - 2019-07-04T17:03:24.798300Z | 25.0 Hz, 751 samples\nCI.DTP..HHZ  | 2019-07-04T17:02:54.958300Z - 2019-07-04T17:03:24.958300Z | 25.0 Hz, 751 samples\nCI.TOW2..HHN | 2019-07-04T17:02:54.998300Z - 2019-07-04T17:03:24.998300Z | 25.0 Hz, 751 samples\nCI.TOW2..HHE | 2019-07-04T17:02:54.998300Z - 2019-07-04T17:03:24.998300Z | 25.0 Hz, 751 samples\nCI.WRC2..HHN | 2019-07-04T17:02:54.998300Z - 2019-07-04T17:03:24.998300Z | 25.0 Hz, 751 samples\nCI.WRC2..HHE | 2019-07-04T17:02:54.998300Z - 2019-07-04T17:03:24.998300Z | 25.0 Hz, 751 samples\nCI.WBM..HHE  | 2019-07-04T17:02:57.243100Z - 2019-07-04T17:03:27.243100Z | 25.0 Hz, 751 samples\nCI.WBM..HHN  | 2019-07-04T17:02:57.243100Z - 2019-07-04T17:03:27.243100Z | 25.0 Hz, 751 samples\nCI.MPM..HHE  | 2019-07-04T17:02:57.318300Z - 2019-07-04T17:03:27.318300Z | 25.0 Hz, 751 samples\nCI.MPM..HHN  | 2019-07-04T17:02:57.318300Z - 2019-07-04T17:03:27.318300Z | 25.0 Hz, 751 samples\nCI.JRC2..HHN | 2019-07-04T17:02:57.958300Z - 2019-07-04T17:03:27.958300Z | 25.0 Hz, 751 samples\nCI.JRC2..HHE | 2019-07-04T17:02:57.958300Z - 2019-07-04T17:03:27.958300Z | 25.0 Hz, 751 samples\nCI.WCS2..HHE | 2019-07-04T17:02:58.118300Z - 2019-07-04T17:03:28.118300Z | 25.0 Hz, 751 samples\nCI.WCS2..HHN | 2019-07-04T17:02:58.118300Z - 2019-07-04T17:03:28.118300Z | 25.0 Hz, 751 samples\nCI.WMF..HHN  | 2019-07-04T17:03:01.838300Z - 2019-07-04T17:03:31.838300Z | 25.0 Hz, 751 samples\nCI.WMF..HHE  | 2019-07-04T17:03:01.838300Z - 2019-07-04T17:03:31.838300Z | 25.0 Hz, 751 samples\nCI.DTP..HHN  | 2019-07-04T17:03:02.038300Z - 2019-07-04T17:03:32.038300Z | 25.0 Hz, 751 samples\nCI.DTP..HHE  | 2019-07-04T17:03:02.038300Z - 2019-07-04T17:03:32.038300Z | 25.0 Hz, 751 samples\n</pre> In\u00a0[12]: Copied! <pre># plot them!\nfig = event_waveforms.select(component=\"Z\").plot(equal_scale=False)\n</pre> # plot them! fig = event_waveforms.select(component=\"Z\").plot(equal_scale=False) In\u00a0[13]: Copied! <pre>selected_event_meta = event_meta.set_index(\"event_index\").loc[EVENT_IDX]\nselected_event_meta\n</pre> selected_event_meta = event_meta.set_index(\"event_index\").loc[EVENT_IDX] selected_event_meta Out[13]: <pre>time                        2019-07-04 17:02:55.057058245\nadloc_score                                      0.770696\nadloc_residual_time                              0.079675\nnum_picks                                              90\nmagnitude                                        4.476724\nadloc_residual_amplitude                         0.143489\nlongitude                                     -117.495094\nlatitude                                        35.711607\ndepth_km                                        13.586411\nName: 1101, dtype: object</pre> In\u00a0[14]: Copied! <pre>selected_event_picks = picks[picks[\"event_index\"] == EVENT_IDX]\nselected_event_picks\n</pre> selected_event_picks = picks[picks[\"event_index\"] == EVENT_IDX] selected_event_picks Out[14]: station_id phase_index phase_time phase_score phase_type dt_s phase_polarity phase_amplitude event_index sp_ratio event_amplitude adloc_mask adloc_residual_time adloc_residual_amplitude 565 CI.CLC..HN 6137848 2019-07-04 17:02:58.488 0.969 P 0.01 -0.879 -0.511873 1101 1.017569 2.474507e-07 1 -0.010048 -0.054426 566 CI.CLC..BH 6137847 2019-07-04 17:02:58.489 0.633 P 0.01 -0.324 -0.666956 1101 1.014822 2.022085e-07 1 0.048153 -0.127168 567 CI.CLC..HH 6137849 2019-07-04 17:02:58.498 0.965 P 0.01 -0.883 -0.586030 1101 1.086886 2.141392e-07 1 -0.007135 -0.236796 568 CI.SRT..HN 6137990 2019-07-04 17:02:59.908 0.879 P 0.01 0.699 -0.643401 1101 1.377537 4.387864e-07 1 -0.069463 0.065039 569 CI.SRT..HH 6137990 2019-07-04 17:02:59.908 0.891 P 0.01 0.734 -0.601019 1101 0.729745 5.910421e-07 1 -0.078144 0.020251 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 650 CI.WMF..HH 6139185 2019-07-04 17:03:11.858 0.672 S 0.01 0.021 -1.368759 1101 1.096820 3.073536e-07 1 0.140179 -0.329117 651 CI.WMF..HN 6139189 2019-07-04 17:03:11.898 0.633 S 0.01 0.026 -1.371407 1101 0.989939 2.891483e-07 1 0.159858 -0.171350 652 CI.DTP..BH 6139200 2019-07-04 17:03:12.019 0.734 S 0.01 0.035 -1.322484 1101 1.385395 1.624267e-07 1 0.068940 0.175198 653 CI.DTP..HH 6139203 2019-07-04 17:03:12.038 0.707 S 0.01 -0.015 -1.306977 1101 1.122809 3.205840e-07 1 0.052824 0.125587 654 CI.DTP..HN 6139211 2019-07-04 17:03:12.118 0.625 S 0.01 0.043 -1.282829 1101 0.863380 2.702479e-07 1 0.105026 0.211253 <p>90 rows \u00d7 14 columns</p> In\u00a0[15]: Copied! <pre>def fetch_day_waveforms(dir_waveforms):\n    \"\"\"\n    Fetches the continuous seismograms for a given day.\n\n    Parameters\n    ----------\n    dir_waveforms : str\n        Directory where the waveform data is stored, by default DIR_WAVEFORMS.\n\n    Returns\n    -------\n    obspy.Stream\n        Stream object containing the fetched continuous seismograms.\n    \"\"\"\n    stream = obs.Stream()\n    files = glob.glob(os.path.join(dir_waveforms, \"*mseed\"))\n    for _file in files:\n        stream += obs.read(_file)\n    return stream\n</pre> def fetch_day_waveforms(dir_waveforms):     \"\"\"     Fetches the continuous seismograms for a given day.      Parameters     ----------     dir_waveforms : str         Directory where the waveform data is stored, by default DIR_WAVEFORMS.      Returns     -------     obspy.Stream         Stream object containing the fetched continuous seismograms.     \"\"\"     stream = obs.Stream()     files = glob.glob(os.path.join(dir_waveforms, \"*mseed\"))     for _file in files:         stream += obs.read(_file)     return stream In\u00a0[16]: Copied! <pre># first, read the continuous seismograms into an `obspy.Stream`\ncontinuous_seismograms = fetch_day_waveforms(DIR_WAVEFORMS)\nprint(continuous_seismograms.__str__(extended=True))\n</pre> # first, read the continuous seismograms into an `obspy.Stream` continuous_seismograms = fetch_day_waveforms(DIR_WAVEFORMS) print(continuous_seismograms.__str__(extended=True)) <pre>57 Trace(s) in Stream:\nCI.WCS2..HHE | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nPB.B916..EHZ | 2019-07-03T23:59:59.998200Z - 2019-07-04T23:59:59.958200Z | 25.0 Hz, 2160000 samples\nCI.CLC..HHN  | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.WRV2..EHZ | 2019-07-04T00:00:00.000000Z - 2019-07-04T23:59:59.960000Z | 25.0 Hz, 2160000 samples\nPB.B916..EH2 | 2019-07-03T23:59:59.998200Z - 2019-07-04T23:59:59.958200Z | 25.0 Hz, 2160000 samples\nPB.B917..EH1 | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.WMF..HHZ  | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nPB.B918..EHZ | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nPB.B918..EH2 | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.TOW2..HHZ | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.SLA..HHE  | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.WRC2..HHZ | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.JRC2..HHZ | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.CCC..HHZ  | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.DTP..HHN  | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.SRT..HHZ  | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.LRL..HHZ  | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.SLA..HHN  | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.MPM..HHZ  | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.DTP..HHE  | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.WBM..HHZ  | 2019-07-04T00:00:00.003100Z - 2019-07-04T23:59:59.963100Z | 25.0 Hz, 2160000 samples\nCI.WCS2..HHN | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.CLC..HHE  | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nPB.B921..EH1 | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.DAW..HHZ  | 2019-07-04T00:00:00.003100Z - 2019-07-04T23:59:59.963100Z | 25.0 Hz, 2160000 samples\nCI.CLC..HHZ  | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.WMF..HHN  | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.TOW2..HHN | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.DAW..HHE  | 2019-07-04T00:00:00.003100Z - 2019-07-04T23:59:59.963100Z | 25.0 Hz, 2160000 samples\nPB.B918..EH1 | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.WRC2..HHN | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.JRC2..HHN | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.CCC..HHN  | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nPB.B916..EH1 | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.MPM..HHE  | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.DTP..HHZ  | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.WBM..HHE  | 2019-07-04T00:00:00.003100Z - 2019-07-04T23:59:59.963100Z | 25.0 Hz, 2160000 samples\nCI.SRT..HHN  | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nPB.B917..EHZ | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.WVP2..EHZ | 2019-07-04T00:00:00.000000Z - 2019-07-04T23:59:59.960000Z | 25.0 Hz, 2160000 samples\nPB.B917..EH2 | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.LRL..HHN  | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.CCC..HHE  | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.SLA..HHZ  | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.WNM..EHZ  | 2019-07-04T00:00:00.000000Z - 2019-07-04T23:59:59.960000Z | 25.0 Hz, 2160000 samples\nCI.WRC2..HHE | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.JRC2..HHE | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.SRT..HHE  | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.LRL..HHE  | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.MPM..HHN  | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nPB.B921..EHZ | 2019-07-03T23:59:59.998200Z - 2019-07-04T23:59:59.958200Z | 25.0 Hz, 2160000 samples\nCI.WBM..HHN  | 2019-07-04T00:00:00.003100Z - 2019-07-04T23:59:59.963100Z | 25.0 Hz, 2160000 samples\nPB.B921..EH2 | 2019-07-03T23:59:59.998200Z - 2019-07-04T23:59:59.958200Z | 25.0 Hz, 2160000 samples\nCI.WMF..HHE  | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.WCS2..HHZ | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.TOW2..HHE | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.DAW..HHN  | 2019-07-04T00:00:00.003100Z - 2019-07-04T23:59:59.963100Z | 25.0 Hz, 2160000 samples\n</pre> In\u00a0[17]: Copied! <pre># plot the continuous seismograms from a single station\nfig = continuous_seismograms.select(station=\"CLC\").plot()\n</pre> # plot the continuous seismograms from a single station fig = continuous_seismograms.select(station=\"CLC\").plot() In\u00a0[18]: Copied! <pre># then, cast data into `numpy.ndarray`\nstation_codes = list(set([st.stats.station for st in continuous_seismograms]))\ncomponent_codes = [\"N\", \"E\", \"Z\"]\ncomponent_aliases={\"E\": [\"E\", \"2\"], \"N\": [\"N\", \"1\"], \"Z\": [\"Z\"]}\n\nnum_stations = len(station_codes)\nnum_channels = len(component_codes)\nnum_samples = len(continuous_seismograms[0].data)\n\ncontinuous_seismograms_arr = np.zeros((num_stations, num_channels, num_samples), dtype=np.float32)\nfor s, sta in enumerate(station_codes):\n    for c, cp in enumerate(component_codes):\n        for cp_alias in component_aliases[cp]:\n            sel_seismogram = continuous_seismograms.select(station=sta, component=cp_alias)\n            if len(sel_seismogram) &gt; 0:\n                continuous_seismograms_arr[s, c, :] = sel_seismogram[0].data\n                break\n            \ncontinuous_seismograms_arr\n</pre> # then, cast data into `numpy.ndarray` station_codes = list(set([st.stats.station for st in continuous_seismograms])) component_codes = [\"N\", \"E\", \"Z\"] component_aliases={\"E\": [\"E\", \"2\"], \"N\": [\"N\", \"1\"], \"Z\": [\"Z\"]}  num_stations = len(station_codes) num_channels = len(component_codes) num_samples = len(continuous_seismograms[0].data)  continuous_seismograms_arr = np.zeros((num_stations, num_channels, num_samples), dtype=np.float32) for s, sta in enumerate(station_codes):     for c, cp in enumerate(component_codes):         for cp_alias in component_aliases[cp]:             sel_seismogram = continuous_seismograms.select(station=sta, component=cp_alias)             if len(sel_seismogram) &gt; 0:                 continuous_seismograms_arr[s, c, :] = sel_seismogram[0].data                 break              continuous_seismograms_arr  Out[18]: <pre>array([[[ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n          0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n          0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n        [ 1.84921765e-11,  7.31533306e-11,  1.31919392e-10, ...,\n          4.41088166e-10, -3.08060050e-10, -4.02700095e-10]],\n\n       [[ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n          0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n          0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n        [-2.38018650e-11,  4.41909790e-12, -4.36377739e-11, ...,\n         -4.77496098e-11, -1.39665293e-10, -1.53408883e-10]],\n\n       [[ 2.37386014e-11,  1.53665813e-11, -3.40902896e-11, ...,\n         -6.43728626e-10,  2.97708330e-10,  4.56077842e-10],\n        [-2.53759392e-11, -4.20401144e-11, -6.94136068e-11, ...,\n          2.63994826e-09, -1.96964622e-09, -2.60437849e-09],\n        [-7.44370451e-12, -5.62608536e-11, -1.99733771e-11, ...,\n          1.09469663e-10,  4.91648389e-10, -2.35225506e-10]],\n\n       ...,\n\n       [[-2.02754636e-11,  7.27350949e-12,  2.90683051e-11, ...,\n         -1.27593561e-10,  4.12396610e-11,  2.02138389e-10],\n        [-8.89091231e-12,  1.46336155e-11,  4.67565708e-11, ...,\n         -1.25822366e-10,  5.78710552e-11,  4.38750390e-11],\n        [ 5.92738358e-11,  3.67502244e-11, -1.45799206e-10, ...,\n         -4.12412986e-11,  2.63505495e-10, -1.57162269e-11]],\n\n       [[ 9.44847282e-13, -4.88573140e-11, -1.88315839e-11, ...,\n          6.89705126e-10, -9.44167411e-10, -3.75163900e-10],\n        [-8.84086315e-13,  4.09784810e-11,  1.77303294e-11, ...,\n         -1.74284795e-10,  4.16906287e-10,  4.08679451e-10],\n        [ 9.03369653e-12, -5.57506714e-11,  3.33405768e-11, ...,\n          6.59004448e-11,  4.27518090e-11,  1.05661646e-10]],\n\n       [[ 4.09647620e-12,  1.35595988e-11,  1.61010198e-11, ...,\n          1.87989971e-10,  5.81934279e-10,  1.59266211e-11],\n        [ 8.85055397e-12,  1.41080064e-11,  2.29350306e-12, ...,\n         -1.22290775e-10,  4.84999090e-11,  1.02444719e-10],\n        [-1.44921930e-11, -8.83197541e-13,  5.94794233e-13, ...,\n          3.61067509e-10,  1.61199512e-10, -2.93843921e-10]]],\n      dtype=float32)</pre> In\u00a0[19]: Copied! <pre># PHASE_ON_COMP: dictionary defining which moveout we use to extract the waveform.\n#                Here, we use windows centered around the S wave for horizontal components\n#                and windows starting 1sec before the P wave for the vertical component.\nPHASE_ON_COMP = {\"N\": \"S\", \"E\": \"S\", \"Z\": \"P\"}\n# OFFSET_PHASE_SEC: dictionary defining the time offset taken before a given phase\n#               for example OFFSET_PHASE_SEC[\"P\"] = 1.0 means that we extract the window\n#               1 second before the predicted P arrival time\nOFFSET_PHASE_SEC = {\"P\": 1.0, \"S\": 4.0}\n# TEMPLATE_DURATION_SEC\nTEMPLATE_DURATION_SEC = 8. \n# SAMPLING_RATE_HZ\nSAMPLING_RATE_HZ = 25.\n# TEMPLATE_DURATION_SAMP\nTEMPLATE_DURATION_SAMP = int(TEMPLATE_DURATION_SEC * SAMPLING_RATE_HZ)\n</pre> # PHASE_ON_COMP: dictionary defining which moveout we use to extract the waveform. #                Here, we use windows centered around the S wave for horizontal components #                and windows starting 1sec before the P wave for the vertical component. PHASE_ON_COMP = {\"N\": \"S\", \"E\": \"S\", \"Z\": \"P\"} # OFFSET_PHASE_SEC: dictionary defining the time offset taken before a given phase #               for example OFFSET_PHASE_SEC[\"P\"] = 1.0 means that we extract the window #               1 second before the predicted P arrival time OFFSET_PHASE_SEC = {\"P\": 1.0, \"S\": 4.0} # TEMPLATE_DURATION_SEC TEMPLATE_DURATION_SEC = 8.  # SAMPLING_RATE_HZ SAMPLING_RATE_HZ = 25. # TEMPLATE_DURATION_SAMP TEMPLATE_DURATION_SAMP = int(TEMPLATE_DURATION_SEC * SAMPLING_RATE_HZ)  In\u00a0[20]: Copied! <pre># add station_code columns to `selected_event_picks`\nselected_event_picks.set_index(\"station_id\", inplace=True)\nfor staid in selected_event_picks.index:\n    station_code = staid.split(\".\")[1]\n    selected_event_picks.loc[staid, \"station_code\"] = station_code\nselected_event_picks\n</pre> # add station_code columns to `selected_event_picks` selected_event_picks.set_index(\"station_id\", inplace=True) for staid in selected_event_picks.index:     station_code = staid.split(\".\")[1]     selected_event_picks.loc[staid, \"station_code\"] = station_code selected_event_picks <pre>/tmp/ipykernel_2902902/1881751651.py:5: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  selected_event_picks.loc[staid, \"station_code\"] = station_code\n</pre> Out[20]: phase_index phase_time phase_score phase_type dt_s phase_polarity phase_amplitude event_index sp_ratio event_amplitude adloc_mask adloc_residual_time adloc_residual_amplitude station_code station_id CI.CLC..HN 6137848 2019-07-04 17:02:58.488 0.969 P 0.01 -0.879 -0.511873 1101 1.017569 2.474507e-07 1 -0.010048 -0.054426 CLC CI.CLC..BH 6137847 2019-07-04 17:02:58.489 0.633 P 0.01 -0.324 -0.666956 1101 1.014822 2.022085e-07 1 0.048153 -0.127168 CLC CI.CLC..HH 6137849 2019-07-04 17:02:58.498 0.965 P 0.01 -0.883 -0.586030 1101 1.086886 2.141392e-07 1 -0.007135 -0.236796 CLC CI.SRT..HN 6137990 2019-07-04 17:02:59.908 0.879 P 0.01 0.699 -0.643401 1101 1.377537 4.387864e-07 1 -0.069463 0.065039 SRT CI.SRT..HH 6137990 2019-07-04 17:02:59.908 0.891 P 0.01 0.734 -0.601019 1101 0.729745 5.910421e-07 1 -0.078144 0.020251 SRT ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... CI.WMF..HH 6139185 2019-07-04 17:03:11.858 0.672 S 0.01 0.021 -1.368759 1101 1.096820 3.073536e-07 1 0.140179 -0.329117 WMF CI.WMF..HN 6139189 2019-07-04 17:03:11.898 0.633 S 0.01 0.026 -1.371407 1101 0.989939 2.891483e-07 1 0.159858 -0.171350 WMF CI.DTP..BH 6139200 2019-07-04 17:03:12.019 0.734 S 0.01 0.035 -1.322484 1101 1.385395 1.624267e-07 1 0.068940 0.175198 DTP CI.DTP..HH 6139203 2019-07-04 17:03:12.038 0.707 S 0.01 -0.015 -1.306977 1101 1.122809 3.205840e-07 1 0.052824 0.125587 DTP CI.DTP..HN 6139211 2019-07-04 17:03:12.118 0.625 S 0.01 0.043 -1.282829 1101 0.863380 2.702479e-07 1 0.105026 0.211253 DTP <p>90 rows \u00d7 14 columns</p> <p>In the following cell, we build the <code>numpy.ndarray</code> of moveouts $\\tilde{\\tau}_{s,c}$, expressed in units of samples.</p> In\u00a0[21]: Copied! <pre># first, we extract the set of relative delay times of the beginning of each\n# template window on a given station and component\ntau_s_c_sec = np.zeros((num_stations, num_channels), dtype=np.float64)\nfor s, sta in enumerate(station_codes):\n    for c, cp in enumerate(component_codes):\n        phase_type = PHASE_ON_COMP[cp]\n        picks_s_c = selected_event_picks[\n            (\n                (selected_event_picks[\"station_code\"] == sta)\n                &amp; (selected_event_picks[\"phase_type\"] == phase_type)\n            )\n        ]\n        if len(picks_s_c) == 0:\n            # no pick for this station/component: set to -999\n            tau_s_c_sec[s, c] = -999\n        elif len(picks_s_c) == 1:\n            # express pick relative to beginning of day (midnight)\n            _pick = pd.Timestamp(picks_s_c[\"phase_time\"])\n            _relative_pick_sec = (_pick - pd.Timestamp(_pick.strftime(\"%Y-%m-%d\"))).total_seconds()\n            tau_s_c_sec[s, c] = _relative_pick_sec - OFFSET_PHASE_SEC[phase_type]\n        else:\n            # there were several picks from different channels: average them\n            _relative_pick_sec = 0.\n            for _pick in picks_s_c[\"phase_time\"].values:\n                _pick = pd.Timestamp(_pick)\n                _relative_pick_sec += (_pick - pd.Timestamp(_pick.strftime(\"%Y-%m-%d\"))).total_seconds()\n            _relative_pick_sec /= float(len(picks_s_c[\"phase_time\"]))\n            tau_s_c_sec[s, c] = _relative_pick_sec - OFFSET_PHASE_SEC[phase_type]\n# now, we convert these relative times into samples \n# and express them relative to the earliest time\n# we also store in memory the minimum time offset `tau_min_samp` for the next step\nmoveouts_samp_arr = (tau_s_c_sec * SAMPLING_RATE_HZ).astype(np.int64)\ntau_min_samp = np.min(moveouts_samp_arr[moveouts_samp_arr &gt; 0])\nmoveouts_samp_arr = moveouts_samp_arr - tau_min_samp\nmoveouts_samp_arr\n</pre> # first, we extract the set of relative delay times of the beginning of each # template window on a given station and component tau_s_c_sec = np.zeros((num_stations, num_channels), dtype=np.float64) for s, sta in enumerate(station_codes):     for c, cp in enumerate(component_codes):         phase_type = PHASE_ON_COMP[cp]         picks_s_c = selected_event_picks[             (                 (selected_event_picks[\"station_code\"] == sta)                 &amp; (selected_event_picks[\"phase_type\"] == phase_type)             )         ]         if len(picks_s_c) == 0:             # no pick for this station/component: set to -999             tau_s_c_sec[s, c] = -999         elif len(picks_s_c) == 1:             # express pick relative to beginning of day (midnight)             _pick = pd.Timestamp(picks_s_c[\"phase_time\"])             _relative_pick_sec = (_pick - pd.Timestamp(_pick.strftime(\"%Y-%m-%d\"))).total_seconds()             tau_s_c_sec[s, c] = _relative_pick_sec - OFFSET_PHASE_SEC[phase_type]         else:             # there were several picks from different channels: average them             _relative_pick_sec = 0.             for _pick in picks_s_c[\"phase_time\"].values:                 _pick = pd.Timestamp(_pick)                 _relative_pick_sec += (_pick - pd.Timestamp(_pick.strftime(\"%Y-%m-%d\"))).total_seconds()             _relative_pick_sec /= float(len(picks_s_c[\"phase_time\"]))             tau_s_c_sec[s, c] = _relative_pick_sec - OFFSET_PHASE_SEC[phase_type] # now, we convert these relative times into samples  # and express them relative to the earliest time # we also store in memory the minimum time offset `tau_min_samp` for the next step moveouts_samp_arr = (tau_s_c_sec * SAMPLING_RATE_HZ).astype(np.int64) tau_min_samp = np.min(moveouts_samp_arr[moveouts_samp_arr &gt; 0]) moveouts_samp_arr = moveouts_samp_arr - tau_min_samp moveouts_samp_arr Out[21]: <pre>array([[     160,      160,      105],\n       [     165,      165,      105],\n       [     101,      101,       68],\n       [      78,       78,       53],\n       [-1559399, -1559399, -1559399],\n       [      94,       94,       66],\n       [     102,      102,       64],\n       [      72,       72,       48],\n       [-1559399, -1559399, -1559399],\n       [     175,      175,      115],\n       [     277,      277,      174],\n       [-1559399, -1559399, -1559399],\n       [     156,      156,      104],\n       [      91,       91,       62],\n       [     179,      179,      116],\n       [     159,      159,      102],\n       [     225,      225,      140],\n       [     272,      272,      171],\n       [-1559399, -1559399, -1559399],\n       [       0,        0,       13],\n       [-1559399, -1559399, -1559399]])</pre> <p>Next, we use the moveouts, in samples, to clip out the relevant template waveforms from the continuous seismograms.</p> In\u00a0[22]: Copied! <pre>template_waveforms_arr = np.zeros((num_stations, num_channels, TEMPLATE_DURATION_SAMP), dtype=np.float32)\nweights_arr = np.ones((num_stations, num_channels), dtype=np.float32)\n\nfor s, sta in enumerate(station_codes):\n    for c, cp in enumerate(component_codes):\n        if moveouts_samp_arr[s, c] &lt; 0:\n            # no picks were found on this station\n            weights_arr[s, c] = 0.\n            continue\n        starttime = tau_min_samp + moveouts_samp_arr[s, c]\n        endtime = starttime + TEMPLATE_DURATION_SAMP\n        template_waveforms_arr[s, c, :] = continuous_seismograms_arr[s, c, starttime:endtime]\n        if template_waveforms_arr[s, c, :].sum() == 0.:\n            # no data was available on this channel\n            weights_arr[s, c] = 0.\n        \ntemplate_waveforms_arr\n</pre> template_waveforms_arr = np.zeros((num_stations, num_channels, TEMPLATE_DURATION_SAMP), dtype=np.float32) weights_arr = np.ones((num_stations, num_channels), dtype=np.float32)  for s, sta in enumerate(station_codes):     for c, cp in enumerate(component_codes):         if moveouts_samp_arr[s, c] &lt; 0:             # no picks were found on this station             weights_arr[s, c] = 0.             continue         starttime = tau_min_samp + moveouts_samp_arr[s, c]         endtime = starttime + TEMPLATE_DURATION_SAMP         template_waveforms_arr[s, c, :] = continuous_seismograms_arr[s, c, starttime:endtime]         if template_waveforms_arr[s, c, :].sum() == 0.:             # no data was available on this channel             weights_arr[s, c] = 0.          template_waveforms_arr Out[22]: <pre>array([[[ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n          0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n          0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n        [ 1.27826226e-07,  1.54385773e-07,  1.52002073e-07, ...,\n          1.35540016e-04, -3.16935242e-04, -1.91862506e-04]],\n\n       [[ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n          0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n          0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n        [ 6.11326456e-08,  1.84645728e-07,  2.68150814e-07, ...,\n         -1.40672455e-05,  9.12318046e-06,  1.10272435e-06]],\n\n       [[ 1.66893849e-04, -2.58728869e-05,  6.50364236e-05, ...,\n         -7.73223348e-07, -1.86355668e-04, -9.10725430e-05],\n        [-1.66780126e-04, -1.16682213e-05,  5.75094709e-05, ...,\n          6.42596220e-04,  1.95589619e-05, -4.81434923e-04],\n        [-5.93588076e-08, -3.46537746e-07, -6.83403698e-07, ...,\n         -1.18089200e-04,  2.95030859e-05,  2.25110809e-04]],\n\n       ...,\n\n       [[ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n          0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n          0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n          0.00000000e+00,  0.00000000e+00,  0.00000000e+00]],\n\n       [[-5.25659072e-09, -1.48266039e-08, -3.46690996e-08, ...,\n          1.44427861e-04,  1.56432026e-04,  1.08182969e-04],\n        [-1.68753012e-09,  1.41827234e-08,  3.03298116e-08, ...,\n          2.74698214e-05, -3.77784309e-05, -6.10038915e-05],\n        [-5.34306466e-08, -2.99862990e-07, -4.93349717e-07, ...,\n          3.39780963e-05,  6.50056099e-05,  1.51939503e-06]],\n\n       [[ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n          0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n          0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n          0.00000000e+00,  0.00000000e+00,  0.00000000e+00]]],\n      dtype=float32)</pre> In\u00a0[23]: Copied! <pre># normalize template waveforms for numerical reasons\nnorm = np.std(template_waveforms_arr, axis=-1, keepdims=True)\nnorm[norm == 0.] = 1. \ntemplate_waveforms_arr /= norm\n\n# normalize weights so that they sum up to one\nweights_arr /= np.sum(weights_arr)\n\n# normalize continuous seismograms for numerical reasons\nnorm = np.std(continuous_seismograms_arr, axis=-1, keepdims=True)\nnorm[norm == 0.] = 1. \ncontinuous_seismograms_arr /= norm\n</pre> # normalize template waveforms for numerical reasons norm = np.std(template_waveforms_arr, axis=-1, keepdims=True) norm[norm == 0.] = 1.  template_waveforms_arr /= norm  # normalize weights so that they sum up to one weights_arr /= np.sum(weights_arr)  # normalize continuous seismograms for numerical reasons norm = np.std(continuous_seismograms_arr, axis=-1, keepdims=True) norm[norm == 0.] = 1.  continuous_seismograms_arr /= norm In\u00a0[24]: Copied! <pre># FMF_STEP_SAMP: this is the step between two consecutive calculation of the correlation coefficient\nFMF_STEP_SAMP = 1\n# ARCH: it determines whether you want to use GPUs or CPUs \n#       If you do not have an Nvidia GPU, set ARCH = \"cpu\"\nARCH = \"gpu\"\n</pre> # FMF_STEP_SAMP: this is the step between two consecutive calculation of the correlation coefficient FMF_STEP_SAMP = 1 # ARCH: it determines whether you want to use GPUs or CPUs  #       If you do not have an Nvidia GPU, set ARCH = \"cpu\" ARCH = \"gpu\" In\u00a0[25]: Copied! <pre>cc = fmf.matched_filter(\n    template_waveforms_arr.astype(np.float32),\n    moveouts_samp_arr.astype(np.int32),\n    weights_arr.astype(np.float32),\n    continuous_seismograms_arr.astype(np.float32),\n    FMF_STEP_SAMP,\n    arch=ARCH,\n)\n</pre> cc = fmf.matched_filter(     template_waveforms_arr.astype(np.float32),     moveouts_samp_arr.astype(np.int32),     weights_arr.astype(np.float32),     continuous_seismograms_arr.astype(np.float32),     FMF_STEP_SAMP,     arch=ARCH, ) In\u00a0[26]: Copied! <pre># FMF is programmed to handle multiple templates at once. Here, we only used\n# a single template, hence the size of the outermost axis of \"1\"\ncc.shape\n</pre> # FMF is programmed to handle multiple templates at once. Here, we only used # a single template, hence the size of the outermost axis of \"1\" cc.shape Out[26]: <pre>(1, 2159801)</pre> In\u00a0[27]: Copied! <pre># let's print the output of our template matching run, which a time series of network-averaged correlation coefficients\n# of same duration as the continuous seismograms\n_cc = cc[0, :]\ntime_cc = np.arange(len(_cc)) / SAMPLING_RATE_HZ\n\nfig = plt.figure(\"network_averaged_cc\", figsize=(20, 6))\ngs = fig.add_gridspec(ncols=4)\n\nax1 = fig.add_subplot(gs[:3])\nax1.plot(time_cc, _cc, lw=0.75)\nax1.set_xlabel(\"Elapsed time (sec)\")\nax1.set_ylabel(\"Network-averaged cc\")\nax1.set_xlim(time_cc.min(), time_cc.max())\nax1.set_title(\"Time series of template/continuous seismograms similarity\")\n\nax2 = fig.add_subplot(gs[3], sharey=ax1)\n_ = ax2.hist(_cc, orientation=\"horizontal\", bins=250, density=True)\nax2.set_xlabel(\"Normalized count\")\nax2.set_title(\"Histogram\")\n\nfor ax in [ax1, ax2]:\n    ax.grid()\n</pre> # let's print the output of our template matching run, which a time series of network-averaged correlation coefficients # of same duration as the continuous seismograms _cc = cc[0, :] time_cc = np.arange(len(_cc)) / SAMPLING_RATE_HZ  fig = plt.figure(\"network_averaged_cc\", figsize=(20, 6)) gs = fig.add_gridspec(ncols=4)  ax1 = fig.add_subplot(gs[:3]) ax1.plot(time_cc, _cc, lw=0.75) ax1.set_xlabel(\"Elapsed time (sec)\") ax1.set_ylabel(\"Network-averaged cc\") ax1.set_xlim(time_cc.min(), time_cc.max()) ax1.set_title(\"Time series of template/continuous seismograms similarity\")  ax2 = fig.add_subplot(gs[3], sharey=ax1) _ = ax2.hist(_cc, orientation=\"horizontal\", bins=250, density=True) ax2.set_xlabel(\"Normalized count\") ax2.set_title(\"Histogram\")  for ax in [ax1, ax2]:     ax.grid()  In\u00a0[28]: Copied! <pre>def select_cc_indexes(\n    cc_t,\n    threshold,\n    search_win,\n):\n    \"\"\"Select the peaks in the CC time series.\n\n    Parameters\n    ------------\n    cc_t: (n_corr,) numpy.ndarray\n        The CC time series for one template.\n    threshold: (n_corr,) numpy.ndarray or scalar\n        The detection threshold.\n    search_win: scalar int\n        The minimum inter-event time, in units of correlation step.\n\n\n    Returns\n    --------\n    cc_idx: (n_detections,) numpy.ndarray\n        The list of all selected CC indexes. They give the timings of the\n        detected events.\n    \"\"\"\n\n    cc_detections = cc_t &gt; threshold\n    cc_idx = np.where(cc_detections)[0]\n\n    cc_idx = list(cc_idx)\n    n_rm = 0\n    for i in range(1, len(cc_idx)):\n        if (cc_idx[i - n_rm] - cc_idx[i - n_rm - 1]) &lt; search_win:\n            if cc_t[cc_idx[i - n_rm]] &gt; cc_t[cc_idx[i - n_rm - 1]]:\n                # keep (i-n_rm)-th detection\n                cc_idx.remove(cc_idx[i - n_rm - 1])\n            else:\n                # keep (i-n_rm-1)-th detection\n                cc_idx.remove(cc_idx[i - n_rm])\n            n_rm += 1\n    cc_idx = np.asarray(cc_idx)\n    return cc_idx\n    \n</pre> def select_cc_indexes(     cc_t,     threshold,     search_win, ):     \"\"\"Select the peaks in the CC time series.      Parameters     ------------     cc_t: (n_corr,) numpy.ndarray         The CC time series for one template.     threshold: (n_corr,) numpy.ndarray or scalar         The detection threshold.     search_win: scalar int         The minimum inter-event time, in units of correlation step.       Returns     --------     cc_idx: (n_detections,) numpy.ndarray         The list of all selected CC indexes. They give the timings of the         detected events.     \"\"\"      cc_detections = cc_t &gt; threshold     cc_idx = np.where(cc_detections)[0]      cc_idx = list(cc_idx)     n_rm = 0     for i in range(1, len(cc_idx)):         if (cc_idx[i - n_rm] - cc_idx[i - n_rm - 1]) &lt; search_win:             if cc_t[cc_idx[i - n_rm]] &gt; cc_t[cc_idx[i - n_rm - 1]]:                 # keep (i-n_rm)-th detection                 cc_idx.remove(cc_idx[i - n_rm - 1])             else:                 # keep (i-n_rm-1)-th detection                 cc_idx.remove(cc_idx[i - n_rm])             n_rm += 1     cc_idx = np.asarray(cc_idx)     return cc_idx      In\u00a0[29]: Copied! <pre># INTEREVENT_TIME_RESOLUTION_SEC: In some cases, a template might trigger multiple, closely spaced detections because\n#                                 of a phenomenon similar to that of \"cycle skipping\", where the waveform correlates\n#                                 well with a time-shifted version of itself. Thus, to avoid redundant detections, we\n#                                 set a minimum time separation between triggers (rule of thumb: about half the template duration)\nINTEREVENT_TIME_RESOLUTION_SEC = 5.\nINTEREVENT_TIME_RESOLUTION_SAMP = int(INTEREVENT_TIME_RESOLUTION_SEC * SAMPLING_RATE_HZ)\n_cc = cc[0, :]\ntime_cc = np.arange(len(_cc)) * FMF_STEP_SAMP / SAMPLING_RATE_HZ\nNUM_RMS = 8.\ndetection_threshold = NUM_RMS * np.std(_cc)\nevent_cc_indexes = select_cc_indexes(_cc, detection_threshold, INTEREVENT_TIME_RESOLUTION_SAMP)\n\nfig = plt.figure(\"network_averaged_cc\", figsize=(20, 6))\ngs = fig.add_gridspec(ncols=4)\n\nax1 = fig.add_subplot(gs[:3])\nax1.plot(time_cc, _cc, lw=0.75)\nax1.scatter(time_cc[event_cc_indexes], _cc[event_cc_indexes], linewidths=0.25, edgecolor=\"k\", color=\"r\", zorder=2)\nax1.set_xlabel(\"Elapsed time (sec)\")\nax1.set_ylabel(\"Network-averaged cc\")\nax1.set_xlim(time_cc.min(), time_cc.max())\nax1.set_title(\"Time series of template/continuous seismograms similarity\")\n\nax2 = fig.add_subplot(gs[3], sharey=ax1)\n_ = ax2.hist(_cc, orientation=\"horizontal\", bins=250, density=True, zorder=2)\nax2.set_xlabel(\"Normalized count\")\nax2.set_title(\"Histogram\")\n\nlabel = f\"Detection threshold: {NUM_RMS:.0f}\"r\"$\\times \\mathrm{RMS}(\\mathrm{CC}(t))$\"f\"\\n{len(event_cc_indexes):d} detected events\"\nfor ax in [ax1, ax2]:\n    ax.grid()\n    ax.axhline(\n        detection_threshold, ls=\"--\", color=\"r\",\n        label=label\n        )\nax1.legend(loc=\"upper left\")\n</pre> # INTEREVENT_TIME_RESOLUTION_SEC: In some cases, a template might trigger multiple, closely spaced detections because #                                 of a phenomenon similar to that of \"cycle skipping\", where the waveform correlates #                                 well with a time-shifted version of itself. Thus, to avoid redundant detections, we #                                 set a minimum time separation between triggers (rule of thumb: about half the template duration) INTEREVENT_TIME_RESOLUTION_SEC = 5. INTEREVENT_TIME_RESOLUTION_SAMP = int(INTEREVENT_TIME_RESOLUTION_SEC * SAMPLING_RATE_HZ) _cc = cc[0, :] time_cc = np.arange(len(_cc)) * FMF_STEP_SAMP / SAMPLING_RATE_HZ NUM_RMS = 8. detection_threshold = NUM_RMS * np.std(_cc) event_cc_indexes = select_cc_indexes(_cc, detection_threshold, INTEREVENT_TIME_RESOLUTION_SAMP)  fig = plt.figure(\"network_averaged_cc\", figsize=(20, 6)) gs = fig.add_gridspec(ncols=4)  ax1 = fig.add_subplot(gs[:3]) ax1.plot(time_cc, _cc, lw=0.75) ax1.scatter(time_cc[event_cc_indexes], _cc[event_cc_indexes], linewidths=0.25, edgecolor=\"k\", color=\"r\", zorder=2) ax1.set_xlabel(\"Elapsed time (sec)\") ax1.set_ylabel(\"Network-averaged cc\") ax1.set_xlim(time_cc.min(), time_cc.max()) ax1.set_title(\"Time series of template/continuous seismograms similarity\")  ax2 = fig.add_subplot(gs[3], sharey=ax1) _ = ax2.hist(_cc, orientation=\"horizontal\", bins=250, density=True, zorder=2) ax2.set_xlabel(\"Normalized count\") ax2.set_title(\"Histogram\")  label = f\"Detection threshold: {NUM_RMS:.0f}\"r\"$\\times \\mathrm{RMS}(\\mathrm{CC}(t))$\"f\"\\n{len(event_cc_indexes):d} detected events\" for ax in [ax1, ax2]:     ax.grid()     ax.axhline(         detection_threshold, ls=\"--\", color=\"r\",         label=label         ) ax1.legend(loc=\"upper left\") Out[29]: <pre>&lt;matplotlib.legend.Legend at 0x7f791dd0bdf0&gt;</pre> <p>Use the trigger times to build the earthquake catalog and extract event waveforms on a given station/component.</p> In\u00a0[30]: Copied! <pre>STATION_NAME = \"CLC\"\nCOMPONENT_NAME = \"Z\"\ndate = pd.Timestamp(\n    (continuous_seismograms[0].stats.starttime.timestamp + continuous_seismograms[0].stats.endtime.timestamp) / 2.,\n    unit=\"s\"\n).strftime(\"%Y-%m-%d\")\n\nsta_idx = station_codes.index(STATION_NAME)\ncp_idx = component_codes.index(COMPONENT_NAME)\n\ncatalog = {\n    \"detection_time\": [],\n    \"peak_amplitude\": [],\n    \"cc\": [],\n    \"normalized_cc\": []\n}\n\ndetected_event_waveforms = []\nfor i in range(len(event_cc_indexes)):\n    idx_start = event_cc_indexes[i] * FMF_STEP_SAMP + moveouts_samp_arr[sta_idx, cp_idx]\n    idx_end = idx_start + TEMPLATE_DURATION_SAMP\n    detected_event_waveforms.append(continuous_seismograms_arr[sta_idx, cp_idx, idx_start:idx_end])\n    # --------------------------------------\n    detection_time = pd.Timestamp(date) + pd.Timedelta(event_cc_indexes[i] * FMF_STEP_SAMP / SAMPLING_RATE_HZ, \"s\")\n    cc = _cc[event_cc_indexes[i]]\n    normalized_cc = cc / detection_threshold\n    peak_amplitude = np.abs(detected_event_waveforms[-1]).max()\n    catalog[\"detection_time\"].append(detection_time)\n    catalog[\"peak_amplitude\"].append(peak_amplitude)\n    catalog[\"cc\"].append(cc)\n    catalog[\"normalized_cc\"].append(normalized_cc)\ndetected_event_waveforms = np.asarray(detected_event_waveforms)\ncatalog = pd.DataFrame(catalog)\ncatalog\n</pre> STATION_NAME = \"CLC\" COMPONENT_NAME = \"Z\" date = pd.Timestamp(     (continuous_seismograms[0].stats.starttime.timestamp + continuous_seismograms[0].stats.endtime.timestamp) / 2.,     unit=\"s\" ).strftime(\"%Y-%m-%d\")  sta_idx = station_codes.index(STATION_NAME) cp_idx = component_codes.index(COMPONENT_NAME)  catalog = {     \"detection_time\": [],     \"peak_amplitude\": [],     \"cc\": [],     \"normalized_cc\": [] }  detected_event_waveforms = [] for i in range(len(event_cc_indexes)):     idx_start = event_cc_indexes[i] * FMF_STEP_SAMP + moveouts_samp_arr[sta_idx, cp_idx]     idx_end = idx_start + TEMPLATE_DURATION_SAMP     detected_event_waveforms.append(continuous_seismograms_arr[sta_idx, cp_idx, idx_start:idx_end])     # --------------------------------------     detection_time = pd.Timestamp(date) + pd.Timedelta(event_cc_indexes[i] * FMF_STEP_SAMP / SAMPLING_RATE_HZ, \"s\")     cc = _cc[event_cc_indexes[i]]     normalized_cc = cc / detection_threshold     peak_amplitude = np.abs(detected_event_waveforms[-1]).max()     catalog[\"detection_time\"].append(detection_time)     catalog[\"peak_amplitude\"].append(peak_amplitude)     catalog[\"cc\"].append(cc)     catalog[\"normalized_cc\"].append(normalized_cc) detected_event_waveforms = np.asarray(detected_event_waveforms) catalog = pd.DataFrame(catalog) catalog Out[30]: detection_time peak_amplitude cc normalized_cc 0 2019-07-04 15:42:49.840 0.003044 0.167660 1.426171 1 2019-07-04 16:07:21.880 0.002178 0.162098 1.378857 2 2019-07-04 16:13:44.960 0.035267 0.252016 2.143727 3 2019-07-04 17:02:56.960 16.214684 1.000000 8.506307 4 2019-07-04 17:09:21.680 0.155795 0.339568 2.888471 5 2019-07-04 17:11:41.320 0.006684 0.120572 1.025621 6 2019-07-04 17:12:16.520 0.116193 0.181086 1.540373 7 2019-07-04 17:12:39.680 0.030746 0.150769 1.282485 8 2019-07-04 17:13:28.560 0.001898 0.156879 1.334461 9 2019-07-04 17:13:53.480 0.003748 0.125633 1.068674 10 2019-07-04 17:15:48.640 0.019283 0.254803 2.167428 11 2019-07-04 17:16:20.080 0.002100 0.200094 1.702063 12 2019-07-04 17:17:43.000 0.000707 0.123891 1.053855 13 2019-07-04 17:18:09.040 0.001339 0.118229 1.005688 14 2019-07-04 17:20:56.360 0.000434 0.154932 1.317895 15 2019-07-04 17:27:36.680 0.002425 0.143366 1.219511 16 2019-07-04 17:32:54.160 0.003028 0.174316 1.482783 17 2019-07-04 17:37:28.920 9.444638 0.129424 1.100916 18 2019-07-04 17:39:37.680 3.430590 0.118089 1.004503 19 2019-07-04 18:10:10.960 0.157595 0.127386 1.083580 20 2019-07-04 18:19:09.040 0.277436 0.121896 1.036887 21 2019-07-04 19:03:40.560 0.265189 0.148633 1.264321 22 2019-07-04 20:09:45.160 0.116027 0.146313 1.244580 23 2019-07-04 20:55:00.480 0.023483 0.179276 1.524973 24 2019-07-04 21:20:53.760 0.014506 0.133311 1.133981 25 2019-07-04 21:46:58.000 0.026888 0.132058 1.123324 26 2019-07-04 21:50:55.440 0.042805 0.216717 1.843457 27 2019-07-04 23:38:45.680 0.012385 0.130068 1.106398 In\u00a0[31]: Copied! <pre>fig = plt.figure(\"detected_event_waveforms\", figsize=(10, 15))\ngs = fig.add_gridspec(nrows=4)\n\nax1 = fig.add_subplot(gs[:3])\n\n_time_wav = np.arange(detected_event_waveforms.shape[1]) / SAMPLING_RATE_HZ\n\nstack = np.zeros(detected_event_waveforms.shape[1])\nfor i in range(detected_event_waveforms.shape[0]):\n    norm = np.abs(detected_event_waveforms[i, :]).max()\n    if catalog[\"cc\"].iloc[i] &gt; 0.999:\n        color = \"r\"\n        template_wav = detected_event_waveforms[i, :] / norm\n    else:\n        color = \"k\"\n    time_of_day = catalog[\"detection_time\"].iloc[i].strftime(\"%H:%M:%S\")\n    ax1.plot(_time_wav, detected_event_waveforms[i, :] / norm + i * 1.5, color=color)\n    ax1.text(0.98 * _time_wav.max(), i * 1.5 + 0.1, time_of_day, ha=\"right\", va=\"bottom\")\n    stack += detected_event_waveforms[i, :] / norm\nstack /= np.abs(stack).max()\nax1.set_xlabel(\"Time (s)\")\nax1.set_xlim(_time_wav.min(), _time_wav.max())\nax1.set_ylabel(\"Normalized offset amplitude\")\nax1.set_title(f\"Events detected on {date} and recorded by {STATION_NAME}.{COMPONENT_NAME}\")\n\nax2 = fig.add_subplot(gs[3], sharex=ax1)\nax2.plot(_time_wav, stack, color=\"blue\", label=\"Stacked waveforms\")\nax2.plot(_time_wav, template_wav, color=\"red\", ls=\"--\", label=\"Template waveform\")\nax2.legend(loc=\"upper left\")\nax2.set_xlabel(\"Time (s)\")\nax2.set_ylabel(\"Normalized amplitude\")\n</pre> fig = plt.figure(\"detected_event_waveforms\", figsize=(10, 15)) gs = fig.add_gridspec(nrows=4)  ax1 = fig.add_subplot(gs[:3])  _time_wav = np.arange(detected_event_waveforms.shape[1]) / SAMPLING_RATE_HZ  stack = np.zeros(detected_event_waveforms.shape[1]) for i in range(detected_event_waveforms.shape[0]):     norm = np.abs(detected_event_waveforms[i, :]).max()     if catalog[\"cc\"].iloc[i] &gt; 0.999:         color = \"r\"         template_wav = detected_event_waveforms[i, :] / norm     else:         color = \"k\"     time_of_day = catalog[\"detection_time\"].iloc[i].strftime(\"%H:%M:%S\")     ax1.plot(_time_wav, detected_event_waveforms[i, :] / norm + i * 1.5, color=color)     ax1.text(0.98 * _time_wav.max(), i * 1.5 + 0.1, time_of_day, ha=\"right\", va=\"bottom\")     stack += detected_event_waveforms[i, :] / norm stack /= np.abs(stack).max() ax1.set_xlabel(\"Time (s)\") ax1.set_xlim(_time_wav.min(), _time_wav.max()) ax1.set_ylabel(\"Normalized offset amplitude\") ax1.set_title(f\"Events detected on {date} and recorded by {STATION_NAME}.{COMPONENT_NAME}\")  ax2 = fig.add_subplot(gs[3], sharex=ax1) ax2.plot(_time_wav, stack, color=\"blue\", label=\"Stacked waveforms\") ax2.plot(_time_wav, template_wav, color=\"red\", ls=\"--\", label=\"Template waveform\") ax2.legend(loc=\"upper left\") ax2.set_xlabel(\"Time (s)\") ax2.set_ylabel(\"Normalized amplitude\") Out[31]: <pre>Text(0, 0.5, 'Normalized amplitude')</pre> In\u00a0[32]: Copied! <pre>catalog[\"return_time_s\"] = catalog[\"detection_time\"].diff().dt.total_seconds()\ncatalog\n</pre> catalog[\"return_time_s\"] = catalog[\"detection_time\"].diff().dt.total_seconds() catalog Out[32]: detection_time peak_amplitude cc normalized_cc return_time_s 0 2019-07-04 15:42:49.840 0.003044 0.167660 1.426171 NaN 1 2019-07-04 16:07:21.880 0.002178 0.162098 1.378857 1472.04 2 2019-07-04 16:13:44.960 0.035267 0.252016 2.143727 383.08 3 2019-07-04 17:02:56.960 16.214684 1.000000 8.506307 2952.00 4 2019-07-04 17:09:21.680 0.155795 0.339568 2.888471 384.72 5 2019-07-04 17:11:41.320 0.006684 0.120572 1.025621 139.64 6 2019-07-04 17:12:16.520 0.116193 0.181086 1.540373 35.20 7 2019-07-04 17:12:39.680 0.030746 0.150769 1.282485 23.16 8 2019-07-04 17:13:28.560 0.001898 0.156879 1.334461 48.88 9 2019-07-04 17:13:53.480 0.003748 0.125633 1.068674 24.92 10 2019-07-04 17:15:48.640 0.019283 0.254803 2.167428 115.16 11 2019-07-04 17:16:20.080 0.002100 0.200094 1.702063 31.44 12 2019-07-04 17:17:43.000 0.000707 0.123891 1.053855 82.92 13 2019-07-04 17:18:09.040 0.001339 0.118229 1.005688 26.04 14 2019-07-04 17:20:56.360 0.000434 0.154932 1.317895 167.32 15 2019-07-04 17:27:36.680 0.002425 0.143366 1.219511 400.32 16 2019-07-04 17:32:54.160 0.003028 0.174316 1.482783 317.48 17 2019-07-04 17:37:28.920 9.444638 0.129424 1.100916 274.76 18 2019-07-04 17:39:37.680 3.430590 0.118089 1.004503 128.76 19 2019-07-04 18:10:10.960 0.157595 0.127386 1.083580 1833.28 20 2019-07-04 18:19:09.040 0.277436 0.121896 1.036887 538.08 21 2019-07-04 19:03:40.560 0.265189 0.148633 1.264321 2671.52 22 2019-07-04 20:09:45.160 0.116027 0.146313 1.244580 3964.60 23 2019-07-04 20:55:00.480 0.023483 0.179276 1.524973 2715.32 24 2019-07-04 21:20:53.760 0.014506 0.133311 1.133981 1553.28 25 2019-07-04 21:46:58.000 0.026888 0.132058 1.123324 1564.24 26 2019-07-04 21:50:55.440 0.042805 0.216717 1.843457 237.44 27 2019-07-04 23:38:45.680 0.012385 0.130068 1.106398 6470.24 In\u00a0[33]: Copied! <pre>fig, ax = plt.subplots(num=\"return_time_vs_detection_time\", figsize=(16, 7))\n\nax.scatter(catalog[\"detection_time\"], catalog[\"return_time_s\"], c=catalog[\"cc\"], linewidths=0.25, edgecolor=\"k\", cmap=\"magma\", s=50)\nax.set_xlabel(\"Detection time\")\nax.set_ylabel(\"Return time (s)\")\nax.set_title(\"Return time vs detection time\")\nax.grid()\ncbar = plt.colorbar(ax.collections[0], ax=ax)\ncbar.set_label(\"CC\")\nax.set_yscale(\"log\")\n</pre> fig, ax = plt.subplots(num=\"return_time_vs_detection_time\", figsize=(16, 7))  ax.scatter(catalog[\"detection_time\"], catalog[\"return_time_s\"], c=catalog[\"cc\"], linewidths=0.25, edgecolor=\"k\", cmap=\"magma\", s=50) ax.set_xlabel(\"Detection time\") ax.set_ylabel(\"Return time (s)\") ax.set_title(\"Return time vs detection time\") ax.grid() cbar = plt.colorbar(ax.collections[0], ax=ax) cbar.set_label(\"CC\") ax.set_yscale(\"log\") In\u00a0[34]: Copied! <pre>M_REF = selected_event_meta.magnitude\n</pre> M_REF = selected_event_meta.magnitude <p>We can use the peak amplitudes that we read before.</p> In\u00a0[35]: Copied! <pre>template_index = np.where(catalog[\"cc\"] &gt; 0.999)[0][0]\n\nm_rel = M_REF + np.log10(catalog[\"peak_amplitude\"] / catalog[\"peak_amplitude\"].iloc[template_index])\n\ncatalog[\"m_rel_one_channel\"] = m_rel\ncatalog\n</pre> template_index = np.where(catalog[\"cc\"] &gt; 0.999)[0][0]  m_rel = M_REF + np.log10(catalog[\"peak_amplitude\"] / catalog[\"peak_amplitude\"].iloc[template_index])  catalog[\"m_rel_one_channel\"] = m_rel catalog Out[35]: detection_time peak_amplitude cc normalized_cc return_time_s m_rel_one_channel 0 2019-07-04 15:42:49.840 0.003044 0.167660 1.426171 NaN 0.750315 1 2019-07-04 16:07:21.880 0.002178 0.162098 1.378857 1472.04 0.604785 2 2019-07-04 16:13:44.960 0.035267 0.252016 2.143727 383.08 1.814178 3 2019-07-04 17:02:56.960 16.214684 1.000000 8.506307 2952.00 4.476724 4 2019-07-04 17:09:21.680 0.155795 0.339568 2.888471 384.72 2.459368 5 2019-07-04 17:11:41.320 0.006684 0.120572 1.025621 139.64 1.091826 6 2019-07-04 17:12:16.520 0.116193 0.181086 1.540373 35.20 2.331995 7 2019-07-04 17:12:39.680 0.030746 0.150769 1.282485 23.16 1.754599 8 2019-07-04 17:13:28.560 0.001898 0.156879 1.334461 48.88 0.545083 9 2019-07-04 17:13:53.480 0.003748 0.125633 1.068674 24.92 0.840592 10 2019-07-04 17:15:48.640 0.019283 0.254803 2.167428 115.16 1.551985 11 2019-07-04 17:16:20.080 0.002100 0.200094 1.702063 31.44 0.588959 12 2019-07-04 17:17:43.000 0.000707 0.123891 1.053855 82.92 0.116397 13 2019-07-04 17:18:09.040 0.001339 0.118229 1.005688 26.04 0.393615 14 2019-07-04 17:20:56.360 0.000434 0.154932 1.317895 167.32 -0.096164 15 2019-07-04 17:27:36.680 0.002425 0.143366 1.219511 400.32 0.651551 16 2019-07-04 17:32:54.160 0.003028 0.174316 1.482783 317.48 0.747959 17 2019-07-04 17:37:28.920 9.444638 0.129424 1.100916 274.76 4.242001 18 2019-07-04 17:39:37.680 3.430590 0.118089 1.004503 128.76 3.802184 19 2019-07-04 18:10:10.960 0.157595 0.127386 1.083580 1833.28 2.464358 20 2019-07-04 18:19:09.040 0.277436 0.121896 1.036887 538.08 2.709977 21 2019-07-04 19:03:40.560 0.265189 0.148633 1.264321 2671.52 2.690371 22 2019-07-04 20:09:45.160 0.116027 0.146313 1.244580 3964.60 2.331373 23 2019-07-04 20:55:00.480 0.023483 0.179276 1.524973 2715.32 1.637578 24 2019-07-04 21:20:53.760 0.014506 0.133311 1.133981 1553.28 1.428355 25 2019-07-04 21:46:58.000 0.026888 0.132058 1.123324 1564.24 1.696379 26 2019-07-04 21:50:55.440 0.042805 0.216717 1.843457 237.44 1.898305 27 2019-07-04 23:38:45.680 0.012385 0.130068 1.106398 6470.24 1.359715 <p>However, it is much better to average the log ratios over multiple channels.</p> In\u00a0[36]: Copied! <pre># first, extract clips from the continuous seismograms\ndetected_event_waveforms_all_channels = np.zeros(\n    (len(catalog), num_stations, num_channels, TEMPLATE_DURATION_SAMP), dtype=np.float32\n)\nfor i in range(len(catalog)):\n    for s in range(num_stations):\n        for c in range(num_channels):\n            idx_start = event_cc_indexes[i] * FMF_STEP_SAMP + moveouts_samp_arr[sta_idx, cp_idx]\n            idx_end = idx_start + TEMPLATE_DURATION_SAMP\n            detected_event_waveforms_all_channels[i, s, c, :] = (\n                continuous_seismograms_arr[s, c, idx_start:idx_end]\n                )\n</pre> # first, extract clips from the continuous seismograms detected_event_waveforms_all_channels = np.zeros(     (len(catalog), num_stations, num_channels, TEMPLATE_DURATION_SAMP), dtype=np.float32 ) for i in range(len(catalog)):     for s in range(num_stations):         for c in range(num_channels):             idx_start = event_cc_indexes[i] * FMF_STEP_SAMP + moveouts_samp_arr[sta_idx, cp_idx]             idx_end = idx_start + TEMPLATE_DURATION_SAMP             detected_event_waveforms_all_channels[i, s, c, :] = (                 continuous_seismograms_arr[s, c, idx_start:idx_end]                 )  In\u00a0[37]: Copied! <pre># then, measure the peak amplitudes\namplitude_ratios = (\n    np.max(detected_event_waveforms_all_channels, axis=-1)\n    / np.max(detected_event_waveforms_all_channels[template_index, ...], axis=-1)[None, ...]\n)\n\n# if some data were missing, we may have divided by 0 and nans or infs\n# mask them and ignore them in the following calculation\ninvalid = (np.isnan(amplitude_ratios) | np.isinf(amplitude_ratios))\namplitude_ratios = np.ma.masked_where(invalid, amplitude_ratios)\n\n# apply the above formula to get the relative magnitudes\nm_rel_all_channels = M_REF + np.ma.mean(np.log10(amplitude_ratios), axis=(1, 2))\n\ncatalog[\"m_rel_all_channels\"] = m_rel_all_channels\ncatalog\n</pre> # then, measure the peak amplitudes amplitude_ratios = (     np.max(detected_event_waveforms_all_channels, axis=-1)     / np.max(detected_event_waveforms_all_channels[template_index, ...], axis=-1)[None, ...] )  # if some data were missing, we may have divided by 0 and nans or infs # mask them and ignore them in the following calculation invalid = (np.isnan(amplitude_ratios) | np.isinf(amplitude_ratios)) amplitude_ratios = np.ma.masked_where(invalid, amplitude_ratios)  # apply the above formula to get the relative magnitudes m_rel_all_channels = M_REF + np.ma.mean(np.log10(amplitude_ratios), axis=(1, 2))  catalog[\"m_rel_all_channels\"] = m_rel_all_channels catalog <pre>/tmp/ipykernel_2902902/457280950.py:3: RuntimeWarning: invalid value encountered in true_divide\n  np.max(detected_event_waveforms_all_channels, axis=-1)\n</pre> Out[37]: detection_time peak_amplitude cc normalized_cc return_time_s m_rel_one_channel m_rel_all_channels 0 2019-07-04 15:42:49.840 0.003044 0.167660 1.426171 NaN 0.750315 0.948512 1 2019-07-04 16:07:21.880 0.002178 0.162098 1.378857 1472.04 0.604785 0.914793 2 2019-07-04 16:13:44.960 0.035267 0.252016 2.143727 383.08 1.814178 1.781097 3 2019-07-04 17:02:56.960 16.214684 1.000000 8.506307 2952.00 4.476724 4.476724 4 2019-07-04 17:09:21.680 0.155795 0.339568 2.888471 384.72 2.459368 2.556656 5 2019-07-04 17:11:41.320 0.006684 0.120572 1.025621 139.64 1.091826 1.440503 6 2019-07-04 17:12:16.520 0.116193 0.181086 1.540373 35.20 2.331995 2.389888 7 2019-07-04 17:12:39.680 0.030746 0.150769 1.282485 23.16 1.754599 2.043036 8 2019-07-04 17:13:28.560 0.001898 0.156879 1.334461 48.88 0.545083 1.010687 9 2019-07-04 17:13:53.480 0.003748 0.125633 1.068674 24.92 0.840592 1.131839 10 2019-07-04 17:15:48.640 0.019283 0.254803 2.167428 115.16 1.551985 1.764089 11 2019-07-04 17:16:20.080 0.002100 0.200094 1.702063 31.44 0.588959 1.078424 12 2019-07-04 17:17:43.000 0.000707 0.123891 1.053855 82.92 0.116397 0.674097 13 2019-07-04 17:18:09.040 0.001339 0.118229 1.005688 26.04 0.393615 0.755659 14 2019-07-04 17:20:56.360 0.000434 0.154932 1.317895 167.32 -0.096164 0.674497 15 2019-07-04 17:27:36.680 0.002425 0.143366 1.219511 400.32 0.651551 1.081399 16 2019-07-04 17:32:54.160 0.003028 0.174316 1.482783 317.48 0.747959 1.004731 17 2019-07-04 17:37:28.920 9.444638 0.129424 1.100916 274.76 4.242001 4.492626 18 2019-07-04 17:39:37.680 3.430590 0.118089 1.004503 128.76 3.802184 4.082750 19 2019-07-04 18:10:10.960 0.157595 0.127386 1.083580 1833.28 2.464358 2.506172 20 2019-07-04 18:19:09.040 0.277436 0.121896 1.036887 538.08 2.709977 3.136539 21 2019-07-04 19:03:40.560 0.265189 0.148633 1.264321 2671.52 2.690371 2.455167 22 2019-07-04 20:09:45.160 0.116027 0.146313 1.244580 3964.60 2.331373 2.180166 23 2019-07-04 20:55:00.480 0.023483 0.179276 1.524973 2715.32 1.637578 1.975095 24 2019-07-04 21:20:53.760 0.014506 0.133311 1.133981 1553.28 1.428355 1.859795 25 2019-07-04 21:46:58.000 0.026888 0.132058 1.123324 1564.24 1.696379 2.244630 26 2019-07-04 21:50:55.440 0.042805 0.216717 1.843457 237.44 1.898305 2.152390 27 2019-07-04 23:38:45.680 0.012385 0.130068 1.106398 6470.24 1.359715 1.358247 <p>Next, we plot the distribution of earthquake magnitudes. The cumulative distribution usually follows the so-called Gutenberg-Richter law: $$ \\log N(m \\geq M) = a - b M$$ In this example, the number of events is too low to estimate a meaningful b-value.</p> In\u00a0[38]: Copied! <pre>fig, ax = plt.subplots(num=\"magnitude_distribution\", figsize=(8, 8))\naxb = ax.twinx()\nax.set_title(\"Distribution of earthquake magnitudes\")\n\n_ = ax.hist(catalog[\"m_rel_all_channels\"], bins=15, color=\"k\", alpha=0.5, label=\"All channels\")\naxb.plot(np.sort(catalog[\"m_rel_all_channels\"]), np.arange(len(catalog))[::-1], color=\"k\", lw=2)\n\nax.set_xlabel(\"Magnitude\")\nax.set_ylabel(\"Event Count\")\naxb.set_ylabel(\"Cumulative Event Count\")\n\nfor ax in [ax, axb]:\n    ax.set_yscale(\"log\")\n</pre> fig, ax = plt.subplots(num=\"magnitude_distribution\", figsize=(8, 8)) axb = ax.twinx() ax.set_title(\"Distribution of earthquake magnitudes\")  _ = ax.hist(catalog[\"m_rel_all_channels\"], bins=15, color=\"k\", alpha=0.5, label=\"All channels\") axb.plot(np.sort(catalog[\"m_rel_all_channels\"]), np.arange(len(catalog))[::-1], color=\"k\", lw=2)  ax.set_xlabel(\"Magnitude\") ax.set_ylabel(\"Event Count\") axb.set_ylabel(\"Cumulative Event Count\")  for ax in [ax, axb]:     ax.set_yscale(\"log\")"},{"location":"notebooks/tm_one_template/#template-matching-with-a-single-template","title":"Template matching with a single template\u00b6","text":"<p>Templates are selected from Weiqiang Zhu's PhaseNet catalog.</p> <p>Download the seismic data at: https://doi.org/10.5281/zenodo.15097180</p> <p></p>"},{"location":"notebooks/tm_one_template/#load-phasenet-catalog","title":"Load PhaseNet catalog\u00b6","text":"<p>Here, we read the catalog of the 2019 Ridgecrest sequence made with PhaseNet. Information is divided into three files:</p> <ul> <li>a station metadata file,</li> <li>an event metadata file (the catalog per se),</li> <li>a pick database, which contains all the P- and S-wave picks found by PhaseNet.</li> </ul>"},{"location":"notebooks/tm_one_template/#pick-one-event","title":"Pick one event\u00b6","text":"<p>Let's use the PhaseNet catalog to read the waveforms of an event.</p>"},{"location":"notebooks/tm_one_template/#run-template-matching","title":"Run template matching\u00b6","text":"<p>We will now use one of the events from the PhaseNet catalog as a template event to detect events with template matching.</p>"},{"location":"notebooks/tm_one_template/#read-data-from-same-day","title":"Read data from same day\u00b6","text":""},{"location":"notebooks/tm_one_template/#build-template","title":"Build template\u00b6","text":""},{"location":"notebooks/tm_one_template/#-background-","title":"------------------ Background ------------------\u00b6","text":"<p>A template is a collection of waveforms at different channels, $T_{s,c}(t)$, which are clips taken from the continuous seismograms, $u_{s,c}$. These clips are taken at times defined by: $$ u_{s,c}(t)\\ |\\ t \\in \\lbrace \\tau_{s,c}; \\tau_{s,c} + D \\rbrace, $$ where $\\tau_{s,c}$ is the start time of the template window and $D$ is the template duration.</p> <p>$\\tau_{s,c}$ is given by some prior information on the event: picks or modeled arrival times. The moveouts, $\\tilde{\\tau}_{s,c}$, are the collection of delay times relative to the earliest $\\tau_{s,c}$: $$ \\tilde{\\tau}_{s,c} = \\tau_{s,c} - \\underset{s,c}{\\min} \\lbrace \\tau_{s,c} \\rbrace .$$</p>"},{"location":"notebooks/tm_one_template/#-on-the-necessity-to-clip-template-waveforms-out-of-numpyndarray-instead-of-obspystream-","title":"------------------ On the necessity to clip template waveforms out of <code>numpy.ndarray</code> instead of <code>obspy.Stream</code> ------------------\u00b6","text":"<p>Looking carefully at the output of <code>print(continuous_seismograms.__str__(extended=True))</code>, a few cells before, we see that start times are generally not exactly at midnight. This is a consequence of the discrete nature of the continuous seismograms (here, sampled at 25 samples per second). Thus, in general, the $\\tau_{s,c}$ computed from picks or modeled arrival times fall in between two samples of the seismograms.</p> <p>When running a matched-filter search, we need to make sure the moveouts, $\\tilde{\\tau}_{s,c}$, ultimately expressed in samples, match exactly the times that were used when clipping the template waveforms out of $u_{s,c}$. One way to ensure this is to first cast the $\\tau_{s,c}$ to times in samples and then operate exclusively on the <code>numpy.ndarray</code>: <code>continuous_seismograms_arr</code>:</p> <p>$$ T_{s,c}[t_n] = u_{s,c}[\\tau_{s,c} + n \\Delta t],$$ where $\\Delta t$ is the sampling time.</p>"},{"location":"notebooks/tm_one_template/#clip-out-waveforms-and-moveout-and-station-weight-arrays","title":"Clip out waveforms and moveout and station-weight arrays\u00b6","text":""},{"location":"notebooks/tm_one_template/#run-fmf","title":"Run FMF\u00b6","text":"<p>After all this data formatting, we can now run template matching (also called matched-filtering) to detect new events that are similar to our template event.</p> <p>For that, use the software Fast Matched Filter (FMF): https://github.com/beridel/fast_matched_filter</p> <p>FMF offers C and CUDA-C routines to efficiently run template matching on CPUs, or even on GPUs if available to you.</p>"},{"location":"notebooks/tm_one_template/#set-detection-threshold-and-find-events","title":"Set detection threshold and find events\u00b6","text":"<p>We will use the time series of correlation coefficients to build an earthquake catalog. For that, we need to set a detection threshold and define all times above that threshold as triggers caused by near-repeats of the template event.</p>"},{"location":"notebooks/tm_one_template/#plot-some-waveforms","title":"Plot some waveforms\u00b6","text":"<p>When building a catalog, it is always necessary to visualize some of the detected event waveforms to get a sense of the ratio of true-to-false detection rate.</p> <p>In the following, we plot the waveforms of each detected event and we also compare the stack of all the waveforms to the original template waveform. Since all events share similar waveforms, the stack is similar to the template waveform. Moreover, since noise across all these waveforms sums up incoherently, stacking acts as a denoiser which may help you produce a cleaner version of the template waveform, for example on remote stations.</p>"},{"location":"notebooks/tm_one_template/#plot-template-return-time-vs-detection-time","title":"Plot template return time vs detection time\u00b6","text":""},{"location":"notebooks/tm_one_template/#relative-magnitude","title":"Relative magnitude\u00b6","text":"<p>Template matching lends itself well to quickly estimate relative event magnitudes, relative to the magnitude of the template event.</p> <p>$$ M_r = M_{\\mathrm{ref}} + \\dfrac{1}{N} \\sum_{ch=1}^{N} \\log \\dfrac{A_{ch}}{A_{\\mathrm{ref},ch}} $$</p> <p>where the sum is taken over $N$ channels and $A_{ch}$ is the peak amplitude measured on channel $ch$ while $A_{\\mathrm{ref},ch}$ is the peak amplitude of the reference event (the template) measured on the same channel.</p>"}]}